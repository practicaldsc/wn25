{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec69f86",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lec_utils import *\n",
    "import lec20_util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143de48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Discussion 11\n",
    "\n",
    "# Gradient Descent and Classification\n",
    "\n",
    "### EECS 398: Practical Data Science, Winter 2025\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> ‚Ä¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/wn25\">github.com/practicaldsc/wn25</a> ‚Ä¢ üì£ See latest announcements [**here on Ed**](https://edstem.org/us/courses/69737/discussion/5943734) </small>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504aee40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda üìÜ\n",
    "\n",
    "- Gradient descent.\n",
    "- Classification.\n",
    "- Evaluating classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3133e5e7-7dc6-4848-8612-ff683602363e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb4627-1fc8-4a9b-b53b-e3f8f1a524c4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In order to find optimal model parameters, $w_0^*, w_1^*, ..., w_d^*$, our goal has been to minimize empirical risk.\n",
    "\n",
    "$$R_\\text{sq}(\\vec w) = \\frac{1}{n} \\sum_{i = 1}^n (y_i - H(\\vec x_i))^2$$\n",
    "\n",
    "<center><small>Empirical risk using squared loss.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b16280b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We've minimized empirical risk functions (like $R$ above) a few ways so far:\n",
    "    - Taking (partial) derivatives, setting them to 0, and solving.\n",
    "    - Using a linear algebraic-argument, which led to the normal equations, $X^TX \\vec w^* = X^T \\vec y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecb7a55",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Gradient descent** is a method for minimizing functions computationally, when doing so by hand is difficult or impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df89503",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b4dfc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we want to minimize a function $f$, which takes in a vector $\\vec w$ as input and returns a scalar as output.<br><small>For example, $f$ could be (but doesn't need to be) some empirical risk function, like the one on the previous slide.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec40ea1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To do so:\n",
    "    1. Start with an initial guess of the minimizing input, $\\vec w^{(0)}$.\n",
    "    1. Choose a learning rate, or step size, $\\alpha$.\n",
    "    1. Use the following update rule:\n",
    "    \n",
    "    $$\\vec w^{(t+1)} = \\vec w^{(t)} - \\alpha \\nabla f(\\vec w^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b664d09",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\nabla f(\\vec{w}^{(t)})$ is the **gradient of $f$**, evaluated at timestep $t$.<br>The gradient of a function is a vector containing all of its partial derivatives.\n",
    "\n",
    "\n",
    "$$f(\\vec{w}) = w_1^2 + w_2^2 \\implies \\nabla f(\\vec w) = \\begin{bmatrix} 2w_1 \\\\ 2w_2 \\end{bmatrix}$$<center><small>An example gradient calculation.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d663ce1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We repeat this **iterative** process until convergence, i.e. until the changes between $\\vec{w}^{(t+1)}$ and $\\vec{w}^{(t)}$ are small (or equivalently, until $\\nabla f(\\vec w^{(t)})$ is close to $\\vec 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f77d8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "util.minimizing_animation(w0=0, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f85f48-957a-42be-87f0-4be14d8e53be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression vs. classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e727c-f8d1-49a3-b94c-fc2260e47d46",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So far in the course, we've been working with **regression** problems ‚Äì where our goal is to predict some **quantitative** variable.<br><small>Examples: Predicting commute times, predicting house prices.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76622cc-805a-49bc-be1a-a4cf99d00ac3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- With **classification** problems, our goal is to predict a **categorical** variable. These problems involve questions like:\n",
    "  - Is this email spam or not?\n",
    "    </br><small>This is an example of binary classification, in which we predict one of **two** possible classes..</small>\n",
    "  - Is this picture of a dog, cat, zebra, or elephant?\n",
    "    </br><small>This is an example of multiclass classification, in which we predict **one** class from **more than two** possible options.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427b9fa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We learned about two classification models in Wednesday's lecture: $k$-nearest neighbors and decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6745a-aee0-485b-bdc0-f1650f902445",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The most common evaluation metric in classification is **accuracy**:\n",
    "\n",
    "    $$\\text{accuracy} = \\frac{\\text{# points classified correctly}}{\\text{# points}}$$\n",
    "    \n",
    "    Accuracy ranges from 0 to 1, i.e. 0% to 100%. **Higher** values indicate **better** model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bf7196-6d22-4808-a2b4-c38142d2b7c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae7294c-6c05-4f64-9346-895b4bac58ec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When we perform binary classification, there are 4 possible outcomes.\n",
    "\n",
    "|Outcome of Prediction|Definition|True Class|\n",
    "|---|---|---|\n",
    "|**True** positive (TP) ‚úÖ|The predictor **correctly** predicts the positive class.|P|\n",
    "|False negative (FN) ‚ùå|The predictor incorrectly predicts the negative class.|P|\n",
    "|**True** negative (TN) ‚úÖ|The predictor **correctly** predicts the negative class.|N|\n",
    "|False positive (FP) ‚ùå|The predictor incorrectly predicts the positive class.|N|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13674d0c-ce8a-4977-8790-2b86eeff12da",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We often evaluate classifiers using **precision** and **recall**.\n",
    "$$\\text{precision} = \\frac{TP}{TP + FP} \\: \\: \\: \\:  \\: \\: \\: \\: \\text{recall} = \\frac{TP}{TP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe413f-5cab-4b33-8243-da6e54f44dad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/Precisionrecall.svg.png\" width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfac177-ab80-4435-a3e0-8917436d3011",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attendance üôã\n",
    "\n",
    "<center><img src='imgs/disc11.png' width=\"500\"></img></center> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09ae71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <a href='https://study.practicaldsc.org/disc10/index.html'>Worksheet</a> üìù\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
