{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec69f86",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lec_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143de48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Discussion 12\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "### EECS 398: Practical Data Science, Winter 2025\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> ‚Ä¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/wn25\">github.com/practicaldsc/wn25</a> ‚Ä¢ üì£ See latest announcements [**here on Ed**](https://edstem.org/us/courses/69737/discussion/5943734) </small>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504aee40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda üìÜ\n",
    "\n",
    "- Logistic regression.\n",
    "- The logistic function.\n",
    "- Cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf6e40-c4c4-4b77-ba09-15352a86be76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cab749-d799-4d30-85d7-18c44553c5a6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Logistic regression is a **binary classification** technique that predicts the **probability** that a data point belongs to class 1 given its feature vector, $\\vec x_i$.\n",
    "\n",
    "$$P(y_i = 1 | \\vec{x}_i) = \\sigma (w_0 + w_1 x_i^{(1)} + w_2 x_i^{(2)} + ... + w_d x_i^{(d)}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$$\n",
    "\n",
    "<center><small>Remember, in binary classification, each $y_i$ is either 0 or 1.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398bcf66-008c-486c-a316-00e1981fe206",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we're able to predict the probability of an event, we can **classify** the event by using a **threshold**.\n",
    "<br><small>For example, if we set the threshold to 50% and our model estimates a 60% chance that a point belongs to class 1, we classify that point as class 1.</small> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6329fa66-6cdc-4235-a645-8a9e447ac03d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- __Logistic__ regression is similar to __linear__ regression in that it computes a linear combination of the input features (where the weights come from a parameter vector, $\\vec w$) to output a number as a prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bca2c7b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- However, instead of predicting a real number in the range $(-\\infty, \\infty)$, logistic regression predicts a probability, which is in the range $[0, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfaabd5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The logistic function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee7d456-854c-4744-ac48-c7c1412ffa06",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To perform this transformation from a real number to a probability, we use the **logistic function**, which transforms numerical inputs to the interval $(0,1)$:\n",
    "    $$\\sigma(t) = \\frac{1}{1 + e^{-t}} = \\frac{1}{1 + \\text{exp}(-t)}$$\n",
    "    \n",
    "- As $t \\to +\\infty$, $\\sigma(t) \\to 1$.\n",
    "- As $t \\to -\\infty$, $\\sigma(t) \\to 0$.\n",
    "- When $t = 0$, $\\sigma(0) = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34307a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.linspace(-5, 5)\n",
    "px.line(x=ts, y=1 / (1 + np.e ** (-ts)), title=r'$\\sigma(x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf81a7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the one-feature case, where $P(y_i = 1 | \\vec x_i) = \\sigma(w_0 + w_1 x_i)$:\n",
    "    - If $w_1 > 0$, then $\\sigma(w_0 + w_1 x_i)$ is an increasing function of $x_i$.\n",
    "    - If $w_1 < 0$, then $\\sigma(w_0 + w_1 x_i)$ is a decreasing function of $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(x=ts, y=1 / (1 + np.e ** -(3 - 2 * ts)), title=r'$\\sigma(3 - 2x)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5844dc12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8752fa06-f0a6-4fff-a143-eeb61bf2c7ad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As a reminder, the logistic regression model looks like:\n",
    "$$P(y_i = 1 | \\vec{x}_i) = \\sigma (w_0 + w_1 x_i^{(1)} + w_2 x_i^{(2)} + ... + w_d x_i^{(d)}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdbb4e5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To find optimal model parameters, we minimize average **cross-entropy loss**.<br>If $y_i$ is an observed value and $p_i$ is a predicted **probability**, then: \n",
    "\n",
    "$$L_\\text{ce}(y_i, p_i) = \\begin{cases} - \\log(p_i) & \\text{if $y_i = 1$} \\\\ -\\log(1 - p_i) & \\text{if $y_i = 0$} \\end{cases} = - \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f8fc2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Benefits of cross-entropy loss:\n",
    "    - The loss surface of **average cross-entropy loss** is convex, which means it's easy(er) to minimize with gradient descent than non-convex loss surfaces.\n",
    "    - Cross-entropy loss steeply penalizes incorrect probabilities, unlike squared loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfac177-ab80-4435-a3e0-8917436d3011",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attendance üôã\n",
    "\n",
    "<center><img src='imgs/disc12.png' width=\"500\"></img></center> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09ae71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <a href='https://study.practicaldsc.org/disc12/index.html'>Worksheet</a> üìù\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
