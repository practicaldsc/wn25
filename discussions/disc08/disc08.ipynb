{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec69f86",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lec_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143de48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Discussion 8\n",
    "\n",
    "# Multiple Linear Regression\n",
    "\n",
    "### EECS 398: Practical Data Science, Winter 2025\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> ‚Ä¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/wn25\">github.com/practicaldsc/wn25</a> ‚Ä¢ üì£ See latest announcements [**here on Ed**](https://edstem.org/us/courses/69737/discussion/5943734) </small>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504aee40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda üìÜ\n",
    "- The design matrix, observation vector, and parameter vector.\n",
    "- \"Solving\" the normal equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea891b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Lifespan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa42ba20",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Consider the DataFrame `lifespan`, which contains the `'lifespan'`, average `'hours_exercised'` per day, and average `'packs_cigs'` (packs of cigarettes smoked per day) for various individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d51033",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifespan = pd.DataFrame({\n",
    "    'lifespan': [86, 82, 72, 60],\n",
    "    'hours_exercised': [2, 1.5, 1, 0.5],\n",
    "    'packs_cigs': [0, 0, 1, 4]\n",
    "})\n",
    "lifespan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b4f361",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we want to predict the `'lifespan'` of an individual as a linear function of their `'hours_exercised'` and `'packs_cigs'`. In other words:\n",
    "\n",
    "$$\\text{predicted lifespan}_i = H(\\text{hours exercised}_i, \\text{packs cigs}_i) = w_0 + w_1 \\cdot \\text{hours exercised}_i + w_2 \\cdot \\text{packs cigs}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba901c7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **How do we find the optimal values of $w_0^*$, $w_1^*$, and $w_2^*$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07a8f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Augmented feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a80e65",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A feature vector, $\\vec x_i$, contains the information used to make a prediction **for a single individual**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f60f1c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In our example, $\\vec x_i = \\begin{bmatrix} \\text{hours exercised}_i \\\\ \\text{packs cigs}_i \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a509c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An **augmented** feature vector, $\\text{Aug}(\\vec x_i)$, inserts a 1 at the start of the feature vector, $\\vec x_i$.\n",
    "\n",
    "$$\\text{Aug}(\\vec x_i) = \\begin{bmatrix} 1 \\\\ \\text{hours exercised}_i \\\\ \\text{packs cigs}_i \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b2a99-9b2d-4a4e-a74c-cb8676935b42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Design matrices, observation vectors, and parameter vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63437400",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we want to build a **multiple** linear regression model that uses multiple ‚Äì specifically, $d$ ‚Äì features to make predictions.\n",
    "\n",
    "$$H(\\vec x_i) = w_0 + w_1 x_i^{(1)} + w_2 x_i^{(2)} + ... + w_d x_i^{(d)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a4127b-c7b6-435b-8c1d-392947492c80",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Define the **design matrix** $\\color{#007aff} X \\in \\mathbb{R}^{n \\times (d + 1)}$, **observation vector** $\\color{orange}{\\vec{y}} \\in \\mathbb{R}^n$, and parameter vector $\\vec{w} \\in \\mathbb{R}^{d+1}$ as:\n",
    "\n",
    "$${\\color{#007aff}{ X=  \\begin{bmatrix}  \n",
    "{1} & { x^{(1)}_1} & { x^{(2)}_1} & \\dots & { x^{(d)}_1} \\\\\\\\\n",
    "{ 1} & { x^{(1)}_2} & { x^{(2)}_2} & \\dots & { x^{(d)}_2} \\\\\\\\\n",
    "\\vdots & \\vdots & \\vdots  &  & \\vdots \\\\\\\\\n",
    "{ 1} & { x^{(1)}_n} & { x^{(2)}_n} & \\dots & { x^{(d)}_n}\n",
    "\\end{bmatrix} = \\begin{bmatrix} \n",
    "       \\text{Aug}({\\vec{x_1}})^T \\\\\\\\\n",
    "       \\text{Aug}({\\vec{x_2}})^T \\\\\\\\\n",
    "       \\vdots \\\\\\\\\n",
    "       \\text{Aug}({\\vec{x_n}})^T\n",
    "   \\end{bmatrix}}} \\qquad {\\color{orange}{\\vec{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}} \\qquad \\vec{w} = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_d \\end{bmatrix}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65df09a-0bb9-4031-a1e0-7d17087e08bf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **design matrix**, $\\color{#007aff} X$, represents our entire dataset, whereas **an _augmented_ feature vector** $\\text{Aug}(\\vec x_i)$ represents a single point.<br><small>We include a 1 in the first column of our design matrix ‚Äì and as the first element in our **augmented** feature vector ‚Äì so that the intercept (bias) term $w_0$ can be incorporated into our parameter vector.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0f6db-f90e-4162-b29c-c0699e5d14bd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **observation vector** contains all of the target (or response) values corresponding to each observation in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7687109",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimizing mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc8f215-5af6-4ef9-93f8-4e4d1e18dfa6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Our goal is to find the **optimal** parameter vector, $\\vec w^*$, which minimizes mean squared error.\n",
    "\n",
    "$$\\begin{align*} R_\\text{sq}(\\vec w) &= \\frac{1}{n} \\sum_{i = 1}^n (y_i - H(\\vec x_i))^2 \\\\ &= \\frac{1}{n} \\sum_{i = 1}^n \\left(y_i - (w_0 + w_1 x_i^{(1)} + w_2 x_i^{(2)} + ... + w_d x_i^{(d)} ) \\right)^2 \\\\ &= \\frac{1}{n} \\sum_{i = 1}^n \\left(y_i - \\text{Aug}(\\vec x_i) \\cdot \\vec{w} \\right)^2 \\end{align*}$$\n",
    "\n",
    "<!-- $$R_\\text{sq}(\\vec{w}) = \\frac{1}{n}  \\lVert {\\color{orange}{\\vec{y}}} - {\\color{#007aff} X}\\vec{w} \\rVert^2$.\n",
    "<center><small><center><small>Recall that $R_\\text{sq}$ represents the average loss $L_\\text{sq}$ of our predictions and the actual observations.</small></center> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1457af87",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we were to use a calculus-based approach, we'd need to take the partial derivative of $R_\\text{sq}(\\vec w)$ with respect to $w_0$, and with respect to $w_1$, and with respect to $w_2$, and so on, set them all to 0, and solve the resulting system of equations. **That's infeasible!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71792fce",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The reason for introducing the concept of a design matrix, $X$, and observation vector, $\\vec y$, was so that we could rewrite $R_\\text{sq}(\\vec w)$ as follows:\n",
    "\n",
    "$$R_\\text{sq}(\\vec w) = \\frac{1}{n} \\lVert \\vec y - X \\vec w \\rVert_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e2c9ae",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **How does this help us?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf8c82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The normal equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58d9f08",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Finding the optimal parameter vector, $\\vec w^*$, boils down to finding the $\\vec w$ that minimizes:\n",
    "\n",
    "$$R_\\text{sq}(\\vec w) = \\frac{1}{n} \\lVert \\vec y - X \\vec w \\rVert_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8237780",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Intuitively, this means we want $X \\vec w$ to be \"as close\" to $\\vec y$ as possible. Remember, the only unknown is $\\vec w$; $X$ and $\\vec y$ come from our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fb4089",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As mentioned in this week's lectures, this is can be done by **choosing $\\vec w^*$ such that**:\n",
    "    - the error vector, $\\vec e = \\vec y - X \\vec w^*$,\n",
    "    - is **orthogonal** to the columns of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7162583",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The above condition can be expressed as:\n",
    "\n",
    "$$X^T (\\vec y - X \\vec w^*) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c639f32",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Expanding, we have:\n",
    "\n",
    "$$X^T \\vec y - X^TX \\vec w^* = 0 \\implies \\boxed{X^T\\vec y = X^TX \\vec w^*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b0f497",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The boxed equation above is known as the **normal equations**. The $\\vec w^*$ that minimizes mean squared error is the one that satisfies the boxed condition.\n",
    "<br><small>Why are they called the normal equation**s**?</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb205378-adef-41b1-9d1e-26e90d8ab486",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **If** $X^TX$ is invertible, there's a unique solution for $\\vec w^*$:\n",
    "\n",
    "$$\\vec w^* = (X^TX)^{-1}X^T \\vec y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136a9ef5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Big takeaway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aebdc40",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We chose $\\vec w^*$ so that:\n",
    "    - The vector of predictions, $\\vec h^* = X \\vec w^*$,\n",
    "    - is the **orthogonal projection of** $\\vec y$\n",
    "    - onto the **span of the columns of the design matrix**, $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25798ba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This $\\vec w^*$ is guaranteed to minimize mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e8bd4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Lifespan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1c44c7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's return to our example from earlier. Remember, the goal is to find the best choices for $w_0^*$, $w_1^*$, and $w_2^*$ in:\n",
    "\n",
    "$$\\text{predicted lifespan}_i = H(\\text{hours exercised}_i, \\text{packs cigs}_i) = w_0 + w_1 \\cdot \\text{hours exercised}_i + w_2 \\cdot \\text{packs cigs}_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifespan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ab314-9853-4f71-92a9-a11559c80ea2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Our design matrix, observation vector, and parameter vector are defined as follows:\n",
    "\n",
    "$$\n",
    "{\\color{black} {X = \\begin{bmatrix}\n",
    "{1} & {1.0} & {1} \\\\\n",
    "{1} & {1.5} & {0} \\\\\n",
    "{1} & {0.0} & {2} \\\\\n",
    "{1} & {1.0} & {0.5}\n",
    "\\end{bmatrix}}} \\qquad {\\color{black} {\\vec{y} = \\begin{bmatrix}\n",
    "82 \\\\ 90 \\\\ 68 \\\\ 77\n",
    "\\end{bmatrix}}} \\qquad\n",
    "\\vec{w} = \\begin{bmatrix}\n",
    "w_0 \\\\ w_1 \\\\ w_2\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae91aac5-8aad-41a7-9093-e11fdd82c4be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solving for the optimal parameter vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994764d-2439-40fc-9ee9-3c6a02645595",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We'll find $\\vec{w}^* = \\begin{bmatrix} w_0^* \\\\ w_1^* \\\\ w_2^* \\end{bmatrix}$ using code. First, let's construct our design matrix, $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c12bcf-a3b5-448a-9191-7976deb2419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lifespan[['hours_exercised', 'packs_cigs']].copy()\n",
    "X['1'] = 1\n",
    "X = X.iloc[:, [-1, 0, 1]]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2642a6b8-efb3-4c0b-8aa9-2583cb3d62a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Our **observation vector**, $\\vec y$, is the `'lifespan'` column of `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb0eff-d617-4732-903f-538fdc091629",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lifespan['lifespan'].to_numpy()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ea255",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recall that the optimal parameter vector, $\\vec w^*$, is defined as:\n",
    "\n",
    "$$\\vec w^* = (X^TX)^{-1}X^T \\vec y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c1a2db-be6d-4da0-a045-1e22c7346f9a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w_star = np.linalg.inv(X.T @ X) @ X.T @ y \n",
    "w_star"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c6d40",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is telling us that the optimal way to predict `'lifespan'` as a function of `'hours_exercised'` and `'packs_cigs'` is:\n",
    "\n",
    "$$\\text{predicted lifespan}_i = H(\\text{hours exercised}_i, \\text{packs cigs}_i) = 64.35 + 11.04 \\cdot \\text{hours exercised}_i -2.52 \\cdot \\text{packs cigs}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60acae2f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can now use this parameter vector to make predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd8528-7ce1-4d29-af58-6dceb92fb1ea",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# My predicted lifespan if I exercise 3 hours a day and don't smoke.\n",
    "# Equivalent to plugging in hours_exercised_i = 3 and packs_cigs_i = 0 into the equation above.\n",
    "np.dot(w_star, np.array([1, 3, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfac177-ab80-4435-a3e0-8917436d3011",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attendance üôã"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796c70bc-6a17-4802-a511-fc32e99fb43a",
   "metadata": {},
   "source": [
    "<center><img src='imgs/disc08.png' width=\"500\"></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09ae71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <a href='https://study.practicaldsc.org/disc08/index.html'>Worksheet</a> üìù\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
