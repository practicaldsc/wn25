{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec69f86",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lec_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143de48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Discussion 10\n",
    "\n",
    "# Cross-Validation and Regularization\n",
    "\n",
    "### EECS 398: Practical Data Science, Winter 2025\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> ‚Ä¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/wn25\">github.com/practicaldsc/wn25</a> ‚Ä¢ üì£ See latest announcements [**here on Ed**](https://edstem.org/us/courses/69737/discussion/5943734) </small>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504aee40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda üìÜ\n",
    "\n",
    "- The bias-variance tradeoff.\n",
    "- Cross-validation.\n",
    "- Regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3133e5e7-7dc6-4848-8612-ff683602363e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4c0dc-d3d4-4632-921e-9d5db4c7a5f4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the real world, we're concerned with our model's ability to **generalize** on **different datasets** drawn from the same population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613c3787-3be9-41f2-930c-db6d7ba2104d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src='imgs/tt-errors.png' width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ba3c0-1f56-4c78-b29f-6636e008a769",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In lecture, we trained three different polynomial regression models ‚Äì degree 1, 3, and 25 ‚Äì each on two different datasets, <span style=\"color:orange\"><b>Sample 1</b></span> and <span style=\"color:purple\"><b>Sample 2</b></span>.<br><small>The points in <span style=\"color:blue\"><b>blue</b></span> come from Sample 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2339df62-8098-4309-a678-411136bc5f18",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<center><img src='imgs/bias-variance.png' width=900></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091fbc9c-8a04-49e5-ba92-2365d4908e6f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The degree 1 polynomials have the highest bias ‚Äì on average, they are **consistently wrong** ‚Äì while the degree 25 polynomial has the lowest bias ‚Äì on average, they are **consistently good**.\n",
    "\n",
    "$$\\text{low complexity} \\rightarrow \\text{underfits the training data} \\rightarrow \\text{high bias and low variance}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208cb65a-f171-450d-be01-1131d28aaf11",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The degree 25 polynomials have the highest variance ‚Äì from training set to training set, they vary more than the degree 1 and 3 polynomials.\n",
    "\n",
    "$$\\text{high complexity} \\rightarrow \\text{overfits the training data} \\rightarrow \\text{low bias and high variance}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d0e725-fe69-4a6b-80df-6776b7b7ff4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad902ee-9845-4752-a6c4-d91c9dc0b80a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Cross-validation, as we talked about in lecture, is one way we can split our data into training and validation sets. We can create $k$ <span style='color: green'><b>validation</b></span> sets, where $k$ is some positive integer (5 in the example below).\n",
    "\n",
    "<center><img src='imgs/k-fold.png' width=500></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c5e32e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we're choosing between **10** different hyperparameter values for our model and decide to use **5**-fold cross-validation to determine which hyperparameter performs best. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd98ec9b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, we divide the entire dataset into 5 equally-sized \"slices\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfddd28",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For each of the **10** hyperparameters, we perform **5** training rounds, for a total of **5 x 10 = 50** trainings.<br><small>**In each training**, we'll use 4 folds to train the model and the remaining 1 fold to validate (test) it. This gives us **5** test error measurements per hyperparameter choice.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad638f80",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Finally, we calculate the average validation error for each of our 10 hyperparameters, and choose the one with the lowest error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd34f874-8e8a-44ea-b75f-1a6dc7624e72",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Aside**: Some of the worksheet questions use the term \"accuracy\". Although we haven't covered it yet, accuracy is one of the ways to evaluate a classification model, where **higher accuracy is better**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd2cab-aff6-4052-a702-3e5ec8168928",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f08132c-3884-41e3-aa7d-b1f01b692bec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In general, the larger the optimal parameters $w_0^*, w_1^*, ..., w_d^*$ are, the more overfit our model is.<br>We can prevent large parameter values by minimizing mean squared error with **regularization**.\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\mathbf{+} \\underbrace{\\lambda \\sum_{j = 1}^d w_j^2}_{\\text{regularization penalty!}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf99e37e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Linear regression with $L_2$ regularization is called **ridge regression**.<br><small>Linear regression with $L_1$ regularization is called LASSO.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c2decb-779e-49a6-9c67-3874bd5ab3ee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Intuition: Instead of just minimizing mean squared error, we balance minimizing mean squared error and a penalty on the size of the fit coefficients, $w_1^*$, $w_2^*$, ..., $w_d^*$.<br><small>We don't regularize the intercept term!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daf87b8-d324-46c2-9e3d-9972eb3e8d5b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\lambda$ is a **hyperparameter**, which we choose through cross-validation.\n",
    "  - Higher $\\lambda$ ‚Üí stronger penalty, coefficients shrink more ‚Üí higher bias, lower variance (underfitting).  \n",
    "  - Lower $\\lambda$ ‚Üí weaker penalty, coefficients can grow ‚Üí lower bias, higher variance (overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfac177-ab80-4435-a3e0-8917436d3011",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attendance üôã\n",
    "\n",
    "<center><img src='imgs/disc10.png' width=\"500\"></img></center> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09ae71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <a href='https://study.practicaldsc.org/disc10/index.html'>Worksheet</a> üìù\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
