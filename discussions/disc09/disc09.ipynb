{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec69f86",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lec_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143de48",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Discussion 9\n",
    "\n",
    "# Feature Engineering and Pipelines\n",
    "\n",
    "### EECS 398: Practical Data Science, Winter 2025\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> • <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/wn25\">github.com/practicaldsc/wn25</a> • 📣 See latest announcements [**here on Ed**](https://edstem.org/us/courses/69737/discussion/5943734) </small>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504aee40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda 📆\n",
    "\n",
    "- Feature engineering.\n",
    "- Pipelines.\n",
    "- One hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3133e5e7-7dc6-4848-8612-ff683602363e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3728c17-c8ae-41d1-9ef4-5f6de0422bfe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Feature engineering** is the act of finding **transformations** that transform data into effective **quantitative variables**.<br><small>Put simply: feature engineering is creating new features using existing features. Our model can then fit different weights for each new feature we create.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b0bf19-64b4-4af9-902a-3af1a86274a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Example**: One hot encoding, in which we transform one column of categorical variables into several binary features.\n",
    "\n",
    "<center><img src=\"imgs/one-hot.png\" width=40%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d3027-ca40-411b-8feb-27f2945b49e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| **Transformation Type**    | **Purpose**                                                                                                                                 | **Example**                                                                                                                                                      | **`sklearn` Syntax**                                                                                         |\n",
    "|----------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|\n",
    "| **One-Hot Encoding**       | Converts a categorical variable with $N$ unique values into $N-1$ binary features. Each indicator is 1 if the observation is in that category (with one dropped as the baseline).  | **Before:** `conference = [\"ACC\", \"SEC\", \"Big Ten\"]`<br>**After:** `conference_SEC = [0, 1, 0]`, `conference_Big Ten = [0, 0, 1]`</br> We drop one column (\"ACC\") when one-hot encoding.            | `OneHotEncoder(drop='first')`                                                                                       |\n",
    "| **Polynomial Features**    | Expands a numerical variable by adding higher-order terms to capture non-linear relationships.                                              | **Before:** `seed = [1, 4, 2]`<br>**After:** `seed = [1, 4, 2]`, `seed^2 = [1, 16, 4]`, `seed^3 = [1, 64, 8]`                                                     | `PolynomialFeatures(degree=3, include_bias=False)`                                                                  |\n",
    "| **Standardization**        | Rescales features so that they have a mean of 0 and a standard deviation of 1, making them directly comparable.                               | **Before:** `seed = [1, 3, 2]`<br>**After:** `seed_std ≈ [-1.225, 1.225, 0]`  | `StandardScaler()`                                                                                                  |\n",
    "| **Function Transformation**| Applies a custom function to a feature, e.g., to bin continuous values into categories.                                                    | **Before:** `day_of_month = [5, 16, 22, 30]`<br>**After:** `day_bin = [\"early\", \"late\", \"late\", \"late\"]` </br> We would then one-hot encode this feature.                                 | `FunctionTransformer(lambda X: np.where(X <= 15, \"early\", \"late\").reshape(-1, 1))`                  |\n",
    "\n",
    "<center><small>Use this as a reference; don't try and memorize it.</small></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea891b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa42ba20",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A Pipeline in `sklearn` is a way to chain multiple feature engineering and model building steps together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af796d09-1312-483a-9584-71d7376e77e2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For example, let's build a Pipeline that predicts a team's March Madness `'tournament_wins'`.<br><small>You'll do something similar in Homework 8, which will be released soon.</small> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d51033",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"seed\": [1, 4, 2, 8, 3, 12],\n",
    "    \"conference\": [\"ACC\", \"Big Ten\", \"ACC\", \"SEC\", \"Big Ten\", \"American\"],\n",
    "    \"win_percentage\": [0.90, 0.75, 0.88, 0.65, 0.80, 0.9],\n",
    "    \"tournament_wins\": [6, 3, 5, 1, 4, 1]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98767546",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Specifically, we'll:\n",
    "    - One hot encode a team's `'conference'`.\n",
    "    - Create polynomial features out of the `'seed'` column.\n",
    "    <br><small>**Why might we want to add a polynomial feature to a seemingly linear column (`'seed'`)?** </br> Although seed values are integers representing rankings (with smaller numbers indicating better-ranked teams), their relationship with tournament wins may not be strictly linear. For instance, the jump in performance from a 1 seed to a 2 seed might be different from the jump from a 7 seed to an 8 seed. A polynomial transformation could help us capture this.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25823e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Constructing our Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af31172",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A Pipeline is made up of one or more transformers, followed (optionally) by an estimator.<br><small>Transformers, as we saw in the table a few slides ago, are used for creating features. Estimators are model objects, like `LinearRegression`.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c90618",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To create a Pipeline, either use the `Pipeline` constructor or the `make_pipeline` function. <br>Eventually, we will create our final Pipeline as follows:\n",
    "\n",
    "    ```python\n",
    "    model = make_pipeline(\n",
    "        SomeTransformer, # Doesn't exist yet!\n",
    "        LinearRegression()\n",
    "    )\n",
    "    \n",
    "    model.fit(X=df[['seed', 'conference', 'win_percentage']], y=df['tournament_wins'])\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a0b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140693e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To tell `sklearn` to perform different transformations on different columns, create a `ColumnTransformer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c23fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we one-hot encode the 'conference' column and drop the first category to avoid multicollinearity (which we'll talk about soon!)\n",
    "SomeTransformer = make_column_transformer(\n",
    "    (OneHotEncoder(drop='first'), ['conference']),\n",
    "    (PolynomialFeatures(degree=3, include_bias=False), ['seed']),\n",
    "    remainder='passthrough' # The remaining feature, 'win_percentage', is kept unchanged (the alternative is remainder='drop').\n",
    ")\n",
    "SomeTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2a557",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, we're ready to build our actual Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb1de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = make_pipeline(\n",
    "    SomeTransformer,\n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "model.fit(X=df[['seed', 'conference', 'win_percentage']], y=df['tournament_wins'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0719339",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using our Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c987c5-6850-42b1-b201-ec67d97cfe09",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- After fitting, we can print our model's optimal parameters and transformed features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473dd5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4828c6cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.named_steps # Useful to see what each individual step is named; these names are chosen automatically by the helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4722ebd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e6d3a-5a2c-4a71-bd11-16839060a1b8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Intercept:\", model.named_steps['linearregression'].intercept_)\n",
    "feature_names = model.named_steps['columntransformer'].get_feature_names_out()\n",
    "\n",
    "# Print each feature with its corresponding coefficient.\n",
    "for name, coef in zip(feature_names, model.named_steps['linearregression'].coef_):\n",
    "    print(f\"{name}: {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5559da-64df-4421-a454-5b5f9d816003",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Thus, our hypothesis function looks like:\n",
    "\n",
    "    <center>\n",
    "    <small>$$\n",
    "    \\text{pred. number of games won}_i = 6.935 \n",
    "    - 0.176\\cdot \\{\\text{conference}_i == \\text{American}\\}\n",
    "    + 0.022\\cdot \\{\\text{conference}_i == \\text{Big Ten}\\}\n",
    "    + 0.900\\cdot \\{\\text{conference}_i == \\text{SEC}\\}\n",
    "    - 0.884\\cdot \\text{seed}_i \n",
    "    - 0.056\\cdot \\text{seed}_i^2 \n",
    "    + 0.007\\cdot \\text{seed}_i^3 \n",
    "    - 0.003\\cdot \\text{win percentage}_i\n",
    "    $$</small></center>\n",
    "\n",
    "    <center><small>\n",
    "    \n",
    "    Notice how we don't have a one hot encoded parameter for the ACC `'conference'`, since we used `drop='first'` in our `OneHotEncoder`.\n",
    "    \n",
    "    </small></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955587ed-097d-436e-b692-a2f65f500bda",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Once our pipeline is fit, we can use it to make predictions.<br>For example, what's the predicted number of tournament wins for Michigan this year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54474cd1-17a4-44d3-bec1-48b50b868cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "michigan = pd.DataFrame({\n",
    "    'seed': [5],\n",
    "    'conference': ['Big Ten'],\n",
    "    'win_percentage': [0.735],\n",
    "})\n",
    "\n",
    "predicted_wins = model.predict(michigan)\n",
    "print(\"Predicted Tournament Wins:\", predicted_wins[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43816e9e-9196-4595-af34-02bbf88e20c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why do we drop one column when one hot encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50127dc3-dd6e-4136-bb8d-7960e718b994",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Consider what our design matrix, $X$, would look like if we **don't** drop the one hot encoded ACC column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a6b52d-92ec-4f5a-830a-1e1efc49baeb",
   "metadata": {},
   "source": [
    "$$\n",
    "X = \\left[\n",
    "\\begin{array}{ccccccccc}\n",
    "% Data rows:\n",
    "1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0.90 \\\\\n",
    "1 & 4 & 16 & 64 & 0 & 1 & 0 & 0 & 0.75 \\\\\n",
    "1 & 2 & 4 & 8 & 1 & 0 & 0 & 0 & 0.88 \\\\\n",
    "1 & 8 & 64 & 512 & 0 & 0 & 1 & 0 & 0.65 \\\\\n",
    "1 & 3 & 9 & 27 & 0 & 1 & 0 & 0 & 0.80 \\\\\n",
    "1 & 12 & 144 & 1728 & 0 & 0 & 0 & 1 & 0.90 \\\\\n",
    "% Spacing before label row:\n",
    "% Label row (must match the same 9 columns):\n",
    "\\underbrace{\\phantom{1}}_{\\text{intercept}} &\n",
    "\\underbrace{\\phantom{1}}_{\\text{seed}} &\n",
    "\\underbrace{\\phantom{1}}_{\\text{seed}^2} &\n",
    "\\underbrace{\\phantom{1}}_{\\text{seed}^3} &\n",
    "\\underbrace{\\phantom{1}}_{\\text{ACC}} &\n",
    "\\underbrace{\\phantom{1}}_{\\text{Big Ten}} &\n",
    "\\underbrace{\\phantom{1}}_{\\text{SEC}} &\n",
    "\\underbrace{\\phantom{1}}_{\\text{American}} &\n",
    "\\underbrace{\\phantom{0.90}}_{\\text{win percentage}}\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b33685-e5e1-46d5-b32b-6597df5a8751",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Notice that we can write our intercept column, $\\vec{1}$ as a linear combination of our one hot encoded columns: \n",
    "\n",
    "    $$\\vec{1} = \\vec{\\text{ACC}} + \\vec{\\text{Big Ten}} + \\vec{\\text{SEC}} + \\vec{\\text{American}}$$\n",
    "\n",
    "    This means there is **multicollinearity** present: one of our features is redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f557896e-706f-48f9-8139-70d124e66287",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The columns of $X$ are **not** linearly independent, so:\n",
    "    - $X$ is not full rank, so\n",
    "    - $X^TX$ is not full rank, so\n",
    "    - $X^TX$ isn't invertible, so\n",
    "    - there are infinitely many solutions to the normal equations,\n",
    "    \n",
    "    $$X^TX \\vec w = X^T \\vec y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad1e65f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Avoiding multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a8f12c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When there is multicollinearity present, we don't know which of the infinitely many solutions `sklearn` will give back to us. The resulting coefficients of the fit model are **uninterpretable**.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<small>\n",
    "$$\n",
    "\\text{pred. number of games won}_i = \\boxed{1.5} + \\boxed{2} \\cdot \\{\\text{conference}_i == \\text{ACC}\\} + \\boxed{1} \\cdot \\{\\text{conference}_i == \\text{Big Ten}\\} + \\boxed{4} \\cdot \\{\\text{conference}_i == \\text{SEC}\\} + \\boxed{0.5} \\cdot \\{\\text{conference}_i == \\text{American}\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{pred. number of games won}_i = \\boxed{100} - \\boxed{96.5} \\cdot \\{\\text{conference}_i == \\text{ACC}\\} - \\boxed{97.5} \\cdot \\{\\text{conference}_i == \\text{Big Ten}\\} - \\boxed{94.5} \\cdot \\{\\text{conference}_i == \\text{SEC}\\} - \\boxed{98} \\cdot \\{\\text{conference}_i == \\text{American}\\}\n",
    "$$\n",
    "</small>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db4a056",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To avoid multicollinearity and guarantee a **unique** solution for our optimal parameters, we drop one of the one hot encoded columns (typically using `OneHotEncoder(drop='first')`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382ae92c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Remember, multicollinearity doesn't impact a model's predictions!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfac177-ab80-4435-a3e0-8917436d3011",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attendance 🙋\n",
    "\n",
    "<center><img src='imgs/disc09.png' width=\"500\"></img></center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09ae71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <a href='https://study.practicaldsc.org/disc09/index.html'>Worksheet</a> 📝\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
