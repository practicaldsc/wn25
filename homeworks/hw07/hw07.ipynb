{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553addcb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw07.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46e06a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" markdown=\"1\">\n",
    "\n",
    "#### Homework 7\n",
    "\n",
    "# Multiple Linear Regression\n",
    "\n",
    "### EECS 398: Practical Data Science, Winter 2025\n",
    "\n",
    "#### Due Tuesday, March 18th at 11:59PM\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaaaac5",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Welcome to Homework 7! In this homework, you'll develop a deeper understanding of the inner workings of linear regression through the lens of linear algebra. The concepts in this homework are all _crucial_ to modern machine learning.\n",
    "\n",
    "You are given 8 slip days throughout the semester to extend deadlines. See the [Syllabus](https://practicaldsc.org/syllabus) for more details. With the exception of using slip days, late work will not be accepted unless you have made special arrangements with your instructor.\n",
    "\n",
    "To access this notebook, you'll need to clone our [public GitHub repository](https://github.com/practicaldsc/wn25/). The [Environment Setup](https://practicaldsc.org/env-setup) page on the course website walks you through the necessary steps.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "### Submission\n",
    "    \n",
    "This homework features a mix of autograded programming questions and manually-graded questions.\n",
    "    \n",
    "- Questions 1-2 are **manually graded**, and say **[Written ‚úèÔ∏è]** in their titles. For these questions, **do not write your answers in this notebook**! Instead, write **all** of your answers in a separate PDF. Submit this separate PDF to the **Homework 7 (Questions 1-2; written problems)** assignment on Gradescope, and **make sure to correctly select the pages associated with each question**! Make sure to show your work for all written questions, as answers without work shown may not receive full credit.\n",
    "  \n",
    "- Questions 3-5 are **fully autograded**, and each part will say **[Autograded üíª]** in the title. For these questions, all you need to is write your code in this notebook, run the local `grader.check` tests, and submit to the **Homework 7 (Questions 3-5; autograded problems)** assignment on Gradescope to have your code graded by the autograder.\n",
    "\n",
    "    \n",
    "Your Homework 7 submission time will be the **later** of your two individual submissions. Please start early and submit often. You can submit as many times as you'd like to Gradescope, and we'll take your **most recent** submission. \n",
    "</div>\n",
    "</div>\n",
    "\n",
    "This homework is worth a total of **60 points**, 37 of which come from the autograder (Question 3-5) and 23 which are manually graded by us (Questions 1-2). The number of points each question is worth is listed at the start of each question. **All questions in the assignment are independent, so feel free to move around if you get stuck**. Tip: if you're using Jupyter Lab, you can see a Table of Contents for the notebook by going to View > Table of Contents. You can also view a static version of this homework notebook [**at this link**](https://practicaldsc.org/resources/homeworks/hw07/hw07.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d7b924",
   "metadata": {},
   "source": [
    "To get started, run the cell below, plus the cell at the top of the notebook that imports and initializes `otter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c03934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "# Preferred styles\n",
    "pio.templates[\"pds\"] = go.layout.Template(\n",
    "    layout=dict(\n",
    "        margin=dict(l=30, r=30, t=30, b=30),\n",
    "        autosize=True,\n",
    "        width=600,\n",
    "        height=400,\n",
    "        xaxis=dict(showgrid=True),\n",
    "        yaxis=dict(showgrid=True),\n",
    "        title=dict(x=0.5, xanchor=\"center\")\n",
    "    )\n",
    ")\n",
    "pio.templates.default = \"simple_white+pds\"\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e022df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Question 1: Correlation Bounds üìà\n",
    "\n",
    "---\n",
    "\n",
    "In class, you were told that the correlation coefficient, $r$, ranges between $-1$ and $1$, where $r = -1$ implies a perfect negative linear association and $r = 1$ implies a perfect positive linear association. However, you were never given a proof of the fact that $-1 \\leq r \\leq 1$.\n",
    "\n",
    "Here, you will prove this fact, using linear algebra. Before proceeding, you'll want to review \n",
    "[slide 47 onwards in Lecture 12](https://practicaldsc.org/resources/lectures/lec12/lec12-filled.pdf#page=47).\n",
    "**Remember to show your work all throughout!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de5e3ef",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1.1 [Written ‚úèÔ∏è] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "Let $\\vec{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\\end{bmatrix}$. We define the \"mean-centered\" version of $\\vec{x}$ to be $\\vec{x_c} = \\begin{bmatrix} x_1 - \\bar{x} \\\\ x_2 - \\bar{x} \\\\ \\vdots \\\\ x_n - \\bar{x}\\end{bmatrix}$, where $\\bar{x}$ is the mean of the components of $\\vec{x}$.\n",
    "\n",
    "The mean-centered version of $\\vec{y}$, named $\\vec{y_c}$, is defined similarly. Express $\\vec{x_c} \\cdot \\vec{y_c}$ using summation notation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cece9a9",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1.2 [Written ‚úèÔ∏è] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">3 Points</div>\n",
    "Prove that: $$r=\\frac{\\vec{x_c}\\cdot \\vec{y_c}}{\\lVert \\vec{x_c} \\rVert \\lVert \\vec{y_c} \\rVert}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd7974",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1.3 [Written ‚úèÔ∏è] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div> \n",
    "**Explain** why the result in Question 1.2 implies that $-1 \\leq r \\leq 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e659e823",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 2: Same, but Different üò±\n",
    "\n",
    "---\n",
    "\n",
    "In Lecture 12, we were introduced to one of many formulas for the optimal slope, $w_1^*$, and optimal intercept, $w_0^*$, for the simple linear regression model $H(x_i) = w_0 + w_1 x_i$ when using squared loss:\n",
    "\n",
    "$$w_1^* = r \\frac{\\sigma_{y}}{\\sigma_{x}} \\qquad w_0^* = \\bar y - w_1^* \\bar x$$\n",
    "\n",
    "Then, in Lectures 14 and 15, we revisited the simple linear regression model in terms of linear algebra. When $X \\in \\mathbb{R}^{n \\times 2}$ is the design matrix, $\\vec{y} \\in \\mathbb{R}^n$ is the observation vector, and $\\vec{w} \\in \\mathbb{R}^2$ is the parameter vector, we found that the optimal parameter vector $\\vec{w}^*$ is one that satisfies the normal equations:\n",
    "\n",
    "$$X^TX \\vec{w} = X^T \\vec y$$\n",
    "\n",
    "When $X^TX$ is invertible, $\\vec{w}^*$ can be expressed as:\n",
    "\n",
    "$$\\vec{w}^* = (X^TX)^{-1}X^T \\vec y$$\n",
    "\n",
    "In this problem, we will prove that both of these formulations are equivalent, for any dataset $(x_1, y_1)$, $(x_2, y_2)$, ..., $(x_n, y_n)$. Specifically, we'll show that the vector $\\vec{w}^* = (X^TX)^{-1}X^Ty$ has two components, the first of which is $w_0^*$ and the second of which is $w_1^*$. (To do this, we'll need to assume that $(X^TX)^{-1}$ is invertible, which it is in this setting.)\n",
    "\n",
    "Note that on first glance, it looks like this problem is quite long, since it has seven subparts. However, the subparts are meant to guide you through the proof. The problem would take much longer if we just said \"prove it!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b43fba",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.1 [Written ‚úèÔ∏è] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "Throughout this question, assume $X$ refers to the $\\mathbb{R}^{n \\times 2}$ design matrix and $\\vec y \\in \\mathbb{R}^n$ refers to the observation vector, as defined on [slide 29 of Lecture 14](https://practicaldsc.org/resources/lectures/lec14/lec14-filled.pdf#page=29).\n",
    "\n",
    "Express the vector $X^T \\vec y$ using constants and/or summations involving $x_i$ and/or $y_i$. Make sure that your answer has the correct dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102f24eb",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.2 [Written ‚úèÔ∏è] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "Express the matrix $X^TX$ using constants and/or summations involving $x_i$ and/or $y_i$. Make sure that your answer has the correct dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a33ca8",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.3 [Written ‚úèÔ∏è] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "Recall, if $M = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$ is a $2 \\times 2$ matrix, then the inverse of $M$ is given by:\n",
    "\n",
    "$$M^{-1} = \\frac{1}{ad - bc} \\begin{bmatrix} \n",
    " d & -b \\\\ -c & a\\end{bmatrix}$$\n",
    "\n",
    "Express the matrix $(X^TX)^{-1}$ using constants and/or summations involving $x_i$ and/or $y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e5f59",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.4 [Written ‚úèÔ∏è] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "At this point, the expressions you have for $(X^TX)^{-1}$ and $X^Ty$ likely involve many summation notations and look... complicated. Let's take a step back and simplify things before we proceed.\n",
    "\n",
    "Prove that:\n",
    "\n",
    "$$\\sum_{i = 1}^n x_i^2 = n \\sigma_x^2 + n \\bar{x}^2$$\n",
    "\n",
    "<!-- % $n^2 \\sigma_x^2 = n \\sum_{i = 1}^n x_i^2 - n^2 \\bar{x}^2$ -->\n",
    "\n",
    "where $\\bar{x}$ and $\\sigma_x$ are the mean and standard deviation of $x_1, x_2, ..., x_n$, respectively.\n",
    "\n",
    "Some guidance: Start with the definition of $\\sigma_x^2 = \\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x})^2$ and expand the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba6f9f7",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.5 [Written ‚úèÔ∏è] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "Using your work in Questions 2.3 and 2.4, prove that:\n",
    "\n",
    "$$(X^TX)^{-1} = \\frac{1}{n\\sigma_x^2} \\begin{bmatrix} \\sigma_x^2 + \\bar{x}^2 & -\\bar{x} \\\\ -\\bar{x} & 1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a3c5e",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.6 [Written ‚úèÔ∏è] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "$(X^TX)^{-1}$ is about as simplified as it can be for now. But, before we multiply $(X^TX)^{-1}$ and $X^Ty$, we should deal with the fact that at least one of the components in the vector $X^T \\vec{y}$ still involves a summation.\n",
    "\n",
    "Prove that:\n",
    "\n",
    "$$\\sum_{i=1}^n x_i y_i = nr \\sigma_x \\sigma_y + n \\bar{x}\\bar{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434056cc",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2.7 [Written ‚úèÔ∏è] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">4 Points</div>\n",
    "Now, put it all together. That is, prove that:\n",
    "\n",
    "$$(X^TX)^{-1}X^T \\vec{y} = \\begin{bmatrix} \\bar{y} - r \\frac{\\sigma_y}{\\sigma_x} \\bar{x} \\\\ r \\frac{\\sigma_y}{\\sigma_x}  \\end{bmatrix}$$\n",
    "\n",
    "Note that the second component of the vector above is $w_1^* =  r \\frac{\\sigma_y}{\\sigma_x}$ and the first component of the vector above is $w_0^* = \\bar{y} -  r \\frac{\\sigma_y}{\\sigma_x} \\bar{x} = \\bar{y} - w_1^* \\bar{x}$, as we first saw in Lecture 12! This concludes our proof that both formulations of the optimal parameters of the simple linear regression model are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb66251",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## Question 3: Simple LADs üßç\n",
    "\n",
    "---\n",
    "\n",
    "In lecture, we explored simple linear regression, and defined it as the problem of finding the values of $w_0$ (intercept) and $w_1$ (slope) that minimize mean squared error:\n",
    "\\begin{align*}\n",
    "R_{\\text{sq}}(w_0, w_1) &= \\frac{1}{n} \\sum_{i=1}^{n} (y_i -(w_0 + w_1x_i))^2\n",
    "\\end{align*}\n",
    "\n",
    "The optimal slope and intercept were denoted $w_1^*$ and $w_0^*$, respectively, and have closed-form solutions that we derived in lecture (and even restated in Question 2: Same, but Different üò±). When using squared loss to find our optimal parameters, linear regression is often called \"least squares regression.\" \n",
    "\n",
    "**What if we used a different loss function instead?**\n",
    "\n",
    "In this question, we'll implement another type of linear regression: simple least absolute deviation (LAD) regression. LAD regression uses absolute loss to measure the quality of predictions, rather than squared loss. Put another way, to find the optimal slope $w_1^*$ and intercept $w_0^*$ for LAD regression, we minimize mean absolute error:\n",
    "\n",
    "\\begin{align*}\n",
    "R_{\\text{abs}}(w_0, w_1) &= \\frac{1}{n} \\sum_{i=1}^{n} |y_i -(w_0 + w_1x_i)|\n",
    "\\end{align*}\n",
    "\n",
    "The \"simple\" in \"simple LAD\" refers to the fact that our hypothesis function $H(x_i) = w_0 + w_1 x_i$, like in regular simple linear regression, only uses a single input feature.\n",
    "\n",
    "Since absolute value functions are not differentiable, we cannot just take partial derivatives of $R_{\\text{abs}}$ with respect to $w_0$ and $w_1$, set them equal to zero, and solve for the values of $w_0$ and $w_1$, as we did to minimize $R_{\\text{sq}}$.\n",
    "\n",
    "In order to generate the optimal LAD regression line we are going to leverage the following theorem (which, luckily, we won't need to prove):\n",
    "\n",
    "> The regression model that minimizes mean absolute error passes directly through at least $k$ points, where $k$ is the number of parameters of the model.\n",
    "\n",
    "This theorem is useful to us because it allows us to adopt a very conceptually simple, albeit not very efficient, strategy to compute an optimal simple LAD regression line. Since our hypothesis function has $k = 2$ parameters, an intercept $w_0$ and a slope $w_1$, we can simply:\n",
    "\n",
    "1. Generate all possible pairs of 2 points. We know that the optimal LAD line will pass through at least one of these pairs.\n",
    "1. For each pair of points:\n",
    "    1. Find the equation of the line that passes through the pair. Denote the intercept and slope of this line $w_0$ and $w_1$, respectively.\n",
    "    1. Compute the mean absolute error of the line with intercept $w_0$ and slope $w_1$, i.e. compute $R_\\text{abs}(w_0, w_1)$.\n",
    "1. Return the $(w_0, w_1)$ combination with the minimum value of $R_\\text{abs}(w_0, w_1)$. By the above theorem, this line is guaranteed to minimize mean absolute error.\n",
    "\n",
    "Notice that unlike with simple linear regression, the optimal simple LAD regression line may not be unique!\n",
    "\n",
    "In this question, you will ultimately complete the implementation of the `SimpleLAD` **class**, which can be used as follows:\n",
    "\n",
    "```python\n",
    ">>> model = SimpleLAD()\n",
    ">>> model.fit([1, 2, -1, 4], [15, 6, 7, 8])\n",
    ">>> model.intercept_\n",
    "7.2\n",
    ">>> model.coef_\n",
    "0.2\n",
    ">>> model.predict(5)\n",
    "8.2\n",
    ">>> model.predict([5, -3.5, 5])\n",
    "array([8.2, 6.5, 8.2])\n",
    "```\n",
    "\n",
    "But first, we'll have you implement several standalone helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e567cf5d",
   "metadata": {},
   "source": [
    "### Question 3.1 [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "Complete the implementation of the function `generate_pairs`, which takes in two 1D lists/arrays, `x` and `y`, where the points in our dataset are `(x[0], y[0])`, `(x[1], y[1])`, and so on. `generate_pairs` should return a **list** containing all unique **pairs** of points in `x` and `y`. Each pair in the returned list should be a tuple of tuples.\n",
    "\n",
    "Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> generate_pairs([1, 2, -1, 4], [15, 6, 7, 8])\n",
    "[((1, 15), (2, 6)),\n",
    " ((1, 15), (-1, 7)),\n",
    " ((1, 15), (4, 8)),\n",
    " ((2, 6), (-1, 7)),\n",
    " ((2, 6), (4, 8)),\n",
    " ((-1, 7), (4, 8))]\n",
    "```\n",
    "\n",
    "For more context on the example above:\n",
    "- There are four points in the dataset: $(1, 15), (2, 6), (-1, 7), (4, 8)$. In Python, we represent each point as a tuple.\n",
    "- There are 6 pairs of points:\n",
    "\n",
    "    $$(1, 15) \\text { and } (2, 6)$$$$(1, 15) \\text { and } (-1, 7)$$$$(1, 15) \\text { and } (4, 8)$$$$(2, 6) \\text{ and } (-1, 7)$$$$(2, 6) \\texttt{ and } (4, 8)$$$$(-1, 7) \\text { and } (4, 8)$$\n",
    "    \n",
    "    Note that we don't consider $(2, 6) \\text{ and } (1, 15)$ to be a different pair than $(1, 15) \\text { and } (2, 6)$. That is, order does not matter.\n",
    "    \n",
    "- We represent each pair as a tuple, e.g. the first pair in the list above is represented as `((1, 15), (2, 6))` (or `((2, 6), (1, 15))`; either is fine, but not both).\n",
    "- The returned list contains 6 tuples of tuples.\n",
    "\n",
    "You may assume that `len(x) == len(y) >= 2`. The order in which the resulting pairs appear in the returned list does not matter. If there are duplicated points, there may be duplicated pairs, and that's to be expected:\n",
    "\n",
    "```python\n",
    ">>> generate_pairs([1, 1, 1], [1, 1, 1])\n",
    "[((1, 1), (1, 1)), ((1, 1), (1, 1)), ((1, 1), (1, 1))]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48427bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is a big hint! Using this, our solution only took 2 lines,\n",
    "# and didn't use any for-loops or list comprehensions (but yours can).\n",
    "from itertools import combinations \n",
    "\n",
    "def generate_pairs(x, y):\n",
    "    ...\n",
    "\n",
    "# Feel free to change this input to make sure your function works correctly.\n",
    "generate_pairs([1, 2, -1, 4], [15, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c724589",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q03_01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a4a52",
   "metadata": {},
   "source": [
    "### Question 3.2 [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">3 Points</div>\n",
    "\n",
    "Complete the implementation of the function `generate_lines`. \n",
    "- `generate_lines` takes in a list, `pairs`, in which each element is a tuple. Each tuple is itself made up of two tuples, corresponding to a pair of points. The input to `generate_lines` may look like:\n",
    "\n",
    "```python\n",
    "    [((1, 2), (3, 7)), ((1, 10), (-4, 20))]\n",
    "```\n",
    "\n",
    "- `generate_lines` returns a list with the same length as `pairs`, in which each element is a tuple of the form `(intercept, slope)`. Element `i` of the returned list should be a tuple containing the intercept and slope of the line passing through the two points in `pairs[i]` (the order of the outputted lines should be the same as the order of the inputted pairs).\n",
    "\n",
    "Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> generate_lines([((1, 2), (3, 7)), ((1, 10), (-4, 20))])\n",
    "[(-0.5, 2.5), (12.0, -2.0)]\n",
    "```\n",
    "\n",
    "For more context on the example above:\n",
    "- The input to `generate_lines` contains two pairs of points.\n",
    "- The first pair of points, $(1, 2) \\text{ and } (3, 7)$, sit on the line $y = -0.5 + 2.5x$. The intercept of this line is -0.5 and the slope is 2.5, so the first returned tuple is `(-0.5, 2.5)`.\n",
    "- The second pair of points, $(1, 10) \\text{ and } (-4, 20)$, sit on the line $y = 12 - 2x$. The intercept of this line is 12 and the slope is -2, so the second returned tuple is `(12.0, -2.0)`.\n",
    "\n",
    "\n",
    "Some guidance:\n",
    "- A fact from high school algebra is that given any two points, there is exactly one line that passes through them. You'll need to figure out how to programmatically find the intercept and slope of this line, given any two arbitrary points $(x_1, y_1)$ and $(x_2, y_2)$.\n",
    "- There is theoretically the risk of a `DivisionByZero` error, if a pair of points contains two values with the same $x$-coordinate. We won't test your code on such examples.\n",
    "- You don't have to manually convert the values in the output tuples to floats ‚Äì this will likely happen automatically because your calculations will involve division, and if it doesn't, don't worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc552ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_lines(pairs):\n",
    "    ...\n",
    "    \n",
    "# Feel free to change this input to make sure your function works correctly.\n",
    "generate_lines([((1, 2), (3, 7)), ((1, 10), (-4, 20))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ec374",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q03_02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e181ff9",
   "metadata": {},
   "source": [
    "### Question 3.3 [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">3 Points</div>\n",
    "\n",
    "Complete the implementation of the function `mae_of_candidate_line`, which takes in four inputs:\n",
    "- `intercept`, a float,\n",
    "- `slope`, a float,\n",
    "- `x`, a 1D list/array of numbers, and\n",
    "- `y`, a 1D list/array of numbers.\n",
    "\n",
    "`mae_of_candidate_line` should return the mean absolute error from using the line with intercept `intercept` and slope `slope` to predict `y` from `x`.\n",
    "\n",
    "Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> mae_of_candidate_line(5, 2, [1, 2, -1, 4], [15, 6, 7, 8])\n",
    "5.0\n",
    "```\n",
    "\n",
    "For more context on the example above:\n",
    "\n",
    "- There are four points in the dataset provided: $(1, 15)$, $(2, 6)$, $(-1, 7)$, and $(4, 8)$.\n",
    "- The line we're using to make predictions is $H(x_i) = 5 + 2x_i$. This line, and the four points above, are visualized below:\n",
    "\n",
    "<center><img src=\"imgs/mae-example.png\" width=400></center>\n",
    "\n",
    "- The absolute errors of the line's predictions are 4, 8, 3, and 5. So, the mean of absolute errors is $\\frac{4+8+3+5}{4} = 5$.\n",
    "\n",
    "Don't use a `for`-loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e28127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mae_of_candidate_line(intercept, slope, x, y):\n",
    "    ...\n",
    "\n",
    "# Feel free to change this input to make sure your function works correctly.  \n",
    "mae_of_candidate_line(5, 2, [1, 2, -1, 4], [15, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1273edb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q03_03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a087ce",
   "metadata": {},
   "source": [
    "### Question 3.4 [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">5 Points</div>\n",
    "\n",
    "Now, put it all together. Complete the implementation of the `SimpleLAD` class, which has two methods, apart from the constructor.\n",
    "\n",
    "#### `fit`\n",
    "\n",
    "`fit` takes in two* 1D list/arrays, `x` and `y`. Using the previously-defined helper functions, `fit` determines the intercept and slope that minimize mean absolute error on the dataset defined by `x` and `y`.\n",
    "                \n",
    "`fit` should not return anything, but should instead set the values of `self.intercept_` (the optimal intercept) and `self.coef_` (the optimal slope; we use the attribute name `coef_` instead of `slope_` to match `sklearn`'s naming conventions).\n",
    "\n",
    "If there are multiple optimal combinations of intercepts and slopes, set `self.intercept_` and `self.slope_` to any one of those combinations.\n",
    "\n",
    "*As you'll see in the method stub, `fit` takes in a third argument (at the start), named `self`. The role of the `self` argument is to be able to access attributes and methods of the current instance of the `SimpleLAD` class. Read [this article](https://www.geeksforgeeks.org/self-in-python-class/) for more information on the role of the `self` argument.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `predict`\n",
    "\n",
    "`predict` takes in a single (non-`self`) input, named `x_new`, which can either be a single value or list/array of values.\n",
    "\n",
    "- If `x_new` is a single value, `predict` should return a single value, corresponding to the predicted $y$-value for the passed in $x$-value, using the already-found `self.intercept_` and `self.coef_`.\n",
    "- If `x_new` is a list or array, `predict` should return an **array** corresponding to the predict $y$-values for the passed in $x$-values, using the already-found `self.intercept_` and `self.coef_`.\n",
    "\n",
    "`fit` must be called before `predict`; if not, raise an `AttributeError`.\n",
    "\n",
    "<br>\n",
    "\n",
    "Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> model = SimpleLAD()\n",
    ">>> model.fit([1, 2, -1, 4], [15, 6, 7, 8])\n",
    ">>> model.intercept_\n",
    "7.2\n",
    ">>> model.coef_\n",
    "0.2\n",
    ">>> model.predict(5)\n",
    "8.2\n",
    ">>> model.predict([5, -3.5, 5])\n",
    "array([8.2, 6.5, 8.2])\n",
    "```\n",
    "\n",
    "For more context on the example above:\n",
    "\n",
    "- There are four points in the dataset provided: $(1, 15)$, $(2, 6)$, $(-1, 7)$, and $(4, 8)$.\n",
    "- The helper functions `generate_pairs`, `generate_lines`, and `mae_of_candidate_line` helped us deduce that the line with the minimum mean absolute error on this dataset is $H^*(x_i) = 7.2 + 0.2x_i$, so `model.intercept_` is `7.2` and `model.coef_` is `0.2`.\n",
    "- Using the fit hypothesis function $H^*(x_i) = 7.2 + 0.2x_i$ on the inputs 5, -3.5, and 5 give us the predictions $H(5) = 8.2$, $H(-3.5) = 6.5$, and $H(5) = 8.2$, so we return an array with those three values. (Note that we return an array even though the inputs were provided as a list.) When using this hypothesis function on the single input 5, we return just the value $H(5) = 8.2$, not as an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f6b18a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleLAD:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        __init__ is the name given to the constructor method in a Python class.\n",
    "        We don't need to do anything to initialize a SimpleLAD object, so this constructor\n",
    "        doesn't actually do anything.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        if len(x) != len(y):\n",
    "            raise ValueError(f'Dimension mismatch: x has length {len(x)} while y has length {len(y)}')\n",
    "            \n",
    "        ...\n",
    "        \n",
    "        # The last two lines in the body of `fit` should be the two below.\n",
    "        self.intercept_ = ...\n",
    "        self.coef_ = ...\n",
    "        \n",
    "    def predict(self, x_new):\n",
    "        if isinstance(x_new, list):\n",
    "            x_new = np.array(x_new)\n",
    "        try:\n",
    "            ...\n",
    "        except AttributeError:\n",
    "            raise AttributeError('Cannot use `predict` before `fit`.')\n",
    "            \n",
    "            \n",
    "# Feel free to change the inputs below to make sure your class implementation works correctly.\n",
    "model = SimpleLAD()\n",
    "model.fit([1, 2, -1, 4], [15, 6, 7, 8])\n",
    "preds = model.predict([5, -3.5, 5])\n",
    "print(f'''\n",
    "model.intercept_ = {model.intercept_}\n",
    "model.coef_ = {model.coef_}\n",
    "model.predict([5, -3.5, 5]) = {preds}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc6dd8c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q03_04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd573f8",
   "metadata": {},
   "source": [
    "Now that our implementation of `SimpleLAD` is complete, we can use it to fit real datasets! Run the cell below to load in a DataFrame with two columns, `'x'` and `'y'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9e8643",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_lad = pd.read_csv('data/data-for-lad.csv')\n",
    "data_for_lad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c003c98c",
   "metadata": {},
   "source": [
    "Run the cell below to draw a scatter plot of `'y'` vs. `'x'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52befb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(data_for_lad, x='x', y='y', color_discrete_sequence=['#444']).update_layout(width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa39019",
   "metadata": {},
   "source": [
    "There's a clear linear association at the bottom, with some outliers spread throughout the top. Let's see how the best-fitting lines look on this dataset, when the lines are chosen by minimizing mean squared error vs. mean absolute error.\n",
    "\n",
    "First, we'll find the standard simple linear regression line, i.e. the one that minimizes mean squared error. We'll use `sklearn` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b686ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b21d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse = LinearRegression()\n",
    "model_mse.fit(X=data_for_lad[['x']], y=data_for_lad['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4596b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An array with one optimal parameter.\n",
    "# sklearn's LinearRegression supports multiple regression, meaning it stores\n",
    "# the coef_ attribute in a way that is flexible enough to hold multiple slope parameters.\n",
    "model_mse.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8246998e",
   "metadata": {},
   "source": [
    "Now, let's compute the least absolute deviations line, i.e. the one that minimizes mean absolute error. **This is where your hard work comes in!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c9c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lad = SimpleLAD()\n",
    "model_lad.fit(data_for_lad['x'].to_numpy(), data_for_lad['y'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a0873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lad.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2dbb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lad.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786cc953",
   "metadata": {},
   "source": [
    "Let's graph both of these lines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e66616",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(data_for_lad, x='x', y='y', color_discrete_sequence=['#888']).update_layout(width=800, height=400)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[-1, 11],\n",
    "        y=model_mse.predict([[-1], [11]]),\n",
    "        mode='lines',\n",
    "        name='Best Line with Minimizing MSE',\n",
    "        line={'color': '#00274C'}\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[-1, 11],\n",
    "        y=model_lad.predict([-1, 11]),\n",
    "        mode='lines',\n",
    "        name='Best Line when Minimizing MAE',\n",
    "        line={'color': '#FFCB05'}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b95fc",
   "metadata": {},
   "source": [
    "What do you notice? There's nothing you need to write or comment on here, but you should think about what makes the lines appear so different, and **why** this is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb3fba4",
   "metadata": {},
   "source": [
    "### Question 3.5 [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "We've built a na√Øve implementation of simple LAD (least absolute deviations) regression. Suppose $n$ is the number of points in the dataset that we fit a `SimpleLAD` object on. Which of the following most accurately describe the runtime of `SimpleLAD.fit`, in Big-O notation? Assign `naive_lad_runtime` to an integer between 1 and 8, inclusive, corresponding to your answer among the choices below.\n",
    "\n",
    "1. $O(1)$\n",
    "2. $O(n)$\n",
    "3. $O(n^2)$\n",
    "4. $O(n^3)$\n",
    "5. $O(\\log n)$\n",
    "6. $O(n \\log n)$\n",
    "7. $O(n!)$\n",
    "8. $O(2^n)$\n",
    "\n",
    "_Hint: When computing the theoretical runtime of an algorithm, it doesn't matter which language or package an operation is implemented in ‚Äì a fast `numpy` vectorized operation still involves a loop!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b45c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "naive_lad_runtime = ...\n",
    "naive_lad_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9338e12a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q03_05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1368c208",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Question 4: Interpol...ation üöî [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">6 Points</div>\n",
    "\n",
    "---\n",
    "\n",
    "So far in this class, our primary tool for making predictions has been linear regression ‚Äì that is, a straight line, or if using two features, a plane. However, one potential issue with linear regression, depending on your use case, is that a straight line doesn't pass through each data point, leading to prediction errors.\n",
    "\n",
    "<center><img src=\"imgs/model.png\" width=500></center>\n",
    "\n",
    "In this question, we will explore the idea of **polynomial interpolation**, which is a method of constructing a polynomial that passes directly through a given set of points. Interpolation is widely used in numerical analysis, a subfield of mathematics that deals with approximating solutions to equations that (often) don't have solutions that we can solve for by hand, often by writing code.\n",
    "\n",
    "The specific method for interpolation we'll study in this problem is called Lagrange Interpolation. It solves the following problem:\n",
    "\n",
    "> Given a set of $n+1$ points, $(x_1, y_1), (x_2, y_2), ..., (x_{n+1}, y_{n+1})$, what is the equation of the degree $n$ polynomial that passes through all $n+1$ points?\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "To get started, read [**this guide**](https://practicaldsc.org/guides/machine-learning/interpolation) we've written about the method of maximum likelihood estimation. Think of it as an extension of the homework spec.\n",
    "    \n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Your job** is to complete the implementation of the function `interpolate`.\n",
    "- `interpolate` takes in a list of tuples, `points`, corresponding to points $(x_1, y_1), (x_2, y_2), ..., (x_{n+1}, y_{n+1})$.\n",
    "- `interpolate` returns a **function**, that:\n",
    "    - takes in a single number, $x$, and\n",
    "    - returns the output of passing $x$ into the polynomial that **interpolates** the points in `points`.\n",
    "\n",
    "Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> f = interpolate([(1, 3), (3, 19), (4, 33)])\n",
    ">>> type(f)\n",
    "function\n",
    ">>> f(1)\n",
    "3.0\n",
    ">>> f(3)\n",
    "19.0\n",
    ">>> f(100)\n",
    "20001\n",
    "```\n",
    "\n",
    "For more context on the example above:\n",
    "\n",
    "- As discussed in the linked guide, the polynomial that interpolates the points $(1, 3)$, $(3, 19)$, and $(4, 33)$ is $p(x) = 1 + 2x^2$.\n",
    "- So, `interpolate([(1, 3), (3, 19), (4, 33)])` returns a function that takes in an `x` and outputs `1 + 2 * (x ** 2)`.\n",
    "- This expression, `1 + 2 * (x ** 2)`, is not hard-coded anywhere. Rather, `interpolate` creates a function using the Lagrange interpolation process outlined in the guide.\n",
    "\n",
    "Some guidance:\n",
    "- The bulk of your work is in figuring out how to implement the basis polynomials in code. Feel free to use `for`-loops and list comprehensions as necessary.\n",
    "- For context, the body of `f` in our solution only had 6 lines of code, and we added a few lines before `def f...`.\n",
    "- Don't worry about cases where two points have the same $x$-coordinate ‚Äì as mentioned in the guide, in Lagrange interpolation, we assume there are no duplicate $x_i$s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81257405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def interpolate(points):\n",
    "    def f(x):\n",
    "        ...\n",
    "    return f\n",
    "\n",
    "# Feel free to change this input to make sure your function works correctly.\n",
    "f = interpolate([(1, 3), (3, 19), (4, 33)])\n",
    "f(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff62c3b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ab5a8",
   "metadata": {},
   "source": [
    "Now that you've implemented `interpolate`, let's implement a function that interpolates a set of points and then plots the resulting polynomial. We've done this for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8faf71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_and_plot(points, x_min=None, x_max=None):\n",
    "    f = interpolate(points)\n",
    "    xs = [point[0] for point in points]\n",
    "    ys = [point[1] for point in points]\n",
    "    if x_min == None:\n",
    "        x_min = min(xs) - 1\n",
    "    if x_max == None:\n",
    "        x_max = max(xs) + 1\n",
    "    x_range = np.linspace(x_min, x_max, 10000)\n",
    "    outs = [f(xi) for xi in x_range]\n",
    "    \n",
    "    fig = px.scatter(x=xs, y=ys, size=[1] * len(xs), size_max=15, title='Original Data and Interpolated Polynomial')\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_range,\n",
    "            y=outs,\n",
    "            mode='lines',\n",
    "            name='Interpolated Polynomial',\n",
    "            line={'color': 'red'}\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "interpolate_and_plot([(0, -1), (1, 0), (2, -11), (3, 2), (4, 99)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dc7a30",
   "metadata": {},
   "source": [
    "Let's look at an even more interesting example: the commute times dataset from lecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27449d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "commutes = pd.read_csv('data/commute-times.csv')\n",
    "commutes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2544c0c2",
   "metadata": {},
   "source": [
    "As in lecture, let's let our $x$ variable be `'departure_hour'` and $y$ be `'minutes'`, representing the length of our commute to school in minutes.\n",
    "\n",
    "Unfortunately, the `'departure_hour'` column isn't unique, meaning that there are duplicated $x_i$s, which would cause Lagrange interpolation to fail (think about why!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b0b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "commutes['departure_hour'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaca072",
   "metadata": {},
   "source": [
    "So, let's keep just the first instance of each unique $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d04738",
   "metadata": {},
   "outputs": [],
   "source": [
    "commutes = commutes.groupby('departure_hour').first().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ccb612",
   "metadata": {},
   "source": [
    "Now, we can take the `'departure_hour'` and `'minutes'` columns and create an $(x_i, y_i)$ point out of each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "departure_hours = commutes['departure_hour'].to_numpy()\n",
    "minutes = commutes['minutes'].to_numpy()\n",
    "as_points = list(zip(departure_hours, minutes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2476e5c",
   "metadata": {},
   "source": [
    "Finally, let's call our freshly-minted `interpolate_and_plot` on `as_points`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c01009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate_and_plot(as_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d7101",
   "metadata": {},
   "source": [
    "Pay close attention to the numbers on the $y$-axis. What do they mean?\n",
    "\n",
    "Let's zoom in closer to the middle of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0174ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate_and_plot([point for point in as_points if 7.9 <= point[0] <= 8.1], x_min=7.9, x_max=8.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88b93f",
   "metadata": {},
   "source": [
    "Given these outputs, here's something to think about.\n",
    "\n",
    "The process of Lagrange Interpolation finds a polynomial $p(x)$ with a mean squared error of 0, which is generally much lower than the mean squared error of the simple linear regression line, $H^*(x)$, on a given dataset (unless the dataset consists of points that all fall on a straight line). Why isn't Lagrange Interpolation used very often in the context of finding hypothesis functions to use for prediction, and why do we prefer empirical risk minimization in general?\n",
    "\n",
    "You don't have to write your answer anywhere, because Question 4 is entirely autograded, but it's important that you understand the nature of _how_ interpolation works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a43686b",
   "metadata": {},
   "source": [
    "## Question 5: Billy the Waiter üßë‚Äçüç≥\n",
    "\n",
    "---\n",
    "\n",
    "Run the cell below to load in a dataset containing information about the tips Billy received over the last month as a waiter at Mani Osteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552fe3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = px.data.tips().rename(columns={'size': 'table_size'}).replace('Fri', 'Thur')\n",
    "tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b29adb",
   "metadata": {},
   "source": [
    "Each row corresponds to a single table that he served. Throughout this question, our goal will be to predict `'tip'` using some or all of the other features in the DataFrame. We will do so by implementing all aspects of the linear regression model-building process **manually using `numpy`, i.e. WITHOUT using `sklearn` or other machine learning packages**.\n",
    "\n",
    "Let's start by just using `'total_bill'` to predict `tip`. Here's a scatter plot showing the relationship between the two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f589ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.plot(kind='scatter', x='total_bill', y='tip', title='Total Bill vs. Tip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e18169",
   "metadata": {},
   "source": [
    "Before we get started actually making predictions, we'll need to implement several helper functions. We've defined two for you; read their implementations to make sure you understand what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_normal_equations(X, y):\n",
    "    '''\n",
    "    Equivalent to returning np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    when X.T @ X is invertible, but more efficient and numerically stable.\n",
    "    '''\n",
    "    return np.linalg.solve(X.T @ X, X.T @ y)\n",
    "\n",
    "def compute_mse(X, y, w):\n",
    "    return np.mean((y - X @ w) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba785cbe",
   "metadata": {},
   "source": [
    "Your first task is to complete the final of the three helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e734d51",
   "metadata": {},
   "source": [
    "### Question 5.1 [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "Complete the implementation of the function `create_design_matrix`, which takes in a DataFrame `df` like `tips` and a list of column names, `columns`, and returns a 2D **array** where:\n",
    "- The first column of the output array is all 1s.\n",
    "- All other columns of the output array are the columns of `df` specified in the list `columns`, in the same order in which they appear in `columns`.\n",
    "\n",
    "Example behavior is given below.\n",
    "\n",
    "```python\n",
    ">>> create_design_matrix(tips.head(), ['total_bill', 'table_size'])\n",
    "array([[ 1.  , 16.99,  2.  ],\n",
    "       [ 1.  , 10.34,  3.  ],\n",
    "       [ 1.  , 21.01,  3.  ],\n",
    "       [ 1.  , 23.68,  2.  ],\n",
    "       [ 1.  , 24.59,  4.  ]])\n",
    "```\n",
    "\n",
    "Some guidance:\n",
    "- Make sure your function doesn't make in-place modifications to the passed in DataFrame!\n",
    "- Assume that all of the column names in `columns` refer to numeric columns in `df`.\n",
    "- There could be repeated column names in `columns`; if this happens, include the specified columns multiple times, as requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e6834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_design_matrix(df, columns):\n",
    "    ...\n",
    "\n",
    "# Feel free to change this input to make sure your function works correctly.\n",
    "create_design_matrix(tips.head(), ['total_bill', 'table_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7a47b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q05_01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472dd038",
   "metadata": {},
   "source": [
    "**If you implemented `create_design_matrix` correctly, you should be able to run the next few cells without any issues.**\n",
    "\n",
    "Recall, our goal is to start with a simple linear model that uses `'total_bill'` to predict `'tip'`. Let's use your implementation of `create_design_matrix` to set up our $X$ and $\\vec{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_one_feature = create_design_matrix(tips, ['total_bill'])\n",
    "y = tips['tip']\n",
    "\n",
    "# Notice that X_one_feature has two columns.\n",
    "X_one_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3bd1f5",
   "metadata": {},
   "source": [
    "Next, let's find the optimal parameter vector, $\\vec{w}^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012ffe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding w*.\n",
    "w_one_feature = solve_normal_equations(X_one_feature, y)\n",
    "w_one_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0441a54f",
   "metadata": {},
   "source": [
    "We can now use this hypothesis function to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e21288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot product of an augmented feature vector for a total bill of 15 with the optimal parameter vector.\n",
    "np.array([1, 15]) @ w_one_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdba19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(tips, x='total_bill', y='tip', title='Tip vs. Total Bill')\n",
    "\n",
    "x_range = np.linspace(0, 60)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=tips['total_bill'], y=y, mode='markers', name='actual'))\n",
    "fig.add_trace(go.Scatter(x=x_range, \n",
    "                         y=w_one_feature[0] + w_one_feature[1] * x_range, \n",
    "                         name='Linear Hypothesis Function', \n",
    "                         line=dict(color='red')))\n",
    "\n",
    "fig.update_layout(xaxis_title='Total Bill', yaxis_title='Tip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f99101",
   "metadata": {},
   "source": [
    "The mean squared error of this hypothesis function is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c074f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_one_feature = compute_mse(X_one_feature, y, w_one_feature)\n",
    "mse_one_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b2f2e",
   "metadata": {},
   "source": [
    "We'll define the DataFrame `hypothesis_functions` solely to keep track of the hypothesis functions we've used so far along with their MSEs. (We'll update this DataFrame for you.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff76f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_functions = pd.DataFrame(index=['total_bill'], columns=['MSE'])\n",
    "hypothesis_functions.loc['total_bill'] = mse_one_feature\n",
    "hypothesis_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b880377",
   "metadata": {},
   "source": [
    "### Question 5.2 [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div> \n",
    "\n",
    "Let's suppose Billy works for a day as a waiter at the [Gandy Dancer](https://www.gandydancerrestaurant.com/), a fancy restaurant. He waits a table whose total bill is \\$350. He decides to use the above linear hypothesis function to predict the tip that he will receive.\n",
    "\n",
    "1. What tip would the above single-feature model predict for a total bill of \\$350? In the cell below, assign the answer to the variable `prediction_for_350`. (Try and use the `@` symbol as part of your answer!)\n",
    "1. Is this prediction likely to be accurate? If so, in the cell below, assign the variable `is_accurate` to `True`, otherwise, assign it to `False`. Before assigning `is_accurate` to either `True` or `False`, you should think about what makes a prediction about the future likely to be accurate vs. not.\n",
    "\n",
    "**You should not round any numbers at any point in this question**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8069ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_for_350 = ...\n",
    "is_accurate = ...\n",
    "\n",
    "# Don't change the line below.\n",
    "print(f'The predicted tip for a total bill of $350 is ${round(prediction_for_350, 2)}, and we {\"do\" if is_accurate else \"do not\"} think this prediction is likely to be accurate.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a64118",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q05_02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9355363b",
   "metadata": {},
   "source": [
    "### Question 5.3 [Autograded üíª]  <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "Now, let's suppose we want to use `'total_bill'` **and** `'table_size'` to predict `'tip'`.\n",
    "\n",
    "Below, complete the following tasks:\n",
    "\n",
    "1. Assign `X_two_features` to the design matrix for this new hypothesis function.\n",
    "1. Assign `w_two_features` to the optimal parameter vector for this new hypothesis function.\n",
    "1. Assign `mse_two_features` to the mean squared error of this hypothesis function.\n",
    "1. Did adding `'table_size'` as a feature make our hypothesis function significantly more accurate as compared to the hypothesis function that used just `'total_bill'`? If so, assign `much_more_accurate` to `True`, otherwise assign it to `False`.\n",
    "\n",
    "Tasks 1, 2, and 3 should each only take line; remember to use the helper functions we've already defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d7791c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_two_features = ...\n",
    "w_two_features = ...\n",
    "mse_two_features = ...\n",
    "much_more_accurate = ...\n",
    "\n",
    "# Don't change the lines below.\n",
    "print('first five rows of design matrix:\\n', X_two_features[:5])\n",
    "print('optimal parameter vector:', w_two_features)\n",
    "print('MSE:', mse_two_features)\n",
    "print('much more accurate:', 'yes' if much_more_accurate else 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c0f3fa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q05_03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53eef1a",
   "metadata": {},
   "source": [
    "If you completed Question 5.3 correctly, you should see a 3D scatter plot of the original data points and your hypothesis function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9388f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX, YY = np.mgrid[0:60:2, 0:8:2]\n",
    "Z = w_two_features[0] + w_two_features[1] * XX + w_two_features[2] * YY\n",
    "plane = go.Surface(x=XX, y=YY, z=Z, colorscale='Reds')\n",
    "\n",
    "fig = go.Figure(data=[plane])\n",
    "fig.add_trace(go.Scatter3d(x=tips['total_bill'], \n",
    "                           y=tips['table_size'], \n",
    "                           z=tips['tip'], mode='markers', marker = {'color': '#656DF1'}))\n",
    "\n",
    "fig.update_layout(scene = dict(\n",
    "    xaxis_title='Total Bill',\n",
    "    yaxis_title='Table Size',\n",
    "    zaxis_title='Tip'), title='Tip vs. Total Bill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5aff9",
   "metadata": {},
   "source": [
    "Don't change this cell, just run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c04f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_functions.loc['total_bill and table_size'] = mse_two_features\n",
    "hypothesis_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb3e861",
   "metadata": {},
   "source": [
    "### Question 5.4 [Autograded üíª]  <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "Which feature is more important in predicting tip ‚Äì `'total_bill'` or `'table_size'`?\n",
    "\n",
    "Assuming you answered Question 5.3 correctly, run the cell below to create a **standardized** design matrix, where the two columns for `'total_bill'` and `'tip'` are standardized to have mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb64b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_two_features_standardized = X_two_features.copy()\n",
    "X_two_features_standardized[:, 1:] = (X_two_features[:, 1:] - np.mean(X_two_features[:, 1:], axis=0)) / X_two_features[:, 1:].std(axis=0, ddof=0)\n",
    "X_two_features_standardized[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6db5da7",
   "metadata": {},
   "source": [
    "Below,\n",
    "\n",
    "1. Assign `w_two_features_standardized` to an array containing the standardized regression coefficients for our two-feature hypothesis function.\n",
    "1. Assign `more_important` to either `'total_bill'` or `'table_size'`, depending on which of the two features you think is more important in predicting `'tip'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722ee9ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w_two_features_standardized = ...\n",
    "more_important = ...\n",
    "w_two_features_standardized, more_important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c030c3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q05_04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07a80a2",
   "metadata": {},
   "source": [
    "Don't change this cell, just run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7091d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_functions.loc['total_bill and table_size std'] = compute_mse(X_two_features_standardized, y, w_two_features_standardized)\n",
    "hypothesis_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4546f1c",
   "metadata": {},
   "source": [
    "The MSEs of the last two hypothesis functions were the same! The only difference is that when we standardized the features in creating the most recent hypothesis function, we were able to compare the coefficients directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97885595",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Now, let's revisit the scatter plot of `'tip'` vs. `'total bill'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c2e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(tips, x='total_bill', y='tip', title='Tip vs. Total Bill')\n",
    "fig.update_layout(xaxis_title='Total Bill', yaxis_title='Tip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbb34ec",
   "metadata": {},
   "source": [
    "Let's see if using higher-degree polynomial features yields a better hypothesis function. Specifically, let's try and create a degree 4 polynomial hypothesis function, using the features `'total_bill'`, `'total_bill^2'`, `'total_bill^3'`, and `'total_bill^4'`.\n",
    "\n",
    "(We'll see this in more detail in Lecture 16; this part is meant you to introduce you to the idea of polynomial features.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc796590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a copy of the tips DataFrame so that we don't modify the original data.\n",
    "tips_with_poly_features = tips.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7718caad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing total_bill^2.\n",
    "tips_with_poly_features['total_bill^2'] = tips_with_poly_features['total_bill'] ** 2\n",
    "tips_with_poly_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6bb8d0",
   "metadata": {},
   "source": [
    "### Question 5.5 [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">3 Points</div>\n",
    "\n",
    "Below,\n",
    "\n",
    "1. Add columns `'total_bill^3'` and `'total_bill^4'` to the DataFrame `tips_with_poly_features`.\n",
    "1. Define `X_poly`, `w_poly`, and `mse_poly` to be the design matrix, optimal parameter vector, and mean squared error of our new 4th degree polynomial hypothesis function. Note that this hypothesis function should be of the form:\n",
    "\n",
    "    $$H(x_i) = w_0 + w_1 x_i + w_2 x_i^2 + w_3 x_i^3 + w_4 x_i^4$$\n",
    "\n",
    "    where $x$ is the `'total_bill'`.\n",
    "\n",
    "Again, this subpart should only take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc9abf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tips_with_poly_features = ...\n",
    "X_poly = ...\n",
    "w_poly = ...\n",
    "mse_poly = ...\n",
    "\n",
    "# Don't change the lines below.\n",
    "print('first five rows of design matrix:\\n', X_poly[:5])\n",
    "print('optimal parameter vector:', w_poly)\n",
    "print('MSE:', mse_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534f7870",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q05_05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a61ad79",
   "metadata": {},
   "source": [
    "Don't change this cell, just run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_functions.loc['total_bill 4th degree poly'] = mse_poly\n",
    "hypothesis_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b938d25f",
   "metadata": {},
   "source": [
    "Assuming you completed Question 5.5 correctly, run the following cell to see a visualization of our 4th degree polynomial hypothesis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a1e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.linspace(0, 50)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=tips['total_bill'], y=tips['tip'], mode='markers', name='actual'))\n",
    "fig.add_trace(go.Scatter(x=x_range, \n",
    "                         y=w_poly[0] + w_poly[1] * (x_range) + w_poly[2] * (x_range**2) + \\\n",
    "                             w_poly[3] * (x_range**3) + w_poly[4] * (x_range**4),\n",
    "                         name='4th Degree Polynomial Hypothesis Function', \n",
    "                         line=dict(color='red', width=5)))\n",
    "\n",
    "fig.update_layout(xaxis_title='Total Bill', yaxis_title='Tip', title='Tip vs. Total Bill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa5b6f",
   "metadata": {},
   "source": [
    "The 4th degree polynomial hypothesis function seems to fit the data the best so far, since its MSE is the lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed71eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a78b550",
   "metadata": {},
   "source": [
    "But let's see what happens when we \"zoom out\" and look at how this hypothesis function behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b58658",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.linspace(-20, 70)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=tips['total_bill'], y=tips['tip'], mode='markers', name='actual'))\n",
    "fig.add_trace(go.Scatter(x=x_range, \n",
    "                         y=w_poly[0] + w_poly[1] * (x_range) + w_poly[2] * (x_range**2) + \\\n",
    "                             w_poly[3] * (x_range**3) + w_poly[4] * (x_range**4),\n",
    "                         name='4th Degree Polynomial Hypothesis Function', \n",
    "                         line=dict(color='red', width=5)))\n",
    "\n",
    "fig.update_layout(xaxis_title='Total Bill', yaxis_title='Tip', title='Tip vs. Total Bill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684ea536",
   "metadata": {},
   "source": [
    "**This is precisely the same behavior we saw in Question 5, when we learned about Lagrange Interpolation!** Indeed, if we kept increasing the degrees of the polynomial features we use, our hypothesis function will look more and more like the interpolated polynomial. As we had you ponder in Question 5, **think** about **why** a hypothesis function with a lower MSE is not necessarily better than a hypothesis function with a higher MSE. You don't need to write your answer anywhere, but discuss it with someone (either a peer or IA/GSI/Professor) before submitting this homework.\n",
    "\n",
    "We'll explore polynomial regression in more detail in Lecture 16, as mentioned above, but we'll discuss this more general idea ‚Äì of why we shouldn't make our hypothesis functions overly complex ‚Äì in Lectures 17 and 18."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c82416",
   "metadata": {},
   "source": [
    "### Question 5.6 [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">2 Points</div>\n",
    "\n",
    "Let's again suppose Billy works for a day as a waiter at [The Gandy Dancer](https://www.gandydancerrestaurant.com/). He waits a table whose total bill is \\$350. He decides to use the above 4th degree polynomial hypothesis function to predict the tip that he will receive.\n",
    "\n",
    "What tip would the above polynomial model predict for a total bill of \\$350? In the cell below, assign the answer to the variable `poly_prediction_for_350`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a24c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "poly_prediction_for_350 = ...\n",
    "\n",
    "# Don't change the line below.\n",
    "print(f'The predicted tip for a total bill of $350 is ${round(poly_prediction_for_350, 2)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5ef118",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q05_06\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399e5e0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "There was another column in our original DataFrame, `tips`, that we haven't yet looked at: `'day'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f14621",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b69c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(tips['day'].value_counts().loc[['Thur', 'Sat', 'Sun']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9939fb40",
   "metadata": {},
   "source": [
    "Note that unlike `'total_bill'` and `'table_size'`, `'day'` is **categorical**. This means there's no easy way to put it in our design matrix or find the best hypothesis function.\n",
    "\n",
    "A na√Øve solution would be to encode `'Thur'` as 1, `'Sat'` as 2, and `'Sun'` as 3, but this would make it seem like Sunday is \"more\" than Saturday or Thursday in some regard, which it is not ‚Äì these are all just different days of the week.\n",
    "\n",
    "A more robust and common solution is called **one hot encoding** (OHE). You will be exposed to it in more detail in Lecture 16, but we want to show you an example of how it works now since it's a natural extension of what we've already covered.\n",
    "\n",
    "Let's first get it working on a toy example. Let's pretend we have a DataFrame with just 5 rows and 2 columns, `'total_bill'` and `'day'`. Call it `mini_tips`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c0b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_tips = pd.DataFrame()\n",
    "mini_tips['total_bill'] = tips['total_bill'].iloc[:5]\n",
    "mini_tips['day'] = ['Sat', 'Sun', 'Sun', 'Thur', 'Sat']\n",
    "mini_tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476276ec",
   "metadata": {},
   "source": [
    "When we **one hot encode** a categorical variable, we create a new column for each unique value of that categorical variable. In this case, we'd create three new columns, one each for `'Thur'`, `'Sat'`, and `'Sun'`.\n",
    "\n",
    "Each of these new columns is binary, meaning they only contain the values 1 and 0. \n",
    "- The new column for `'Thur'`, which we'll call `'is_thur'`, will contain a 1 for rows where the value of `'day'` is `'Thur'`, and 0 for all other rows. \n",
    "- Similarly, the new column for `'Sun'`, which we'll call `'is_sun'`, will contain a 1 for rows where the value of day is `'Sun'`, and 0 for all other rows.\n",
    "\n",
    "Again, you'll see more efficient ways to do this in Lecture 16, but here's one way to one hot encode using our understanding of `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1614326",
   "metadata": {},
   "outputs": [],
   "source": [
    "(mini_tips['day'] == 'Thur')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b55f74",
   "metadata": {},
   "source": [
    "Repeating this for all columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7674daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_tips['is_thur'] = (mini_tips['day'] == 'Thur').astype(int)\n",
    "mini_tips['is_sat'] = (mini_tips['day'] == 'Sat').astype(int)\n",
    "mini_tips['is_sun'] = (mini_tips['day'] == 'Sun').astype(int)\n",
    "\n",
    "# Dropping the 'day' column. We've encoded it numerically, we don't need it anymore.\n",
    "mini_tips = mini_tips.drop(columns=['day'])\n",
    "mini_tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddafb22",
   "metadata": {},
   "source": [
    "Now we've converted a categorical feature into three numerical features, so we're good to go!\n",
    "\n",
    "**There's just one more thing.** Since we're used to fitting linear hypothesis functions with an intercept term, our design matrix generally has a column of all 1s in it. In the case of `mini_tips`, which contains three binary columns, this would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c949c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_design_matrix(mini_tips, list(mini_tips.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d950a0",
   "metadata": {},
   "source": [
    "This design matrix contains redundant information! Specifically, we can recreate the column of all 1s by adding together the three one-hot encoded columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68bf90c",
   "metadata": {},
   "source": [
    "$$X^TX\\vec{w} = X^Ty$$\n",
    "\n",
    "$$\\vec{w}^* = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f5e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_not_full_rank = create_design_matrix(mini_tips, list(mini_tips.columns))\n",
    "X_not_full_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c8387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the 0, 1, 2, 3, 4 that you see is the index of this Series, which is irrelevant for our purposes.\n",
    "mini_tips['is_thur'] + mini_tips['is_sat'] + mini_tips['is_sun']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d724bb4a",
   "metadata": {},
   "source": [
    "What this means is that our design matrix $X$ suffers from multicollinearity, and is not **full rank**. There are multiple nasty side effects of this ‚Äì there is no unique solution for $\\vec{w}^*$ and it makes our optimal parameters more difficult to interpret.\n",
    "\n",
    "Again, we'll address this idea in lectures to come, so don't worry if this is a bit confusing. This is more meant to be a preview of what's to come.\n",
    "\n",
    "**For now, know this ‚Äì the way to avoid this problem is to drop one of the one hot encoded columns.** That way, there is no redundant information in the design matrix, and we don't run into any issues. This is not \"getting rid\" of any information, so it will not impact our predictions ‚Äì if we know it is not Saturday or Sunday, it must be Thursday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've arbitrarily chosen to drop 'is_thur', but it would make no difference if we instead dropped 'is_sat' or 'is_sun'.\n",
    "mini_tips = mini_tips.drop(columns=['is_thur'])\n",
    "mini_tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feac33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_design_matrix(mini_tips, list(mini_tips.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de920d3c",
   "metadata": {},
   "source": [
    "Now we have a design matrix that is ready to go. Let's replicate this process on our full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3621f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell.\n",
    "tips_ohe = tips.copy()\n",
    "tips_ohe['is_sat'] = (tips_ohe['day'] == 'Sat').astype(int)\n",
    "tips_ohe['is_sun'] = (tips_ohe['day'] == 'Sun').astype(int)\n",
    "\n",
    "# Design matrix with two one-hot encoded columns.\n",
    "X_ohe = create_design_matrix(tips_ohe, ['total_bill', 'is_sat', 'is_sun'])\n",
    "print('first five rows of design matrix:\\n', X_ohe[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c864854",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ohe = solve_normal_equations(X_ohe, y)\n",
    "w_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439c202",
   "metadata": {},
   "source": [
    "Let's now plot the resulting hypothesis function. We've zoomed into the region where the `'total_bill'`s are less than \\\\$30 and `'tip'`s are less than \\\\$4 to make the hypothesis function more clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3513b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.linspace(0, 30)\n",
    "\n",
    "under_30 = tips[(tips['total_bill'] < 30) & (tips['tip'] < 4)]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=under_30['total_bill'], y=under_30['tip'], mode='markers', name='actual'))\n",
    "\n",
    "# Line for Thursday.\n",
    "fig.add_trace(go.Scatter(x=x_range, \n",
    "                         y=w_ohe[0] + w_ohe[1] * x_range, \n",
    "                         name='Thursday', \n",
    "                         line=dict(color='blue', width=4)))\n",
    "\n",
    "# Line for Saturday.\n",
    "fig.add_trace(go.Scatter(x=x_range, \n",
    "                         y=w_ohe[0] + w_ohe[2] + w_ohe[1] * x_range, \n",
    "                         name='Saturday', \n",
    "                         line=dict(color='orange', width=4)))\n",
    "\n",
    "# Line for Sunday.\n",
    "fig.add_trace(go.Scatter(x=x_range, \n",
    "                         y=w_ohe[0] + w_ohe[3] + w_ohe[1] * x_range, \n",
    "                         name='Sunday', \n",
    "                         line=dict(color='red', width=4)))\n",
    "\n",
    "fig.update_layout(xaxis_title='Total Bill', yaxis_title='Tip', title='Tip vs. Total Bill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98d5fcd",
   "metadata": {},
   "source": [
    "It looks like the hypothesis function is actually three separate lines, each of which have the same slope but different intercepts!\n",
    "\n",
    "Let's try and understand why this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb20e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a4308",
   "metadata": {},
   "source": [
    "Our hypothesis function is of the following form (approximately, since the coefficients are rounded):\n",
    "\n",
    "$$\\text{predicted tip} = 0.925 + 0.105 (\\text{total bill}) - 0.072 (\\text{is saturday}) + 0.089 (\\text{is sunday})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148903f7",
   "metadata": {},
   "source": [
    "### Question 5.7 [Autograded üíª] <div style=\"display:inline-block; vertical-align: middle; padding:7px 7px; font-size:10px; font-weight:light; color:white; background-color:#e84c4a; border-radius:7px; text-align:left;\">3 Points</div>\n",
    "\n",
    "Below, assign `intercept_thur`, `intercept_sat`, and `intercept_sun` to the **$y$-intercepts** of the three lines above, corresponding to when the `'day'` is Thursday, Saturday, or Sunday. You should do this using code,  pulling values from `w_ohe`, but you should think conceptually about where each of the three intercepts are coming from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f93855",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intercept_thur = ...\n",
    "intercept_sat = ...\n",
    "intercept_sun = ...\n",
    "\n",
    "# Don't change the lines below.\n",
    "print('Intercept for Thursday:', intercept_thur)\n",
    "print('Intercept for Saturday:', intercept_sat)\n",
    "print('Intercept for Sunday:', intercept_sun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437eb514",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q05_07\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde5c43d",
   "metadata": {},
   "source": [
    "Just for completeness, we'll also compute the MSE of this hypothesis function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e74dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_ohe = compute_mse(X_ohe, y, w_ohe)\n",
    "hypothesis_functions.loc['total_bill + OHE day'] = mse_ohe\n",
    "hypothesis_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b214a8d",
   "metadata": {},
   "source": [
    "This new hypothesis function didn't have a much lower MSE than the hypothesis function that used `total_bill` only. That's not all that surprising, since the three lines above look quite similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c9afc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Finish Line üèÅ\n",
    "\n",
    "Congratulations! You're ready to submit Homework 7.\n",
    "\n",
    "You need to submit Homework 7 twice:\n",
    "\n",
    "### To submit the manually graded problems (Questions 1-2; marked [Written ‚úèÔ∏è])\n",
    "\n",
    "- Make sure your answers **are not** in this notebook, but rather in a separate PDF.\n",
    "    - You can create this PDF either digitally, using your tablet or using [Overleaf + LaTeX](https://overleaf.com) (or some other sort of digital document), or by writing your answers on a piece of paper and scanning them in.\n",
    "- Submit this separate PDF to the **Homework 7 (Questions 1-2; written problems)** assignment on Gradescope, and **make sure to correctly select the pages associated with each question**!\n",
    "\n",
    "### To submit the autograded problems (Questions 3-5; marked [Autograded üíª])\n",
    "\n",
    "1. Select `Kernel -> Restart & Run All` to ensure that you have executed all cells, including the test cells.\n",
    "2. Read through the notebook to make sure everything is fine and all tests passed.\n",
    "3. Download your notebook using `File -> Download as -> Notebook (.ipynb)`, then upload your notebook to Gradescope under **Homework 7 (Questions 3-5; autograded problems)**.\n",
    "4. Stick around while the Gradescope autograder grades your work.\n",
    "5. Check that you have a confirmation email from Gradescope and save it as proof of your submission.\n",
    "\n",
    "Your Homework 7 submission time will be the **later** of your two individual submissions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "otter": {
   "tests": {
    "q03_01": {
     "name": "q03_01",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = generate_pairs([1, 2, -1, 4], [15, 6, 7, 8])\n>>> len(out) == 6 and isinstance(out[0], tuple) and len(out[0]) == 2\nTrue",
         "failure_message": "generate_pairs([1, 2, -1, 4], [15, 6, 7, 8]) returns the wrong number of tuples, or tuples with more than two elements.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> out = {frozenset(pair) for pair in generate_pairs([1, 2, -1, 4], [15, 6, 7, 8])} # Set of sets, for autograding.\n>>> desired_out = {frozenset({(2, 6), (4, 8)}), frozenset({(-1, 7), (1, 15)}), frozenset({(2, 6), (1, 15)}), frozenset({(2, 6), (-1, 7)}), frozenset({(1, 15), (4, 8)}), frozenset({(-1, 7), (4, 8)})}\n>>> all([s in desired_out for s in out])\nTrue",
         "failure_message": "generate_pairs([1, 2, -1, 4], [15, 6, 7, 8]) returns the wrong output.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q03_02": {
     "name": "q03_02",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = generate_lines([((1, 2), (3, 7)), ((1, 10), (-4, 20))])\n>>> desired_out = [(-0.5, 2.5), (12.0, -2.0)]\n>>> is_close_pair = lambda a, b: np.isclose(a[0], b[0]) and np.isclose(a[1], b[1])\n>>> all([is_close_pair(out[i], desired_out[i]) for i in range(len(out))])\nTrue",
         "failure_message": "generate_lines([((1, 2), (3, 7)), ((1, 10), (-4, 20))]) returns the wrong values.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> out = generate_lines([((0, 0), (1, 1))])\n>>> desired_out = [(0.0, 1.0)]\n>>> is_close_pair = lambda a, b: np.isclose(a[0], b[0]) and np.isclose(a[1], b[1])\n>>> all([is_close_pair(out[i], desired_out[i]) for i in range(len(out))])\nTrue",
         "failure_message": "generate_lines([((0, 0), (1, 1))]) returns the wrong values.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q03_03": {
     "name": "q03_03",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = mae_of_candidate_line(5, 2, [1, 2, -1, 4], [15, 6, 7, 8])\n>>> np.isclose(out, 5)\nTrue",
         "failure_message": "mae_of_candidate_line(5, 2, [1, 2, -1, 4], [15, 6, 7, 8]) returns the wrong value.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> out = mae_of_candidate_line(0.5, 1, np.arange(10), np.arange(1, 11))\n>>> np.isclose(out, 0.5)\nTrue",
         "failure_message": "mae_of_candidate_line(0.5, 1, np.arange(10), np.arange(1, 11)) returns the wrong value.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q03_04": {
     "name": "q03_04",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> model_test = SimpleLAD()\n>>> model_test.fit([1, 2, -1, 4], [15, 6, 7, 8])\n>>> np.isclose(model_test.intercept_, 7.2) and np.isclose(model_test.coef_, 0.2)\nTrue",
         "failure_message": "When fit on x=[1, 2, -1, 4], y=[15, 6, 7, 8], model object computes incorrect optimal parameters.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> model_test = SimpleLAD()\n>>> model_test.fit([1, 2, -1, 4], [15, 6, 7, 8])\n>>> np.allclose(model_test.predict([5, -3.5, 5]), [8.2, 6.5, 8.2])\nTrue",
         "failure_message": "When fit on x=[1, 2, -1, 4], y=[15, 6, 7, 8], model object computes incorrect predictions.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> model_test = SimpleLAD()\n>>> model_test.fit(np.array([1.5, -2.9, 3, 14, -3, 2, np.pi]), [0.8, -0.71, 1776, np.e ** 2, 14, 3, 9])\n>>> np.isclose(model_test.intercept_, 2.268490650178225) and np.isclose(model_test.coef_, 0.3657546749108875)\nTrue",
         "failure_message": "When fit on x=np.array([1.5, -2.9, 3, 14, -3, 2, np.pi]), y=[0.8, -0.71, 1776, np.e ** 2, 14, 3, 9], model object computes incorrect optimal parameters.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q03_05": {
     "name": "q03_05",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> naive_lad_runtime in range(1, 9)\nTrue",
         "failure_message": "Answer must be an integer between 1 and 8, inclusive.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q04": {
     "name": "q04",
     "points": 6,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> f = interpolate([(1, 3), (3, 19), (4, 33)])\n>>> np.allclose([f(1), f(3), f(4), f(100)], [3, 19, 33, 20001])\nTrue",
         "failure_message": "interpolate([(1, 3), (3, 19), (4, 33)]) returns a function with incorrect behavior.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> f = interpolate([(1, 3), (2, 6), (3, 9)])\n>>> np.allclose([f(10), f(-20)], [30, -60])\nTrue",
         "failure_message": "interpolate([(1, 3), (2, 6), (3, 9)]) returns a function with incorrect behavior.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q05_01": {
     "name": "q05_01",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out = create_design_matrix(tips, [\"total_bill\", \"table_size\"])\n>>> out.shape == (244, 3)\nTrue",
         "failure_message": "create_design_matrix(tips, [\"total_bill\", \"table_size\"]) has incorrect dimensions.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> out = create_design_matrix(tips, [\"total_bill\", \"table_size\"])\n>>> np.allclose(out.mean(axis=0), np.array([ 1.        , 19.78594262,  2.56967213]))\nTrue",
         "failure_message": "create_design_matrix(tips, [\"total_bill\", \"table_size\"]) has incorrect values.",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> out = create_design_matrix(tips, [\"total_bill\", \"table_size\"])\n>>> np.allclose(out[15:25, 1], np.array([21.58, 10.33, 16.29, 16.97, 20.65, 17.92, 20.29, 15.77, 39.42, 19.82]))\nTrue",
         "failure_message": "create_design_matrix(tips, [\"total_bill\", \"table_size\"]) has incorrect values.",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q05_02": {
     "name": "q05_02",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(prediction_for_350, float) and isinstance(is_accurate, bool)\nTrue",
         "failure_message": "Make sure prediction_for_350 is a float and is_accurate is a bool,",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q05_03": {
     "name": "q05_03",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(X_two_features, np.ndarray) and X_two_features.shape == (244, 3)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(w_two_features, np.ndarray) and w_two_features.shape == (3,)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(mse_two_features, float) and mse_two_features > 1\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(much_more_accurate, bool)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q05_04": {
     "name": "q05_04",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(w_two_features_standardized, np.ndarray) and len(w_two_features_standardized) == 3\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.all(np.diff(w_two_features_standardized) < 0) # Making sure the coefficients are in the right order.\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> more_important in ['total_bill', 'table_size']\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> more_important == 'total_bill'\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q05_05": {
     "name": "q05_05",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> tips_with_poly_features.shape == (244, 10) and set(tips_with_poly_features.columns) >= set(['total_bill', 'total_bill^2', 'total_bill^3', 'total_bill^4'])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(X_poly, np.ndarray) and X_poly.shape == (244, 5)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(w_poly, np.ndarray) and w_poly.shape == (5,)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> isinstance(mse_poly, float) and mse_poly < 1\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q05_06": {
     "name": "q05_06",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(poly_prediction_for_350, float)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q05_07": {
     "name": "q05_07",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(intercept_thur, float) and isinstance(intercept_sat, float) and isinstance(intercept_sun, float)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> intercept_sat < intercept_thur < intercept_sun\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(intercept_thur, w_ohe[0], atol=0.05) or np.isclose(intercept_thur, 0.9251166109157465, atol=0.05)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
