{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa44e6a3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to get everything set up.\n",
    "from lec_utils import *\n",
    "import lec22_util as util\n",
    "diabetes = pd.read_csv('data/diabetes.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "diabetes = diabetes[(diabetes['Glucose'] > 0) & (diabetes['BMI'] > 0)]\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(diabetes[['Glucose', 'BMI']], diabetes['Outcome'], random_state=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77c19c1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Lecture 22\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "### EECS 398: Practical Data Science, Winter 2025\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> â€¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/wn25\">github.com/practicaldsc/wn25</a> â€¢ ðŸ“£ See latest announcements [**here on Ed**](https://edstem.org/us/courses/69737/discussion/5943734) </small>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<script type=\"text/x-mathjax-config\">\n",
    " MathJax.Hub.Config({\n",
    "   TeX: {\n",
    "     extensions: [\"color.js\"],\n",
    "     packages: {\"[+]\": [\"color\"]},\n",
    "   }\n",
    " });\n",
    " </script>\n",
    " <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee56a563",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda ðŸ“†\n",
    "\n",
    "- Predicting probabilities.\n",
    "- Cross-entropy loss.\n",
    "- From probabilities to decisions.\n",
    "\n",
    "Check out this [Decision Boundary Visualizer](https://ml-visualizer.herokuapp.com/), which allows you to visualize decision boundaries for different classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8088b21d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ðŸ¤” (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Remember that you can always ask questions anonymously at the link above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca98e7e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predicting probabilities ðŸŽ²\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a21c23",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"imgs/needle.png\" width=900>\n",
    "<br>\n",
    "The New York Times maintained <a href=\"https://www.nytimes.com/interactive/2024/11/05/us/elections/results-president-forecast-needle.html\">needles</a><br>that displayed the probabilities of various outcomes in the election.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c6cf57",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation: Predicting probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab32a56d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Often, we're interested in predicting the **probability** of an event occurring, given some other information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420644f8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Given that the score at the start of the second half is Michigan 23-Northwestern 15,<br>what's the probability that Michigan wins?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07e2739",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Here's a picture of an animal. What's the probability it's of a dog? Cat? Hamster? Zebra?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ded0c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>What's the probability that it rains on campus tomorrow?<br><small>In the context of weather apps, this is a nuanced question; <a href=\"https://xkcd.com/1985\">here's a meme about it</a>.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c010f87",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we're able to predict the probability of an event, we can **classify** the event by using a threshold.<br><small>For example, if we predict there's a 70% chance of Michigan winning, we could predict that Michigan will win. Here, we implicitly used a threshold of 50%.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df1974",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The two classification techniques we've seen so far â€“ $k$-nearest neighbors and decision trees â€“ **don't** directly use probabilities in their decision-making process.<br><small>But sometimes it's helpful to model uncertainty and to be able to state a level of confidence along with a prediction!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac95ceef",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Predicting diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba958e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As before, <span style='color: orange'><b>class 0 (orange) is \"no diabetes\"</b></span> and <span style='color: blue'><b>class 1 (blue) is \"diabetes\"</b></span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a93919",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "util.create_base_scatter(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cf2368",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's try to predict whether or not a patient has diabetes (`'Outcome'`) given just their `'Glucose'` level.<br><small>Last class, we used both `'Glucose'` and `'BMI'`; we'll start with just one feature for now.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b4962",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead950e9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems that as a patient's `'Glucose'` value increases, the **chances they have diabetes** also increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd357f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can we model this probability directly, as a function of `'Glucose'`?<br>In other words, can we find some $f$ such that:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i) = f(\\text{Glucose}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a377b0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An attempt to predict probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65def7e9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's try and fit a simple linear model to the data from the previous slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ba4a6",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_linear_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c0cdc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The <span style=\"color:#097054\"><b>simple linear model</b></span> above predicts values greater than 1 and less than 0! This means we can't interpret the outputs as probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d231f07",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We could, technically, **clip** the outputs of the linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d63dd",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_linear_model_clipped(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36ddf22",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bins and proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdcc1e0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Another approach we could try is to:\n",
    "    - Place `'Glucose'` values into **bins**, e.g. 50 to 55, 55 to 60, 60 to 65, etc.\n",
    "    - Within each bin, compute the proportion of patients in the training set who had diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7708f42c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Take a look at the source code in lec22_util.py to see how we did this!\n",
    "# We've hidden a lot of the plotting code in the notebook to make it cleaner.\n",
    "util.make_prop_plot(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96eb9fb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For example, the point near a `'Glucose'` value of 100 has a $y$-axis value of ~0.25. This means that about 25\\% of patients with a `'Glucose'` value near 100 had diabetes in the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd12cd09",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, if a new person comes along with a `'Glucose'` value near 100, we'd predict there's a 25\\% chance they have diabetes (so they likely do not)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906dc36b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Notice that the points form an S-shaped curve!**<br><small>Can we incorporate this S-shaped curve in how we predict probabilities?</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903a6666",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The logistic function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd21f61",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **logistic function** resembles an $S$-shape.\n",
    "\n",
    "    $$\\sigma(t) = \\frac{1}{1 + e^{-t}} = \\frac{1}{1 + \\text{exp}(-t)}$$\n",
    "    \n",
    "    <br><small>The logistic function is an example of a <b>sigmoid function</b>, which is the general term for an S-shaped function. Sometimes, we use the terms \"logistic function\" and \"sigmoid function\" interchangeably.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d9efe9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Below, we'll look at the shape of $y = \\sigma(w_0 + w_1 x)$ for different values of $w_0$ and $w_1$.\n",
    "    - $w_0$ controls the position of the curve on the $x$-axis.\n",
    "    - $w_1$ controls the \"steepness\" of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6f5c1",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_three_sigmoids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a8867",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Notice that $0 < \\sigma(t) < 1$, for all $t$, which means **we can interpret the outputs of $\\sigma(t)$ as probabilities**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb3bfcd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below, interact with the sliders to change the values of $w_0$ and $w_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aece01",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "interact(util.plot_sigmoid, w0=(-15, 15), w1=(-3, 3, 0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23acb920",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8fa878",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Logistic **regression** is a linear **classification** technique that builds upon linear regression.<br><small>It is **not** called logistic**al** regression!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6352c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It models **the probability of belonging to class 1, given a feature vector**:\n",
    "    \n",
    "$$P(y_i = 1 | \\vec{x}_i) = \\sigma (\\underbrace{w_0 + w_1 x_i^{(1)} + w_2 x_i^{(2)} + ... + w_d x_i^{(d)}}_{\\text{linear regression model}}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90bec75",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that the existence of coefficients, $w_0, w_1, ... w_d$, that we need to learn from the data, tells us that logistic regression is a **parametric** method!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79487a0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `LogisticRegression` in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841271de",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1a71f4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's fit a `LogisticRegression` classifier. Specifically, this means we're asking `sklearn` to learn the optimal parameters $w_0^*$ and $w_1^*$ in:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i) = \\sigma \\left( w_0 + w_1 \\cdot \\text{Glucose}_i \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a4f551",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic = LogisticRegression()\n",
    "model_logistic.fit(X_train[['Glucose']], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7dced4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We get a test accuracy that's roughly in line with the test accuracies of the two models we saw last class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b05a65",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.score(X_test[['Glucose']], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b91bb3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What does our fit model **look like**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dc1f66",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing a fit logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb3aba7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The values of $w_0^*$ and $w_1^*$ `sklearn` found are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d5500",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.intercept_[0], model_logistic.coef_[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaa85e1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, our fit model is:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i) = \\sigma(-5.59 + 0.04 \\cdot \\text{Glucose}_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a4b5ed",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_logistic(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e5e78f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, if a patient has a `'Glucose'` level of 150, the model's predicted probability that they have diabetes is:\n",
    "\n",
    "$$\\sigma(-5.59 + 0.04 \\cdot 150) \\approx \\sigma(0.41) \\approx 0.601$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8410f86",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.predict_proba([[150]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f504f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><big>How did <code>sklearn</code> find $w_0^*$ and $w_1^*$?<br>What <b>loss function</b> did it use?</big></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad4b87",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-entropy loss\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b56e3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The modeling recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4d7c8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To train a **parametric model**, we always follow the same three steps.\n",
    "<br><small>$k$-Nearest Neighbors and decision trees didn't quite follow the same process.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fec96ce",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Choose a model.\n",
    "\n",
    "$$P(y_i = 1 | \\vec{x}_i) = \\sigma (w_0 + w_1 x_i^{(1)} + w_2 x_i^{(2)} + ... + w_d x_i^{(d)}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217dd4a4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Choose a loss function.\n",
    "\n",
    "<center>???</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d7d63",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Minimize average loss to find optimal model parameters.<br><small>As we've now seen, average loss could also be regularized!</small>\n",
    "\n",
    "<center>???</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b6a91",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Attempting to use squared loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb1b23",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Our default loss function has always been squared loss, so we could try and use it here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f8b93",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb31a5d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Unfortunately, there's no closed form solution for $\\vec{w}^*$, so we'll need to use gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc59f38",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Before doing so, let's visualize the **loss surface** in the case of our \"simple\" logistic model:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i) = \\sigma(w_0 + w_1 \\cdot \\text{Glucose}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba29fb1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Specifically, we'll visualize:\n",
    "\n",
    "$$R_\\text{sq}(w_0, w_1) = \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - \\sigma(w_0 + w_1 \\underbrace{x_i}_{\\text{Glucose}_i} ) \\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e0554e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_logistic_mse_surface(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81358c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **What do you notice?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf39e3e2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mean squared error doesn't work well with logistic regression! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784abc5a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The following function is **not** convex:\n",
    "\n",
    "$$R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2753056",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There are two flat \"valleys\" with gradients near 0, where gradient descent could get trapped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635911c3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Additionally, squared loss doesn't penalize bad predictions nearly enough. The largest possible value of:\n",
    "\n",
    "    $$\\left( y_i - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)^2$$\n",
    "\n",
    "    is 1, since both $y_i$ and $\\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$ are **bounded** between 0 and 1, and $(1 - 0)^2 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c39b21",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Suppose $y_i = 1$. Then, the graph of the squared loss of the prediction $p_i$ is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405dbc08",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_squared_loss_individual()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1148414a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Predicted $p_i$ values near 0 are really bad, since $y_i = 1$, but the loss for $p_i = 0$ is not very high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1ffce5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems like we need a loss function that more **steeply penalizes incorrect probability predictions** â€“ and hopefully, one that is convex for the logistic regression model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfbc31d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff3648",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A common loss function in this setting is **log loss**, i.e. **cross-entropy loss**.<br><small>The term \"entropy\" comes from information theory. Watch [**this short video**](https://www.youtube.com/watch?v=ErfnhcEV1O8) for more details.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de20ec",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can define the cross-entropy loss function piecewise. If $y_i$ is an observed value and $p_i$ is a predicted **probability**, then: \n",
    "\n",
    "$$L_\\text{ce}(y_i, p_i) = \\begin{cases} - \\log(p_i) & \\text{if $y_i = 1$} \\\\ -\\log(1 - p_i) & \\text{if $y_i = 0$} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a3d5f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that in the two cases â€“ $y_i = 1$ and $y_i = 0$ â€“ the cross-entropy loss function resembles squared loss, but is unbounded when the predicted probabilities $p_i$ are far from $y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b5bbb",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_ce_loss_individual_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c1d013",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_ce_loss_individual_0()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe5e5bf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A non-piecewise definition of cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f752df5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can define the cross-entropy loss function piecewise. If $y_i$ is an observed value and $p_i$ is a predicted **probability**, then: \n",
    "\n",
    "$$L_\\text{ce}(y_i, p_i) = \\begin{cases} - \\log(p_i) & \\text{if $y_i = 1$} \\\\ -\\log(1 - p_i) & \\text{if $y_i = 0$} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17351302",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An equivalent formulation of $L_\\text{ce}$ that isn't piecewise is:\n",
    "\n",
    "$$L_\\text{ce}(y_i, p_i) = - \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e65fdbf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This formulation is easier to work with algebraically!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc977ddb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Average cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d22c5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Cross-entropy loss** for an observed value $y_i$ and predicted **probability** $p_i = P(y = 1 | \\vec{x}_i) = \\sigma \\left(\\vec w \\cdot \\text{Aug}(\\vec x_i) \\right)$ is:\n",
    "\n",
    "$$L_\\text{ce}(y_i, p_i) = - \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b0f6e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To find $\\vec{w}^*$, then, we minimize **average cross-entropy loss**:\n",
    "\n",
    "\\begin{align*}R_\\text{ce}(\\vec{w}) &= - \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right) \\\\ &= - \\frac{1}{n} \\sum_{i = 1}^n \\left[ y_i \\log \\left( \\sigma \\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)  + (1 - y_i) \\log \\left(1 - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)\\right) \\right]\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66288165",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Cross-entropy loss is the default loss function used to find optimal parameters in logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc802747",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There's still no closed-form solution for $\\vec{w}^* = \\underset{\\vec{w}}{\\text{argmin}} \\: R_\\text{ce}(\\vec{w})$, so we'll need to use gradient descent, or some other numerical method.<br><small>But don't worry â€“ we'll leave this to `sklearn`!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fe79ff",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Fortunately, average cross-entropy loss is convex, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9832097",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_logistic_ce_surface(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6f226",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- And, it can be regularized!<br><small>By default, `sklearn` applies regularization when performing logistic regression.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a36153",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_logistic_ce_surface(X_train, y_train, reg_lambda=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454790e2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The modeling recipe, revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b606f0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Choose a model.\n",
    "\n",
    "$$P(y_i = 1 | \\vec{x}_i) = \\sigma (w_0 + w_1 x_i^{(1)} + w_2 x_i^{(2)} + ... + w_d x_i^{(d)}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d84a1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Choose a loss function.\n",
    "\n",
    "$$L_\\text{ce}(y_i, p_i) = - \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right)$$\n",
    "\n",
    "$$\\text{where} \\: p_i = P(y = 1 | \\vec{x}_i) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d182923e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Minimize average loss to find optimal model parameters.<br><small>As we've now seen, average loss could also be regularized!</small>\n",
    "\n",
    "    \\begin{align*}R_\\text{ce}(\\vec{w}) &= - \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right) \\\\ &= - \\frac{1}{n} \\sum_{i = 1}^n \\left[ y_i \\log \\left( \\sigma \\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)  + (1 - y_i) \\log \\left(1 - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)\\right) \\right]\\end{align*}\n",
    "\n",
    "    <br>\n",
    "\n",
    "    The actual minimization here is done using numerical methods, through `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88b7960",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `LogisticRegression` in `sklearn`, revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e541128",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The `LogisticRegression` class in `sklearn` has a lot of hidden, default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d612e5b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c6741f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It performs $L_2$ regularization (\"ridge logistic regression\") **by default**. The hyperparameter for regularization strength, $C$, is the **inverse** of $\\lambda$; by default, it sets $C = 1$.\n",
    "\n",
    "$$C = \\frac{1}{\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99851d0d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, for a given value of $C$, it minimizes:\n",
    "\n",
    "$$R_\\text{ce-reg}(\\vec{w}) - \\frac{1}{n} \\sum_{i = 1}^n \\left[ y_i \\log \\left( \\sigma \\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)  + (1 - y_i) \\log \\left(1 - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)\\right) \\right] + \\frac{1}{C} \\sum_{j = 1}^d w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c03da",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It also specifies `solver='lbfgs'`, i.e. it doesn't use gradient descent per-se, but another more sophisticated numerical method.<br><small>Read more about LBFGS [here](https://en.wikipedia.org/wiki/Limited-memory_BFGS).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b09e05f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">By default, logistic regression in scikit-learn runs w L2 regularization on and defaulting to magic number C=1.0. How many millions of ML/stats/data-mining papers have been written by authors who didn&#39;t report (&amp; honestly didn&#39;t think they were) using regularization?</p>&mdash; Zachary Lipton (@zacharylipton) <a href=\"https://twitter.com/zacharylipton/status/1167298276686589953?ref_src=twsrc%5Etfw\">August 30, 2019</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dd7b7f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ðŸ¤” (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d50d3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From probabilities to decisions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a64281c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predicting probabilities vs. predicting classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd213a9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$P(y_i = 1 | \\vec{x}_i) = \\sigma (w_0 + w_1 x_i^{(1)} + w_2 x_i^{(2)} + ... + w_d x_i^{(d)}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ceb488",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ðŸ¤” **Question**: Suppose our logistic regression model predicts the probability that someone has diabetes is 0.75. What do we predict â€“ diabetes or no diabetes? What if the predicted probability is 0.3?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058af105",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ðŸ™‹ **Answer**: We have to pick a threshold (for example, 0.5)!\n",
    "    - If the predicted probability is above the threshold, we predict diabetes (1).\n",
    "    - Otherwise, we predict no diabetes (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b8a7aa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predicting probabilities vs. predicting classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac452aa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By default, the `predict` method of a fit `LogisticRegression` model predicts a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809869c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.predict(pd.DataFrame([{\n",
    "    'Glucose': 150,\n",
    "}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb393a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But, logistic regression is designed to predict **probabilities**. We can access these predicted probabilities using the `predict_proba` method, as we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b91548",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic.predict_proba(pd.DataFrame([{\n",
    "    'Glucose': 150,\n",
    "}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ca1de7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The above is telling us that the model thinks this person has:\n",
    "    - A 40% chance of belonging to class 0 (no diabetes).\n",
    "    - A 60% chance of belonging to class 1 (diabetes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec014445",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By default, it uses a threshold of 0.5, i.e. it predicts the larger probability.<br>As we'll discuss next class, this may not be what we want!<br><small>Unfortunately, `sklearn` doesn't let us change the threshold ourselves. If we want a different threshold, we need to manually implement it using the results of `predict_proba`.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9bb2cc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thresholding probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82bdaff",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As we did with other classifiers, we can visualize the **decision boundary** of a fit logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ebdf55",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we pick a threshold of $T$, any patient with a `'Glucose'` value such that: \n",
    "\n",
    "    $$\\sigma(w_0^* + w_1^* \\cdot \\text{Glucose}_i) \\geq T$$ \n",
    "\n",
    "    is classified as having diabetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f09ce21",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For example, if $T = 0.5$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57c373b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_logistic_and_y_threshold(X_train, y_train, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bcc2e2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we set $T = 0.5$, then patients with `'Glucose'` values above $\\approx$ 140 are classified as having diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a206c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_logistic_and_x_threshold(X_train, y_train, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605989e8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **How do we find the exact $x$-axis position of the <span style=\"color:purple\">decision boundary</span> above?**<br><small>If we can, then we'd be able to predict whether someone has diabetes just by looking at their `'Glucose'` value.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e49d20f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision boundaries for logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e7e978",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In our single feature model that predicts `'Outcome'` given just `'Glucose'`, our predicted probabilities are of the form:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i) = \\sigma \\left( w_0^* + w_1^* \\cdot \\text{Glucose}_i \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4772ad2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we fix a threshold, $T$. Then, our <b><span style=\"color:purple\">decision boundary</span></b> is of the form:\n",
    "\n",
    "$$\\sigma \\left( w_0^* + w_1^* \\cdot \\text{Glucose}_T \\right) = T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d8b780",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we can invert $\\sigma(t)$, then we can re-arrange the above to solve for the `'Glucose'` value at the threshold:\n",
    "\n",
    "$$\\text{Glucose}_\\text{T} = \\frac{\\sigma^{-1}(T) - w_0^*}{w_1^*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591cb4e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Important**: If $p = \\sigma(t)$, then $\\sigma^{-1}({p}) = \\log \\left( \\frac{p}{1-p} \\right)$ is the inverse of $\\sigma(t)$.<br><small>$\\sigma^{-1}(p)$ is called the **logit** function.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a2fdf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ef30f1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose an event occurs with probability $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0650fb9b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **odds** of that event are:\n",
    "\n",
    "$$\\text{odds}(p) = \\frac{p}{1-p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7e16d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For instance, if there's a $p = \\frac{3}{4}$ chance that Michigan wins this week, then the **odds** that Michigan wins this week are:\n",
    "\n",
    "    $$\\text{odds} \\left( \\frac{3}{4} \\right) = \\frac{\\frac{3}{4}}{\\frac{1}{4}} = 3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0830492",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Interpretation: it's 3 times more likely that Michigan wins than loses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d14d5c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **We can interpret $\\sigma^{-1}(p) = \\log \\left( \\frac{p}{1-p} \\right)$ as the \"log odds\" of $p$!**<br><small>See the reference slides for more details.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4ea8cd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solving for the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fc3bba",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Previously, we said that if we pick a threshold $T$, then:\n",
    "\n",
    "$$\\sigma \\left( w_0^* + w_1^* \\cdot \\text{Glucose}_T \\right) = T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234f007a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We re-arranged this for the `'Glucose'` value on the threshold, $\\text{Glucose}_T$:\n",
    "\n",
    "$$\\text{Glucose}_\\text{T} = \\frac{\\sigma^{-1}(T) - w_0^*}{w_1^*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe731a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Using the fact that $\\sigma^{-1}(T) = \\log \\left( \\frac{T}{1 - T} \\right)$ gives us a closed-form formula for $\\text{Glucose}_T$!\n",
    "\n",
    "$$\\text{Glucose}_\\text{T} = \\frac{\\log \\left( \\frac{T}{1-T} \\right) - w_0^*}{w_1^*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6e53ed",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **This explains why $\\text{Glucose} \\geq 140$ is the <span style=\"color:purple\">decision boundary</span> below!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827c421",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w0_star = model_logistic.intercept_[0]\n",
    "w1_star = model_logistic.coef_[0][0]\n",
    "T = 0.5\n",
    "glucose_threshold = (np.log(T / (1 - T)) - w0_star) / w1_star\n",
    "glucose_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d456903",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_with_logistic_and_x_threshold(X_train, y_train, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19904cd3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The decision boundary in the feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aa5129",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The decision boundary on the previous slide is:\n",
    "\n",
    "$$\\text{Glucose}_T \\geq 140$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f78bf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's visualize this in the **feature space**. We are just using $d = 1$ feature, so let's visualize our decision boundary with a 1D plot, i.e. a number line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23430dc1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_in_1D(X_train, y_train, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fda4e5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression with multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ced4983",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, as we did last class, let's use both `'Glucose'` and `'BMI'` to predict diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c1dbe",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.create_base_scatter(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e980db",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Specifically, our fit model will look like:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i, \\text{BMI}_i) = \\sigma \\left( w_0^* + w_1^* \\cdot \\text{Glucose}_i + w_2^* \\cdot \\text{BMI}_i \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f48da3",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic_multiple = LogisticRegression()\n",
    "model_logistic_multiple.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a26f18",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- After minimizing mean (regularized!) cross-entropy loss, we find that our fit model is of the form:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i, \\text{BMI}_i) = \\sigma \\left( -7.85 + 0.04 \\cdot \\text{Glucose}_i + 0.08 \\cdot \\text{BMI}_i \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18332341",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic_multiple.intercept_, model_logistic_multiple.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d23f462",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing a fit logistic regression model with two features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9233dd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recall, the logistic regression model is trained to predict the probability of <b><span style=\"color:blue\">class 1 (diabetes)</span></b>.\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i, \\text{BMI}_i) = \\sigma \\left( -7.85 + 0.04 \\cdot \\text{Glucose}_i + 0.08 \\cdot \\text{BMI}_i \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d4a7a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The graph below shows the predicted probabilities of <b><span style=\"color:blue\">class 1 (diabetes)</span></b> for different combinations of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85763e07",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_logistic(model_logistic_multiple, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69794c4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The decision boundary in the feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57331d47",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What does the resulting decision boundary look like, in a $d = 2$ dimensional plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f94ca6",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_decision_boundary(model_logistic_multiple, X_train, y_train, title='Logistic Regression Decision Boundary (T = 0.5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f8099",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that unlike the decision boundaries for $k$-nearest neighbors and decision trees, this decision boundary is **linear**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae7409",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Specifically, the decision boundary in the feature space is of the form:\n",
    "\n",
    "$$a \\cdot \\text{Glucose}_i + b \\cdot \\text{BMI}_i + c = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed71418b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **In Homework 11 (released later this week), you'll solve for $a$, $b$, and $c$ in a similar example!**<br><small>It involves retracing the steps we followed in the single-feature case.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173dd1a7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ðŸ¤” (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d993e711",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "#### Reference Slide\n",
    "\n",
    "### Properties of the logistic function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eddc367",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- The logistic function, $\\sigma(t)$, obeys several interesting properties. \n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + e^{-t}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072d9ba0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- It is **symmetric**.\n",
    "\n",
    "$$\\sigma(-t) = 1 - \\sigma(t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a8405",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Its **derivative** is conveniently calculated:\n",
    "\n",
    "$$\\frac{d}{dt}\\sigma(t) = \\sigma(t) (1 - \\sigma(t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9af86ea",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- But, most relevant to us right now, its **inverse** is:\n",
    "\n",
    "$$p = \\sigma(t) \\implies t = \\sigma^{-1}(p) = \\log \\left( \\frac{p}{1-p} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec24fc0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "#### Reference Slide\n",
    "\n",
    "### Linearity of log odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f9294",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Let $p$ represent our predicted probability.\n",
    "\n",
    "$$p = P(y_i = 1 | \\text{Glucose}_i ) = \\sigma \\left( w_0^* + w_1^* \\cdot \\text{Glucose}_i \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5143b8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Using the inverse of the logistic function, we have that:\n",
    "\n",
    "$$w_0^* + w_1^* \\cdot \\text{Glucose}_i = \\log \\left( \\frac{p}{1 - p} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5a783",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- On the left, we have a **linear function of $\\text{Glucose}_i$**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac941f3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- On the right, we have the **log of the odds** of $p$.<br><small>We call the \"log of the odds\" the \"log odds\".</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6728aac",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- **Important**: The logistic regression model assumes that **the log of the odds of $P(y_i = 1 | \\vec{x}_i)$ is linear!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b796daa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "#### Reference Slide\n",
    "\n",
    "### Implications of the linearity of log odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b358959a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Suppose that $w_0^* = -6$ and $w_1^* = 0.05$. Then:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i) = \\sigma(-6 + 0.05 \\cdot \\text{Glucose}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed8465",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- It's hard to interpret the role of the coefficient $0.05$ directly. But, we know that:\n",
    "\n",
    "$$-6 + 0.05 \\cdot \\text{Glucose}_i = \\log \\left( \\frac{p}{1 - p} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39770a54",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Example: Suppose my `'Glucose'` level increases by 1 unit. Then, the predicted log odds that I have diabetes increases by 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf1bcdb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- But, since:\n",
    "\n",
    "$$e^{-6 + 0.05 \\cdot \\text{Glucose}_i} = \\frac{p}{1-p} = \\text{odds}(p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe4503a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- And:\n",
    "\n",
    "$$e^{-6 + 0.05 \\cdot (\\text{Glucose}_i + 1)} = e^{-6 + 0.05 \\cdot \\text{Glucose}_i} \\cdot e^{0.05}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72567d3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- We can say that **if my `'Glucose'` level increases by 1 unit, then my predicted odds of diabetes increases by a _factor_ of $e^{0.05}$**, or more generally $e^{w_1^*}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f87ceca",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- You'll need this interpretation in Homework 11!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dbeb4d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<h3>Question ðŸ¤” (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Which expression describes the **odds ratio**, $$\\frac{P(y_i = 1 | \\vec{x}_i)}{P(y_i = 0 | \\vec{x}_i)}$$\n",
    "    \n",
    "in the logistic regression model?\n",
    "    \n",
    "- A. $\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i)$\n",
    "- B. $-\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i)$\n",
    "- C. $e^{\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i)}$\n",
    "- D. $\\sigma(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i))$\n",
    "- E. None of the above.\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "None",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
