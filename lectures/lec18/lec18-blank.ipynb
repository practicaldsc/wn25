{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba4b65",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lec_utils import *\n",
    "import lec18_util as util\n",
    "def show_cv_slides():\n",
    "    src = \"https://docs.google.com/presentation/d/e/2PACX-1vTydTrLDr-y4nxQu1OMsaoqO5EnPEISz2VYmM6pd83ke8YnnTBJlp40NfNLI1HMgoaKx6GBKXYE4UcA/embed?start=false&loop=false&delayms=60000&rm=minimal\"\n",
    "    display(IFrame(src, width=900, height=361))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3256a1c8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Lecture 18\n",
    "\n",
    "# Generalization and Cross-Validation\n",
    "\n",
    "### EECS 398: Practical Data Science, Winter 2025\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> ‚Ä¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/wn25\">github.com/practicaldsc/wn25</a> ‚Ä¢ üì£ See latest announcements [**here on Ed**](https://edstem.org/us/courses/69737/discussion/5943734) </small>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef36909",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda üìÜ\n",
    "\n",
    "- Generalization üî≠.\n",
    "- Hyperparameters and train-test splits üéõÔ∏è.\n",
    "- Cross-validation.\n",
    "\n",
    "\n",
    "\n",
    "For additional reading, take a look at [mlu-explain.github.io](https://mlu-explain.github.io/), a site with interactive explanations for a lot of core machine learning topics, like:\n",
    "- [Linear Regression](https://mlu-explain.github.io/linear-regression/).\n",
    "- [The Bias-Variance Tradeoff](https://mlu-explain.github.io/bias-variance/).\n",
    "- [Train, Test, and Validation Sets](https://mlu-explain.github.io/train-test-validation/).\n",
    "- [Cross-Validation](https://mlu-explain.github.io/cross-validation/).\n",
    "- and other ideas we'll see later in the semester!\n",
    "We've linked these articles in the [Resources](https://practicaldsc.org/resources) tab of the course website, too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c7a41",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Remember that you can always ask questions anonymously at the link above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c4623",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization üî≠\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0436844",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c727ab24",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You and Billy are studying for an upcoming exam. You both decide to test your understanding by taking a **practice exam**.<br><small>Your logic: If you do well on the practice exam, you should do well on the real exam.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a58b6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You each take the practice exam once and look at the solutions afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74f6d3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Your strategy**: Memorize the answers to all practice exam questions, e.g. \"Question 1: A; Question 2: C; Question 3: A.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205de328",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Billy's strategy**: Learn high-level concepts from the solutions, e.g. \"the TF-IDF of term $t$ in document $d$ is large when $t$ occurs often in $d$ but rarely overall.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d534163",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Who will do better on the **practice exam**? Who will probably do better on the **real exam**? üßê"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ae8bfb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/interpolation.png\" width=700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec3f2f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating the quality of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78561e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So far, we've computed the MSE of our fit regression models on the **data that we used to fit them**, i.e. the **training data**.<br><small>This mean squared error is called the **training MSE**, or **training error**.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b902ee",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We've said that Model A is <b><span style=\"color:green\">better</span></b> than Model B if Model A's MSE is <b><span style=\"color:green\">lower</span></b> than Model B's MSE.\n",
    "    - Remember, our training data is a sample from some population.\n",
    "    - Just because a model fits the training data well doesn't mean it will **generalize** and work well on **similar, unseen samples** from the same population!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b90ff",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting and underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fe983",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's collect two samples $\\{(x_i, y_i)\\}$ from the same population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d047aa",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(23) # For reproducibility.\n",
    "def sample_from_pop(n=100):\n",
    "    x = np.linspace(-2, 3, n)\n",
    "    y = x ** 3 + (np.random.normal(0, 3, size=n))\n",
    "    return pd.DataFrame({'x': x, 'y': y})\n",
    "sample_1 = sample_from_pop()\n",
    "sample_2 = sample_from_pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee48f3f8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For now, let's just look at Sample 1. The relationship between $x$ and $y$ is roughly **cubic**; that is, $y \\approx x^3$.<br><small>Remember, in reality, you won't get to see the population distribution. If you could, there'd be no need to build a model!</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a796b3c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "px.scatter(sample_1, x='x', y='y', title='Sample 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11f7615",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a016df56",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's fit three **polynomial** models on Sample 1: degree 1, degree 3, and degree 25.<br><small>We'll use the `PolynomialFeatures` transformer, which was part of one of our Pipelines from last class.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f8ae81",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# fit_transform fits and transforms the same input.\n",
    "# We tell it not to add a column of 1s, because\n",
    "# LinearRegression() does this automatically later on.\n",
    "d2 = PolynomialFeatures(3, include_bias=False)\n",
    "d2.fit_transform(np.array([1, 2, 3, 4, -2]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba3da41",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Below, we look at our three models' predictions on Sample 1, which they were **trained** on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff53914",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Look at the definition of train_and_plot in lec17_util.py if you're curious as to how the plotting works.\n",
    "fig = util.train_and_plot(train_sample=sample_1, test_sample=sample_1, degs=[1, 3, 25], data_name='Sample 1')\n",
    "fig.update_layout(title='Trained on Sample 1, Performance on Sample 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9744bb7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The degree 25 polynomial has the lowest MSE on Sample 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a9d84",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- How do the same fit polynomials look on Sample 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b4bb5",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = util.train_and_plot(train_sample=sample_1, test_sample=sample_2, degs=[1, 3, 25], data_name='Sample 2')\n",
    "fig.update_layout(title='Trained on Sample 1, Performance on Sample 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c0083d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The degree 3 polynomial has the lowest MSE on Sample 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a076ae1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that **we didn't get to see Sample 2 when fitting our models**! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f1bbf8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As such, it seems that the degree 3 polynomial **generalizes better** to unseen data than the degree 25 polynomial does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f781afed",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- What if we fit a degree 1, degree 3, and degree 25 polynomial **on Sample 2** as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0653ab80",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = util.plot_multiple_models(sample_1, sample_2, degs=[1, 3, 25])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef9ae8c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Key idea**: Degree 25 polynomials seem to **vary more when trained on different samples** than degree 3 and 1 polynomials do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f0c81",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae06100f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The training data we have access to is a sample from the population. We are concerned with our model's ability to **generalize** and work well on **different datasets** drawn from the same population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77234b6a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we **fit** a model $H^*$ (e.g. a degree 3 polynomial) on **several different datasets** from a population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b004544",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There are three main sources of error in our model's predictions:\n",
    "    - Model bias: Two slides from now.\n",
    "    - Model variance: Next slide.\n",
    "    - **Observation error**: The error due to the random noise in the process we are trying to model (e.g. measurement error). We can't control this, without collecting more data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4785a8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6148a118",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Model variance: The variance of a model's predictions, across all datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8f368",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In other words, for a given $\\vec x_i$, how much does $H^*(\\vec x_i)$ vary across all datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ee7d0",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb6b3a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Low model variance is good! ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b8238e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- High model variance is a sign of **overfitting**, i.e. that our model is too **complicated** and is prone to fitting to the noise in our training data.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c08205d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d2c08",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Model bias: The averaged deviation between a predicted value and an actual value, across all datasets**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe6b7ea",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In other words, for a given $\\vec x_i$, how far is $H^*(\\vec x_i)$ from the true $y_i$, on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df1d8ca",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = util.plot_multiple_models(sample_1, sample_2, degs=[1, 3, 25], data=True)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208a6d7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Low bias is good! ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a4ea8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- High bias is a sign of **underfitting**, i.e. that our model is too **basic** to capture the relationship between our features and response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a8ca6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/image_5.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7812871a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Here, suppose:\n",
    "    - The <span style='color:#c6283f'><b>red bulls-eye</b></span> represents your **true weight and height** üßç.\n",
    "    - The <span style='color:#080c6f'><b>dark blue darts</b></span> represent **predictions of your weight and height** using different models that were fit using different samples drawn from the same population. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4223af",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We'd like our models to be in the top left, but in practice that's hard to achieve!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c97bf3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Risk vs. empirical risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d7b165",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since Lecture 11, we've minimized **empirical risk** to find optimal model parameters $\\vec{w}^*$:\n",
    "\n",
    "$$\\vec{w}^* = \\underset{\\vec{w}}{\\text{argmin}} \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - H(\\vec x_i) \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25015a41",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Key idea**: A model that works well on past data should work well on future data, if future data looks like past data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159f621d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What we really want is for the:\n",
    "    - **expected** loss for a new data point $(\\vec x_{\\text{new}}, y_{\\text{new}})$, \n",
    "    - drawn from the same population as the training set, to be small.\n",
    "    \n",
    "    That is, we want to minimize **risk**:\n",
    "    $$\\text{risk} = \\mathbb{E}[y_{\\text{new}} - H(\\vec x_{\\text{new}})]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc84e23",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In general, we don't know the entire population distribution of $x$s and $y$s, so we can't compute risk exactly.<br>That's why we compute **empirical** risk!\n",
    "\n",
    "$$\\mathbb{E}[y_{\\text{new}} - H(\\vec x_{\\text{new}})]^2 \\approx \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - H(\\vec x_i) \\right)^2 = R(H)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d74cec4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The bias-variance decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcff608",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Risk can be decomposed as follows:<br><small>Remember, this expectation $\\mathbb{E}$ is over the entire population of $x$s and $y$s. In real life, we don't know what this population distribution is, so we can't put actual numbers to this.</small>\n",
    "\n",
    "$$\\mathbb{E}[y_{\\text{new}} - H(\\vec x_{\\text{new}})]^2 = \\text{model bias}^2 + \\text{model variance} + \\text{observation error}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f29f9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We won't cover the proof of the decomposition here ‚Äì read [**this**](https://learningds.org/ch/17/inf_pred_gen_prob.html#probability-behind-model-selection) for more ‚Äì but note that in Homework 6, Question 3, you proved a related formula for $R_\\text{sq}(h)$:\n",
    "\n",
    "$$R_\\text{sq}(h) = \\frac{1}{n} \\sum_{i = 1}^n (y_i - h)^2 = \\underbrace{\\frac{1}{n} \\sum_{i = 1}^n (y_i - \\bar{y})^2}_{\\text{variance of } y} + (\\bar{y} - h)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34ff25c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Key takeaway**: If we care about minimizing (empirical) risk, we can equivalently try to minimize both model bias and model variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e021e4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As model variance increases, model bias tends to decrease, and vice versa.<br>That is, there is a **tradeoff** between bias and variance:\n",
    "\n",
    "<center><img src=\"imgs/bv-decomp.svg\" width=800></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93a011",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameters and train-test splits üéõÔ∏è\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd9e1f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1391112",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We recently looked at an example of **polynomial regression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92666b87",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = util.train_and_plot(train_sample=sample_1, test_sample=sample_2, degs=[1, 3, 25], data_name='Sample 2')\n",
    "fig.update_layout(title='Trained on Sample 1, Performance on Sample 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0d20b0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When building these models:\n",
    "    - We **got to choose** the degree of the polynomials ‚Äì we chose 1, 3, and 25.\n",
    "    - We didn't get to choose the exact formulas for the three polynomials ‚Äì their formulas were **learned from data**.<br><small>No matter what the data looked like, the left-most model **had** to look like a line, because we chose its degree in advance.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a5a31",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b409b3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A **parameter** defines the relationship between variables in a model. **We learn parameters from data**.\n",
    "    - For instance, suppose we fit a degree 3 polynomial to data, and end up with:\n",
    "    \n",
    "    $$H^*(x_i) = 1 - 2x_i + 13x_i^2 - 4x_i^3$$\n",
    "    \n",
    "    - 1, -2, 13, and -4 are parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82eaa3c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A **hyperparameter** is a parameter that we choose _before_ our model is fit to the data.\n",
    "    - Think of hyperparameters as knobs üéõ ‚Äì we get to pick and tune them!\n",
    "    - **Polynomial degree** was a hyperparameter in the previous example, and we tried three different values: 1, 3, and 25."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e813e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: How do we choose the \"right\" hyperparameter(s)?<br><small>Degree 3 was a better choice than degree 25, for example ‚Äì but how do we systematically choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f932226",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train-test splits üöÜ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f5b6d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we're choosing between many different models.<br><small>Here, by \"model\" we really mean \"hyperparameter value\", e.g. one \"model\" is a degree 3 polynomial, while another is a degree 4 polynomial.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee26d33",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We won't know whether a model has **overfit** to our sample unless we get to see how well it performs on a new sample from the same population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117d6cfe",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- üí°**Idea**: **Split** our dataset into a <span style='color: blue'><b>training set</b></span> and <span style='color: orange'><b>test set</b></span>.\n",
    "\n",
    "<center><img src='imgs/train-test-first.png' width=700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501a248",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For each model we're considering (e.g. each polynomial degree):\n",
    "    - Use **only** the training set to fit that model (i.e. find $\\vec{w}^*$).\n",
    "    - Use the test set to evaluate that model's error (e.g. compute its MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca96aea",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Pick the model with the **lowest** test error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d9bc17",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why? The test set is like a new sample of data from the same population as the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b861becd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train-test split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4629e70a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `sklearn.model_selection.train_test_split` implements a train-test split for us! üôèüèº "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e558a7c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If `X` is an array/DataFrame of features and `y` is an array/Series of responses,\n",
    "    ```py\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    ```\n",
    "    randomly splits the features and responses into training and test sets, such that the test set contains 0.25 of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00025055",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80c2264",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the documentation!\n",
    "train_test_split?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b481d2f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's perform a train/test split on `sample_1`, since we'll need it to find the optimal polynomial degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896d0db",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce3bdc",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = ...\n",
    "y = ...\n",
    "# We don't have to choose 0.25.\n",
    "# We also don't have to set a random_state;\n",
    "# we've done this so that we get the same results in lecture every time.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f7e7b7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Before proceeding, let's check the sizes of `X_train` and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3caf8",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('Rows in X_train:', X_train.shape[0])\n",
    "display(X_train.head())\n",
    "print('Rows in X_test:', X_test.shape[0])\n",
    "display(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55508a2b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Remember: Train _only_ using the training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b965ab",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now that we've performed a train/test split of Sample 1, we'll create models with degree 1 through 25 polynomial features and compute their train and test errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f35c48d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "train_errs = []\n",
    "test_errs = []\n",
    "for d in range(1, 26):\n",
    "    pl = make_pipeline(PolynomialFeatures(d), LinearRegression())\n",
    "    pl.fit(X_train, y_train)\n",
    "    train_errs.append(mean_squared_error(y_train, pl.predict(X_train)))\n",
    "    test_errs.append(mean_squared_error(y_test, pl.predict(X_test)))\n",
    "errs = pd.DataFrame({'Train Error': train_errs, 'Test Error': test_errs})\n",
    "errs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd350fc8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Notice that we only call `pl.fit` on the training data!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29ba5d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Polynomial degree vs. train/test error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b093383",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's look at the plots of training error vs. degree and test error vs. degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9075d8e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = px.line(errs.iloc[:-1])\n",
    "fig.update_layout(showlegend=True, xaxis_title='Polynomial Degree', yaxis_title='Mean Squared Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36693d3a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Training error appears to decrease as polynomial degree increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d5e3e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Test error appears to decrease until a \"valley\", and then increases again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86611853",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Here, we'd choose a degree of 3, since that degree has the **lowest test error**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ff730",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training error vs. test error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75692971",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The pattern we saw in the previous example is true more generally.\n",
    "\n",
    "<center><img src='imgs/tt-errors.png' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e63cab",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We pick the hyperparameter(s) at the \"valley\" of test error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec960bb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that training error **tends** to underestimate test error, but it doesn't have to ‚Äì i.e., it is possible for test error to be lower than training error (say, if the test set is \"easier\" to predict than the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120313d2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The results ‚Äì and the bias-variance tradeoff more generally ‚Äì hold true for \"classic\" machine learning models, like the ones we're studying here.<br>But in deep neural networks, this pattern is often violated; extremely complex models can have low test error as well.<br><small>This phenomenon is known as \"double descent\"; learn more [**here**](https://en.wikipedia.org/wiki/Double_descent).</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e74676",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conducting train-test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c8471f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recall, <span style='color: blue'><b>training data</b></span> is used to fit our model, and <span style='color: orange'><b>test data</b></span> is used to evaluate our model.\n",
    "\n",
    "<center><img src='imgs/train-test-first.png' width=40%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88bac15",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: _How_ should we split?\n",
    "    - `sklearn`'s `train_test_split` splits **randomly**, which usually works well.\n",
    "    - However, if there is some element of **time** in the training data (say, when predicting the future price of a stock), a better split is \"past\" and \"future\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b4bff2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: How _large_ should the split be, e.g. 90%-10% vs. 75%-25%?\n",
    "    - There's a tradeoff ‚Äì a larger training set should lead to a \"better\" model, while a larger test set should lead to a better estimate of our model's ability to generalize.\n",
    "    - There's no \"right\" choice, but we usually choose between 10% to 25% for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695547f0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### But wait..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e6877e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- With our current strategy, we are choosing the hyperparameter that creates the model that **performs best on the test set**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473656db",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As such, we are **overfitting to the test set** ‚Äì the best hyperparameter for the test set might not be the best hyperparameter for a totally unseen dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c5d638",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems like we need **another** split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372f3f33",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-validation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe881cf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Idea: A single validation set\n",
    "\n",
    "<center><img src='imgs/train-test-val.png' width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e511320",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Split the data into three sets: <span style='color: blue'><b>training</b></span>, <span style='color: green'><b>validation</b></span>, and <span style='color: orange'><b>test</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf01992",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. For each hyperparameter choice, <span style='color: blue'><b>train</b></span> the model only on the <span style='color: blue'><b>training set</b></span>, and <span style='color: green'><b>evaluate</b></span> the model's performance on the <span style='color: green'><b>validation set</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95377a2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Find the hyperparameter with the best <span style='color: green'><b>validation</b></span> performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1163941c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4. Retrain the final model on the <span style='color: blue'><b>training</b></span> and <span style='color: green'><b>validation</b></span> sets, and report its performance on the <span style='color: orange'><b>test set</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b588b1ce",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Issue**: This strategy is too dependent on the <span style='color: green'><b>validation</b></span> set, which may be small and/or not a representative sample of the data. **We're not going to do this.** ‚ùå"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3366824",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A better idea: $k$-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325212ab",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead of relying on a single <span style='color: green'><b>validation</b></span> set, we can create $k$ <span style='color: green'><b>validation</b></span> sets, where $k$ is some positive integer (5 in the example below).\n",
    "\n",
    "<center><img src='imgs/k-fold.png' width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ff302",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since each data point is used for <span style='color: blue'><b>training</b></span> $k-1$ times and <span style='color: green'><b>validation</b></span> once, the (averaged) <span style='color: green'><b>validation</b></span> performance should be a good metric of a model's ability to generalize to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e9513",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $k$-fold cross-validation (or simply \"cross-validation\") is **the** technique we will use for finding hyperparameters, or more generally, for choosing between different possible models. **It's what you should use in your Final Project!** ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a05431",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Illustrating $k$-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdef98c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To illustrate $k$-fold cross-validation, let's use the following example dataset with $n = 12$ rows.<br><small>Suppose this dataset represents our **training set**, i.e. suppose we already performed a train-test split.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a1d59",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "training_data = pd.DataFrame().assign(x=range(0, 120, 10),\n",
    "                                      y=[9, 1, 58, 3, 6, 4, -2, 8, 1, 10, 1.1, -45])        \n",
    "display_df(training_data, rows=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50aa612",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we choose $k = 4$. Then, each fold has $\\frac{12}{4} = 3$ rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6d46f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "show_cv_slides()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b64c7a0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$-fold cross-validation, in general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b23eb9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, **shuffle** the entire training set randomly and **split** it into $k$ disjoint folds, or \"slices\". Then:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cb1c4d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For each hyperparameter:\n",
    "    - For each slice:\n",
    "        - Let the slice be the \"validation set\", $V$.\n",
    "        - Let the rest of the data be the \"training set\", $T$.\n",
    "        - Train a model using the selected hyperparameter on the training set $T$.\n",
    "        - Evaluate the model on the validation set $V$.\n",
    "    - Compute the **average** validation error (e.g. MSE) for the particular hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5fea34",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Choose the hyperparameter with the **lowest** average validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a34bce9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `GridSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c40f8b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's use $k$-fold cross-validation to choose a polynomial degree that best generalizes to unseen data.<br>As before, we'll choose our polynomial degree from the list [1, 2, ..., 25]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0c2096",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `GridSearchCV` takes in:\n",
    "    - an **un-`fit`** instance of an estimator, and\n",
    "    - a **dictionary** of hyperparameter values to try,\n",
    "    \n",
    "  and performs $k$-fold cross-validation to find the **combination of hyperparameters** with the best average validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e8732b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "GridSearchCV?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8288d090",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why do you think it's called \"grid search\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f32167",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Grid searching for the best polynomial degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046d50a4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Here, we want to try values of degree from 1 through 25, so we'll need to specify these values in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa2982",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The key names in this dictionary are chosen very carefully.\n",
    "# They need to be of the format pipelinestep__hyperparametername,\n",
    "# where pipelinestep is a lowercase version of the step in the pipeline\n",
    "# that we want to tune, and \n",
    "# hyperparameter name is the formal name of the hyperparameter (see the documentation).\n",
    "hyperparams = {\n",
    "    'polynomialfeatures__degree': range(1, 26)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b562b1ed",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The scoring metric we need to provide is `'neg_mean_squared_error'`.<br><small>The `scoring` argument is used to specify that we want to compute the MSE; by default, it computes the $R^2$. It's called \"neg\" MSE because, by default, `sklearn` likes to \"maximize\" scores, and maximizing -MSE is the same as minimizing MSE.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfc75e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "searcher = GridSearchCV(\n",
    "    make_pipeline(PolynomialFeatures(), LinearRegression()),\n",
    "    param_grid=hyperparams,\n",
    "    cv=5, # k = 5.\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "searcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897b1ccf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Like any other estimator, `GridSearchCV` instances need to be `fit`.<br><small>Again, notice that we're only fitting it with our training data.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7185afa",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "searcher.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cbfb4f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Once fit, `searcher` can tell us what it found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9614b1f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "searcher.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93265a6a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `searcher` is now a fit regression model. There's no _need_ to refit it on the entire training set; it was already fit on the entire training set automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772db7cd",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "searcher.predict([[4], \n",
    "                  [-1], \n",
    "                  [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7978b30",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpreting the results of $k$-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e8f95",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's peek under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e333f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "errs_df = util.format_results(searcher)\n",
    "errs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c855900",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that for each choice of degree (our hyperparameter), we have **five** MSEs, one for each \"fold\" of the data. This means that in total, $5 \\cdot 25 = 125$ models were trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a94d9",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "errs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a7e6f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Remember, our goal is to choose the **degree** with the **lowest average** validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4fb97a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "errs_df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4371a890",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = errs_df.mean(axis=0).iloc[:18].plot(kind='line', title='Average Validation Error')\n",
    "fig.update_layout(xaxis_title='Degree', yaxis_title='Average Validation MSE', showlegend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e754a558",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Chosen automatically by sklearn.\n",
    "errs_df.mean(axis=0).idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd72969",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that if we didn't perform $k$-fold cross-validation, but instead just used a single validation set, we may have ended up with a different result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37db1c1",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "errs_df.idxmin(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c655a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "        \n",
    "- Suppose you have a training dataset with 1000 rows.\n",
    "- You want to decide between 20 hyperparameters for a particular model.\n",
    "- To do so, you perform 10-fold cross-validation.\n",
    "- **How many times is the first row in the training dataset (`X.iloc[0]`) used for training a model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeb0f17",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another example: Commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f53d5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can also use $k$-fold cross-validation to determine which subset of features to use in a linear model that predicts commute times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c612019f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/commute-times.csv')\n",
    "df['day_of_month'] = pd.to_datetime(df['date']).dt.day\n",
    "df['month'] = pd.to_datetime(df['date']).dt.month_name()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a1c36e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's make several candidate pipelines. But first, **as always**, a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea0e15",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Here, we're letting X_train and X_test keep all of the columns in the DataFrame\n",
    "# OTHER than 'minutes'.\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('minutes', axis=1), df['minutes'], random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1942f4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating many pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285cc6bb",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a698596d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "selecter = FunctionTransformer(lambda x: x) # Shortcut to say \"keep just these columns.\"\n",
    "week_converter = FunctionTransformer(lambda s: 'Week ' + ((s - 1) // 7 + 1).astype(str))\n",
    "day_of_month_transformer = make_pipeline(week_converter, OneHotEncoder(drop='first')) # From last class.\n",
    "pipes = {\n",
    "    'departure_hour only': make_pipeline(\n",
    "        make_column_transformer((selecter, ['departure_hour'])),\n",
    "        LinearRegression()\n",
    "    ),\n",
    "    'departure_hour + day_of_month': make_pipeline(\n",
    "        make_column_transformer((selecter, ['departure_hour', 'day_of_month'])),\n",
    "        LinearRegression()\n",
    "    ),\n",
    "    'departure_hour + day OHE': make_pipeline(\n",
    "        make_column_transformer(\n",
    "            (selecter, ['departure_hour']),\n",
    "            (OneHotEncoder(drop='first', handle_unknown='ignore'), ['day'])\n",
    "        ),\n",
    "        LinearRegression()\n",
    "    ),\n",
    "    'departure_hour + day OHE + month OHE': make_pipeline(\n",
    "        make_column_transformer(\n",
    "            (selecter, ['departure_hour']),\n",
    "            (OneHotEncoder(drop='first', handle_unknown='ignore'), ['day', 'month'])\n",
    "        ),\n",
    "        LinearRegression()\n",
    "    ),\n",
    "    'departure_hour with poly features + day OHE + month OHE + week': make_pipeline(\n",
    "        make_column_transformer(\n",
    "        (PolynomialFeatures(3), ['departure_hour']),\n",
    "        (OneHotEncoder(drop='first', handle_unknown='ignore'), ['day', 'month']),\n",
    "        (day_of_month_transformer, ['day_of_month']),\n",
    "    ),\n",
    "    LinearRegression())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4578194",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Here, we will have to call `GridSearchCV` multiple times. <br><small>Here, we're choosing between many different pipelines, **not** between hyperparameters for a particular pipeline.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb096889",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['Average Training MSE', 'Average Validation MSE'])\n",
    "for pipe in pipes:\n",
    "    fitted = GridSearchCV(\n",
    "        pipes[pipe],\n",
    "        param_grid={}, # No hyperparameters, but we could have them.\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=10, # Change this and see what happens!\n",
    "        return_train_score=True # So that we can compute training MSEs, too.\n",
    "    )\n",
    "    fitted.fit(X_train, y_train)\n",
    "    results.loc[pipe] = [-fitted.cv_results_['mean_train_score'][0], -fitted.cv_results_['mean_test_score'][0]]\n",
    "commute_models_summarized = (\n",
    "    results\n",
    "    .sort_values('Average Training MSE')\n",
    "    .plot(kind='barh', barmode='group', width=1000)\n",
    "    .update_layout(xaxis_title='Mean Squared Error', yaxis_title='Model')\n",
    ")\n",
    "commute_models_summarized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be04bdec",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What's the \"right\" combination of features to choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a613c56",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13daa854",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We care about how well our models **generalize** to unseen data.<br><small><ul><li>The more complex a model is, the more it will **overfit** to the noise in the training data, and have high **model variance**.</li><li>The less complex a model is, the more it will **underfit** the training data, and have high **bias**.</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ee69d4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To navigate this **bias-variance tradeoff**, we choose model complexity by choosing the model with the lowest error on unseen data.\n",
    "\n",
    "<center><img src=\"imgs/tt-errors.png\" width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe7007",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To do so, use cross-validation:\n",
    "    1. Split the data into two sets: <span style='color: blue'><b>training</b></span> and <span style='color: orange'><b>test</b></span>.\n",
    "    2. Use only the <span style='color: blue'><b>training</b></span> data when designing, training, and tuning the model.\n",
    "        - Use <span style='color: green'><b>$k$-fold cross-validation</b></span> to choose hyperparameters and estimate the model's ability to generalize.\n",
    "        - Do not ‚ùå look at the <span style='color: orange'><b>test</b></span> data in this step!\n",
    "    3. Commit to your final model and train it using the entire <span style='color: blue'><b>training</b></span> set.\n",
    "    4. Test the data using the <span style='color: orange'><b>test</b></span> data. If the performance (e.g. MSE) is not acceptable, return to step 2.\n",
    "    5. Finally, train on **all available data** and ship the model to production! üõ≥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6268dc1e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- üö® This is the process you should **always** use! üö® "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b70e52",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "        \n",
    "What questions do you have?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "None",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
