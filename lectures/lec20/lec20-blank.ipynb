{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8173ffc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lec_utils import *\n",
    "import lec20_util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a82d6cd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Lecture 20\n",
    "\n",
    "# Gradient Descent\n",
    "\n",
    "### EECS 398: Practical Data Science, Winter 2025\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> ‚Ä¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/wn25\">github.com/practicaldsc/wn25</a> ‚Ä¢ üì£ See latest announcements [**here on Ed**](https://edstem.org/us/courses/69737/discussion/5943734) </small>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<script type=\"text/x-mathjax-config\">\n",
    " MathJax.Hub.Config({\n",
    "   TeX: {\n",
    "     extensions: [\"color.js\"],\n",
    "     packages: {\"[+]\": [\"color\"]},\n",
    "   }\n",
    " });\n",
    " </script>\n",
    " <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7723d639",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda üìÜ\n",
    "\n",
    "- Intuition for gradient descent üóª.\n",
    "- When is gradient descent guaranteed to work?\n",
    "- Gradient descent for functions of multiple variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1e6e9f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/motivation.png\" width=1000><br><small>What we're building towards today.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f8852",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Remember that you can always ask questions anonymously at the link above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12518168",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuition for gradient descent üóª\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29110ba6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's go hiking!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5710ca24",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Suppose you're at the top of a mountain üèîÔ∏è and need to get **to the bottom**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb93da7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Further, suppose it's really cloudy ‚òÅÔ∏è, meaning you can only see a few feet around you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f0cc7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- **How** would you get to the bottom?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7193be",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimizing arbitrary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b16c24",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Assume $f(w)$ is some **differentiable** function.<br><small>For now, we'll assume $f$ takes in a scalar, $w$, as input and returns a scalar as its output.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f369eaf4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When tasked with minimizing $f(w)$, our general strategy has been to:<br>\n",
    "    1. Find $\\frac{df}{dw}(w)$, the derivative of $f$.\n",
    "    2. Find the input $w^*$ such that $\\frac{df}{dw}(w^*) = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ebd701",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- However, there are cases where we can find $\\frac{df}{dw}(w)$, but **it is either difficult or impossible to solve $\\frac{df}{dw}(w^*) = 0$**. Then what?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bed6bb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$f(w) = 5w^4 - w^3 - 5w^2 + 2w - 9$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc5ed6",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.draw_f()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246046aa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does the derivative of a function tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ff4bc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Goal**: Given a **differentiable** function $f(w)$, find the input $w^*$ that minimizes $f(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85727f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What does $\\frac{d}{dw} f(w)$ mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f7be4",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "interact(util.show_tangent, w0=(-1.5, 1.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaf9737",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Searching for the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f69ea2c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we're given an initial _guess_ for a value of $w$ that minimizes $f(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0bafc",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"imgs/positive-slope.png\" width=500>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f96c1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the <span style=\"color:red\">**slope of the tangent line at $f(w)$**</span> is **positive** üìà:\n",
    "    - Increasing $w$ **increases** $f$.\n",
    "    - This means the minimum must be to the **left** of the point $(w, f(w))$.\n",
    "    - Solution: **Decrease** $w$ ‚¨áÔ∏è."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ee047",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The steeper the slope is, the further we must be from the minimum ‚Äì so, the steeper the slope, the quicker we should decrease $w$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d72b5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Searching for the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7734d28b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we're given an initial _guess_ for a value of $w$ that minimizes $f(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0b312",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"imgs/negative-slope.png\" width=500>\n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394be6d0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the <span style=\"color:red\">**slope of the tangent line at $f(w)$**</span> is **negative** üìâ:\n",
    "    - Increasing $w$ **decreases** $f$.\n",
    "    - This means the minimum must be to the **right** of the point $(w, f(w))$.\n",
    "    - Solution: **Increase** $w$ ‚¨ÜÔ∏è."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9bd957",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The steeper the slope is, the further we must be from the minimum ‚Äì so, the steeper the slope, the quicker we should increase $w$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70e3029",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad10cdba",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To minimize a **differentiable** function $f$:\n",
    "    1. Pick a positive number, $\\alpha$. This number is called the **learning rate**, or **step size**.<br><small>Think of $\\alpha$ as a hyperparameter of the minimization process.</small>\n",
    "    2. Pick an **initial guess**, $w^{(0)}$.\n",
    "    3. Then, repeatedly update your guess using the **update rule**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d1e0c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$w^{(t+1)} = w^{(t)} - \\alpha \\frac{df}{dw}(w^{(t)})$$\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc45ae",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Repeat this process until **convergence** ‚Äì that is, when the difference between $w^{(t)}$ and $w^{(t+1)}$ is small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488f4052",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This procedure is called **gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36146efb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae98f1c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Gradient descent is a numerical method for finding the input to a function $f$ that minimizes the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6df3a7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It is called **gradient descent** because the gradient is the extension of the derivative to functions of multiple variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4f093c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A **numerical method** is a technique for approximating the solution to a mathematical problem, often by using the computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e8fa3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Gradient descent is **widely used** in machine learning, to train models from linear regression to neural networks and transformers (including ChatGPT)!<br><small>In machine learning, we use gradient descent to minimize empirical risk when we can't minimize it by hand, which is true in most, more sophisticated cases.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d16a7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ca2c39",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In practice, we typically don't implement gradient descent ourselves ‚Äì we rely on existing implementations of it. But, we'll implement it here ourselves to understand what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427fb75a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's start with an initial guess $w^{(0)} = 0$ and a learning rate $\\alpha = 0.01$.\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\alpha \\frac{df}{dw}(w^{(t)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d6978",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f974615f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We see that pretty quickly, $w^{(t)}$ converges to $-0.727$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672ba4a0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing $w^{(0)} = 0, \\alpha = 0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be76306",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "util.minimizing_animation(w0=0, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043fe516",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing $w^{(0)} = 1.1, \\alpha = 0.01$\n",
    "\n",
    "What if we start with a different initial guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af483b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.minimizing_animation(w0=1.1, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c45715d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing $w^{(0)} = 0, \\alpha = 0.1$\n",
    "\n",
    "What if we use a different learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a9ba8",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.minimizing_animation(w0=0, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00245dfd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing $w^{(0)} = 0, \\alpha = 1$\n",
    "\n",
    "Some learning rates are so large that the values of $w$ explode towards infinity! Watch what happens when we use a learning rate of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b861d96",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w = 0\n",
    "for t in range(50):\n",
    "    print(round(w, 4), round(util.f(w), 4))\n",
    "    w = w - 1 * util.df(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d5c8f1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lingering questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f151ed",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When is gradient descent _guaranteed_ to converge to a global minimum? What kinds of functions work well with gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c616271",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How do we choose a step size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468aad99",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How do we use gradient descent to minimize functions of multiple variables, e.g.:\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$\n",
    "\n",
    "<center><small>This is a function of $d+1$ variables: $w_0, w_1, ..., w_d$.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46148ac4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: Why **can't** we use gradient descent to find $\\vec{w}_\\text{LASSO}^*$?\n",
    "\n",
    "$$R_\\text{LASSO}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d |w_d|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ae21b1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When is gradient descent guaranteed to work?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d48c67c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What makes a function convex?\n",
    "\n",
    "<div style=\"width: 100%;\">\n",
    "<div style=\"width: 50%; float: left\"> \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"imgs/convex.png\">\n",
    "<center>A <b>convex</b> function ‚úÖ.</center>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-left: 50%\" markdown=\"1\"> \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"imgs/non-convex.png\">\n",
    "<center>A <b>non-convex</b> function ‚ùå.</center>\n",
    "\n",
    "</div/>\n",
    "</div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6193f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Intuitive definition of convexity\n",
    "\n",
    "<div style=\"width: 100%;\">\n",
    "<div style=\"width: 50%; float: left\"> \n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"imgs/convex.png\" width=70%></center>\n",
    "<center>A <b>convex</b> function ‚úÖ.</center>\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-left: 50%\" markdown=\"1\"> \n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"imgs/non-convex.png\" width=70%></center>\n",
    "<center>A <b>non-convex</b> function ‚ùå.</center>\n",
    "\n",
    "</div/>\n",
    "</div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963194af",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A function $f$ is **convex** if, for **every** $a, b$ in the domain of $f$, the line segment between:\n",
    "\n",
    "  $$(a, f(a)) \\text{ and } (b, f(b))$$\n",
    "\n",
    "  does not go below the plot of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b55aa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- See the reference slides for the formal definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dce4f3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "#### Reference Slide\n",
    "\n",
    "### Formal definition of convexity\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div style=\"width: 100%;\">\n",
    "<div style=\"width: 55%; float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9eaa8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- A function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ is **convex** if, for **every** $a, b$ in the domain of $f$, and for every $t \\in [0, 1]$:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\boxed{(1 - t) f(a) + t f(b) \\geq f((1-t)a + tb)}$$\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "- This is a formal way of restating the definition from the previous slide.\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"margin-left: 57%\" markdown=\"1\"> \n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"imgs/convex-definition.png\" width=100%>\n",
    "\n",
    "</center>\n",
    "\n",
    "</div/>\n",
    "</div/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e4a87",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- The slider below depicts an interactive version of the formal definition of convexity above.<br>Drag the `a`, `b`, and `t` sliders and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6cf01",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "interact(util.convexity_visual, a=(-20, 5, 0.1), b=(5, 20, 0.1), t=FloatSlider(min=0, max=1, step=0.01, value=0.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52871ee",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Second derivative test for convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03097242",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $f(w)$ is a function of a single variable and is **twice** differentiable, then $f(w)$ is convex **if and only if**:\n",
    "\n",
    "$$\\frac{d^2f}{dw^2}(w) \\geq 0, \\:\\:\\: \\forall \\: w$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bcd388",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: $f(w) = w^4$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8f9c84",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why does convexity matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f3342b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Convex functions are (relatively) easy to minimize with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e10aec7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Theorem**: If $f(w)$ is convex and differentiable, then gradient descent converges to a **global minimum** of $f$, as long as the step size is small enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1443c675",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Why?**\n",
    "  - Gradient descent converges when the derivative is 0.\n",
    "  - For convex functions, the derivative is 0 only at one place ‚Äì the global minimum.\n",
    "  - In other words, if $f$ is convex, gradient descent won't get \"stuck\" and terminate in places that aren't global minimums (local minimums, saddle points, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d32e6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nonconvex functions and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4191b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We say a function is **nonconvex** if it does not meet the criteria for convexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a60149a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Nonconvex functions are (relatively) difficult to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d94c00",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Gradient descent **might** still work, but it's not guaranteed to find a global minimum.\n",
    "  - We saw this at the start of the lecture, when trying to minimize $f(w) = 5w^4 - w^3 - 5w^2 + 2w - 9$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69532a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing a step size in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0aed8f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In practice, choosing a step size involves a lot of trial-and-error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5340547b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In this class, we've only touched on \"constant\" step sizes, i.e. where $\\alpha$ is a constant.\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\alpha \\frac{df}{dw}(w^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e4172",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Remember**: $\\alpha$ is the \"step size\", but the amount that our guess for $w$ changes is $\\alpha \\frac{df}{dw}(w^{(t)})$, not just $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be060b0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In future courses, you may learn about \"decaying\" step sizes, where the value of $\\alpha$ decreases as the number of iterations increases.<br><small>Intuition: take much bigger steps at the start, and smaller steps as you progress, as you're likely getting closer to the minimum.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76645ae",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient descent for functions of multiple variables\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f53ab1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimizing functions of multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f06ed",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We will typically use gradient descent to minimize empirical risk functions, $R(\\vec w)$, to find optimal model parameters.<br>A model with $d+1$ parameters $w_0, w_1, ..., w_d$ will have a $(d+2)$-dimensional loss surface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af13aa9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Consider the following example function $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$:\n",
    "\n",
    "$$f(w_1, w_2) = 3 \\sin(2 w_1) \\cos(2 w_2) + w_1^2 + w_2^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6151d5b6",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.make_3D_surface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063cf416",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "util.make_3D_contour()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988876c3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimizing functions of multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecacd977",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Consider the function:\n",
    "\n",
    "$$f(w_1, w_2) = 3 \\sin(2 w_1) \\cos(2 w_2) + w_1^2 + w_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a7f2b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It has two **partial derivatives**: $\\frac{\\partial f}{\\partial w_1}$ and $\\frac{\\partial f}{\\partial w_2}$.<br><small>See the annotated slides for what they are and how we find them.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099298b7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The gradient vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fe27f6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $f(\\vec{w})$ is a function of multiple variables, then its **gradient**, $\\nabla f (\\vec{w})$, is a vector containing its partial derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40be378",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: \n",
    "\n",
    "$$f(\\vec{w}) = f(w_1, w_2) = 3 \\sin(2 w_1) \\cos(2 w_2) + w_1^2 + w_2^2$$\n",
    "\n",
    "$$\\nabla f(\\vec w) = \\begin{bmatrix} 6\\cos(2w_1)\\cos(2w_2) + 2w_1 \\\\ -6\\sin(2w_1)\\sin(2w_2) + 2w_2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a022e3c4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example:\n",
    "\n",
    "$$f(\\vec{w}) = \\vec{w}^T \\vec{w}$$\n",
    "\n",
    "$$\\nabla f(\\vec{w}) = 2 \\vec{w}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b0569",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does the gradient vector describe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe240a1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Consider the visual example from two slides ago.\n",
    "\n",
    "$$f(\\vec{w}) = f(w_1, w_2) = 3 \\sin(2 w_1) \\cos(2 w_2) + w_1^2 + w_2^2$$\n",
    "\n",
    "$$\\nabla f(\\vec w) = \\begin{bmatrix} 6\\cos(2w_1)\\cos(2w_2) + 2w_1 \\\\ -6\\sin(2w_1)\\sin(2w_2) + 2w_2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf8da9",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.make_3D_contour(with_gradient=True, w1_start=-0.2, w2_start=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bc1187",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The <b><span style=\"color:gold\">gradient vector</span></b> at a point ($w_1, w_2$) describes the **direction of steepest ascent**, i.e. the direction in which the function $f$ is **increasing the quickest**, when standing at $(w_1, w_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12887760",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Moving _against_ the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e770d68",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The <b><span style=\"color:gold\">gradient vector</span></b> at a point ($w_1, w_2$) describes the **direction of steepest ascent**, i.e. the direction in which the function $f$ is **increasing the quickest**, when standing at $(w_1, w_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ebf995",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To minimize $f$, when starting at ($w_1, w_2$), we should move in the direction <b><span style=\"color:red\">opposite to the gradient</span></b>!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87138633",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.make_3D_contour(with_gradient=True, w1_start=-0.2, w2_start=0.5, neg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd6561",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent for functions of multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023bcb72",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: \n",
    "\n",
    "$$f(\\vec{w}) = f(w_1, w_2) = 3 \\sin(2 w_1) \\cos(2 w_2) + w_1^2 + w_2^2$$\n",
    "\n",
    "$$\\nabla f(\\vec w) = \\begin{bmatrix} 6\\cos(2w_1)\\cos(2w_2) + 2w_1 \\\\ -6\\sin(2w_1)\\sin(2w_2) + 2w_2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c809ca8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The global minimizer* of $f$ is a vector, $\\vec{w}^* = \\begin{bmatrix} w_1^* \\\\ w_2^* \\end{bmatrix}$.<br><small>*If one exists.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f602f0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We start with an initial guess, $\\vec{w}^{(0)}$, and step size $\\alpha$, and update our guesses using:\n",
    "\n",
    "$$\\vec{w}^{(t+1)} = \\vec{w}^{(t)} - \\alpha \\nabla f(\\vec{w}^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8efb6b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent for functions of multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba0a8ff",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's visualize the execution of gradient descent on our trigonometric example.<br>Change `w1_start`, `w2_start`, and `step_size` below and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecae6a9",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.display_paths(w1_start=1, w2_start=-0.5, iterations=10, step_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949359d8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "### Activity\n",
    "    \n",
    "Consider the following function.\n",
    "\n",
    "$$f(w_1, w_2) = (w_1-2)^2 + 2w_1 - (w_2-3)^2$$\n",
    "    \n",
    "<br>\n",
    "\n",
    "Given an initial guess of $\\vec{w}^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ and a step size of $\\alpha = \\frac{1}{3}$, perform **two** iterations of gradient descent. What is $\\vec{w}^{(2)}$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dadecd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1181a5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Gradient descent for simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea455c09",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To find optimal model parameters for the model $H(x_i) = w_0 + w_1 x_i$ and squared loss, we minimized empirical risk:\n",
    "\n",
    "$$R_\\text{sq}(w_0, w_1) = R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\sum_{i = 1}^n ( y_i - (w_0 + w_1 x_i ))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c7a62",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is a function of multiple variables, and is differentiable, so it has a gradient!\n",
    "\n",
    "$$\\nabla R(\\vec{w}) = \\begin{bmatrix} \\displaystyle -\\frac{2}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1 x_i)) \\\\ \\displaystyle -\\frac{2}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1x_i))x_i  \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb81a9e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Key idea**: To find $\\vec{w}^* = \\begin{bmatrix} w_0^* \\\\ w_1^* \\end{bmatrix}$, we _could_ use gradient descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a47c6a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why would we, when closed-form solutions exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5a50e3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent for simple linear regression, visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f5daff",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('oMk6sP7hrbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a77f1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent for simple linear regression, implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae78b22",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's use gradient descent to fit a simple linear regression model to predict commute time in `'minutes'` from `'departure_hour'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f8ed7b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/commute-times.csv')\n",
    "df[['departure_hour', 'minutes']]\n",
    "util.make_scatter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5215fb28",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = df['departure_hour']\n",
    "y = df['minutes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1aeccb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, let's remind ourselves what $w_0^*$ and $w_1^*$ are supposed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf32bd",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "slope = np.corrcoef(x, y)[0, 1] * np.std(y) / np.std(x)\n",
    "slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154e47cc",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "intercept = np.mean(y) - slope * np.mean(x)\n",
    "intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c778bbdf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing partial derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca3d05",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\sum_{i = 1}^n ( y_i - (w_0 + w_1 x_i ))^2$$\n",
    "\n",
    "$$\\nabla R(\\vec{w}) = \\begin{bmatrix} \\displaystyle -\\frac{2}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1 x_i)) \\\\ \\displaystyle -\\frac{2}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1x_i))x_i  \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0421f0da",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def dR_w0(w0, w1):\n",
    "    return -2 * np.mean(y - (w0 + w1 * x))\n",
    "def dR_w1(w0, w1):\n",
    "    return -2 * np.mean((y - (w0 + w1 * x)) * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9211393",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Implementing gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d5e1c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The update rule we'll follow is:\n",
    "\n",
    "$$\\vec{w}^{(t+1)} = \\vec{w}^{(t)} - \\alpha \\nabla R(\\vec{w}^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b4850c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can treat this as two separate update equations:\n",
    "\n",
    "$$w_0^{(t+1)} = w_0^{(t)} - \\alpha \\frac{\\partial R}{\\partial w_0} (\\vec{w}^{(t)}) \\\\ w_1^{(t+1)} = w_1^{(t)} - \\alpha \\frac{\\partial R}{\\partial w_1} (\\vec{w}^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b718dc5a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's initialize $w_0^{(0)} = 100$ and $w_1^{(0)} = -50$, and choose the step size $\\alpha = 0.01$.<br><small>The initial guesses were just parameters that we thought might be close.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c37efc",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We'll store our guesses so far, so we can look at them later.\n",
    "def gradient_descent_for_regression(w0_initial, w1_initial, alpha, threshold=0.0001):\n",
    "    w0, w1 = w0_initial, w1_initial\n",
    "    w0_history = [w0]\n",
    "    w1_history = [w1]\n",
    "    while True:\n",
    "        w0 = w0 - alpha * dR_w0(w0, w1)\n",
    "        w1 = w1 - alpha * dR_w1(w0, w1)\n",
    "        w0_history.append(w0)\n",
    "        w1_history.append(w1)\n",
    "        if np.abs(w0_history[-1] - w0_history[-2]) <= threshold:\n",
    "            break\n",
    "    return w0_history, w1_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75201c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w0_history, w1_history = gradient_descent_for_regression(0, 0, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e6bfc",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w0_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b448f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w1_history[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2882443c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems that we converge at the right value! But how many iterations did it take? What could we do to speed it up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf4f46",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(w0_history)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "None",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
