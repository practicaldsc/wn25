{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e78a4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to get everything set up.\n",
    "from lec_utils import *\n",
    "import lec23_util as util\n",
    "diabetes = pd.read_csv('data/diabetes.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "diabetes = diabetes[(diabetes['Glucose'] > 0) & (diabetes['BMI'] > 0)]\n",
    "X_train, X_test, y_train, y_test = (\n",
    "    train_test_split(diabetes[['Glucose', 'BMI']], diabetes['Outcome'], random_state=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c8a91",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Lecture 23\n",
    "\n",
    "# Logistic Regression, Continued\n",
    "\n",
    "### EECS 398: Practical Data Science, Winter 2025\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> ‚Ä¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/wn25\">github.com/practicaldsc/wn25</a> ‚Ä¢ üì£ See latest announcements [**here on Ed**](https://edstem.org/us/courses/69737/discussion/5943734) </small>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<script type=\"text/x-mathjax-config\">\n",
    " MathJax.Hub.Config({\n",
    "   TeX: {\n",
    "     extensions: [\"color.js\"],\n",
    "     packages: {\"[+]\": [\"color\"]},\n",
    "   }\n",
    " });\n",
    " </script>\n",
    " <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57abecea",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda üìÜ\n",
    "\n",
    "- Recap: Logistic regression.\n",
    "- Choosing a threshold.\n",
    "- Linear separability.\n",
    "- Softmax regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1084268",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Remember that you can always ask questions anonymously at the link above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e68674c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Logistic regression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eff8bd0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb220939",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Logistic **regression** is a linear **classification** technique that builds upon linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204eab62",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It models **the probability of belonging to class 1, given a feature vector**:\n",
    "    \n",
    "$$P(y_i = 1 | \\vec{x}_i) = \\sigma (w_0 + w_1 x_i^{(1)} + w_2 x_i^{(2)} + ... + w_d x_i^{(d)}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e48207",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we train a logistic regression model to predict the probability a patient has diabetes ($y = 1$) given their `'Glucose'` and `'BMI'`.<br>If our optimal parameters end up being $\\vec{w}^* = \\begin{bmatrix} -7.85 & 0.04 & 0.08 \\end{bmatrix}^T$, we then predict probabilities using:\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i, \\text{BMI}_i) = \\sigma(‚àí7.85 + 0.04 \\cdot \\text{Glucose}_i + 0.08 \\cdot \\text{BMI}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8c2e2c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To find the optimal parameters $\\vec{w}^*$, we minimize mean **cross-entropy loss**:\n",
    "<br><small>There's no closed-form solution for $\\vec{w}^*$, so we use some numerical method (or, rather, `sklearn` does).</small>\n",
    "\n",
    "\\begin{align*}R_\\text{ce}(\\vec{w}) &= - \\frac{1}{n} \\sum_{i = 1}^n \\left( y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right) \\\\ &= - \\frac{1}{n} \\sum_{i = 1}^n \\left[ y_i \\log \\left( \\sigma \\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right) \\right)  + (1 - y_i) \\log \\left(1 - \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)\\right) \\right]\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c2c94d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `LogisticRegression` in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d23c671",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To illustrate, let's re-fit a model to predict diabetes from `'Glucose'` and `'BMI'` in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb65d44f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_logistic_multiple = LogisticRegression()\n",
    "model_logistic_multiple.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d451ba3a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By default, the `predict` method of a fit `LogisticRegression` model predicts a **class**; it applies a threshold $T = 0.5$ to the predicted probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131ea5fc",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic_multiple.predict(pd.DataFrame([{\n",
    "    'Glucose': 150,\n",
    "    'BMI': 25,\n",
    "}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad8edb3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can access the predicted **probabilities** using the `predict_proba` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787df37d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_logistic_multiple.predict_proba(pd.DataFrame([{\n",
    "    'Glucose': 150,\n",
    "    'BMI': 25,\n",
    "}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb0400c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The decision boundary in the feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962793c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- After choosing $T = 0.5$, what does the resulting <b><span style=\"color:purple\">decision boundary</span></b> look like, in a $d = 2$ dimensional plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65fe11",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_decision_boundary(model_logistic_multiple, X_train, y_train, title='Logistic Regression Decision Boundary (T = 0.5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54556cbd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that unlike the decision boundaries for $k$-Nearest Neighbors and decision trees, this decision boundary is **linear**. Specifically, it is the line:\n",
    "\n",
    "$$\\sigma(‚àí7.85 + 0.04 \\cdot \\text{Glucose}_i + 0.08 \\cdot \\text{BMI}_i) = 0.5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beebf7f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Important**: Since $\\sigma(0) = 0.5$, we can write the above as:\n",
    "\n",
    "$$-7.85 + 0.04 \\cdot \\text{Glucose}_i + 0.08 \\cdot \\text{BMI}_i = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbebb956",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Which expression describes the **odds ratio**, $$\\frac{P(y_i = 1 | \\vec{x}_i)}{P(y_i = 0 | \\vec{x}_i)}$$\n",
    "    \n",
    "in the logistic regression model?\n",
    "    \n",
    "- A. $\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i)$\n",
    "- B. $-\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i)$\n",
    "- C. $e^{\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i)}$\n",
    "- D. $\\sigma(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i))$\n",
    "- E. None of the above.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d02ee90",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Which expression describes $P(y_i = \\mathbf{0} | \\vec{x}_i)$ in the logistic regression model?\n",
    "    \n",
    "- A. $\\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$\n",
    "- B. $-\\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$\n",
    "- C. $\\sigma\\left(- \\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$\n",
    "- D. $1 - \\log \\left( 1 + e^{\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i)} \\right)$\n",
    "- E. $1 + \\log \\left( 1 + e^{- \\vec{w} \\cdot \\text{Aug}(\\vec{x}_i)} \\right)$\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5190868d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing a threshold\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4ba188",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92e5c5e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As we've seen, in order to classify $\\vec{x}_i$ as either yes ($y_i = 1$) or no ($y_i = 0$), we apply a **threshold** $T$ to the predicted probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24864214",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"imgs/threshold.svg\" width=600><small>With a threshold of $T = 0.6$, a predicted probability of 0.68 is classified as <span style=\"color:blue\">yes diabetes (class 1)</span>,<br>and a predicted probability of 0.55 is classified as <span style=\"color:orange\">no diabetes (class 0)</span>.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc7325",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- More generally, if we pick a threshold of $T$, then any feature vector $\\vec{x}_i$ such that:\n",
    "\n",
    "    $$\\sigma(\\vec{w}^* \\cdot \\text{Aug}(\\vec{x}_i)) \\geq T$$ \n",
    "\n",
    "    is classified as class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9394dbe",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: How do we choose the \"right\" threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4945861f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `sklearn`'s default threshold of $T = 0.5$ is **not** guaranteed to yield the highest **accuracy**!<br><small>Remember, to find $\\vec{w}^*$, we minimized mean cross-entropy loss (that is, we didn't \"maximize\" accuracy), and mean cross-entropy loss doesn't involve our threshold.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc4a8ab",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing a custom threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6a6338",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we want to use a custom threshold, we'll need to implement the logic ourselves.\n",
    "\n",
    "<center><img src=\"imgs/threshold.svg\" width=300></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d21a0",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict_thresholded(X, T):\n",
    "    '''Calls model_logistic_multiple.predict_proba.\n",
    "       For each P(y_i = 1 | x_i), returns 1 if >= T and 0 if < T.'''\n",
    "    probs = model_logistic_multiple.predict_proba(X)[:, 1]\n",
    "    return (probs >= T).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c6ad4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, we can choose any threshold we'd like, and compute the accuracy of the resulting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ea818",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predict_thresholded([[150, 25]], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b7aae",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predict_thresholded([[150, 25]], 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0829b4",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predict_thresholded(X_train, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97dac36",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Training accuracy for the threshold T = 0.4.\n",
    "(predict_thresholded(X_train, 0.4) == y_train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd364de",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accuracy vs. threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2581faf7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Accuracy is defined as:\n",
    "\n",
    "$$\\text{accuracy} = \\frac{\\text{# points classified correctly}}{\\text{# points}} = \\frac{TP + TN}{TP + FP + FN + TN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e577c29",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How does the model's **training** accuracy change as the threshold changes?<br><small>Note that we'd see a similar trend with test accuracy, too.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d17ec",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_vs_threshold(X_train, y_train, 'Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b20ea2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The threshold with the best training accuracy (among the thresholds we tried) is $T = 0.465$, which has a training accuracy of 77.3\\%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b992c27",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Remember that 64\\% of people in the training set don't have diabetes, so we can achieve a 64\\% training accuracy just by always predicting \"no diabetes\"! This means that a good model's accuracy should be much higher than 64\\%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593f1b9",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a9cd38",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Metrics for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eea70e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A few lectures ago, we introduced other metrics for measuring the quality of a binary classifier's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed6d13",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{precision} = \\frac{TP}{\\text{# predicted positive}} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "<center><small>Here, a false positive ($FP$) is when we predict that someone has diabetes when they do not.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae7820",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{recall} = \\frac{TP}{\\text{# actually positive}} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "<center><small>Here, a false negative ($FN$) is when we predict that someone does not have diabetes, when they really do.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785dc850",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A binary classifier's **confusion matrix** displays its number of true positives ($TP$), false positives ($FP$), true negatives ($TN$), and false negatives ($FN$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b9c113",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_confusion(X_train, y_train, T=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95bcf12",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Remember, we're predicting whether or not patients have diabetes. **Which is worse: a false positive or a false negative?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e8fb3c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Observe how the values in the confusion matrix change as the threshold changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc72334",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "interact(lambda T: util.show_confusion(X_train, y_train, T), T=(0, 1, 0.01));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67790b7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision vs. threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f62b8f0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Precision is defined as:\n",
    "\n",
    "    $$\\text{precision} = \\frac{TP}{\\text{# predicted positive}} = \\frac{TP}{TP + FP}$$\n",
    "    \n",
    "    Here, a false positive ($FP$) is when we predict that someone has diabetes when they do not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a892e967",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How does the model's training **precision** change as the threshold changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2811f3f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_vs_threshold(X_train, y_train, 'Precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec75f8a3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the \"bar\" is higher to predict 1, then we will have fewer positives in general, and thus fewer false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34535524",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As the **threshold increases** ‚¨ÜÔ∏è, the denominator in $\\text{precision} = \\frac{TP}{TP + FP}$ will decrease, and so **precision tends to increase** ‚¨ÜÔ∏è.<br><small>There are some cases where a slightly higher threshold led to a slightly lower precision; why?</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5a7ebf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall vs. threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f7d387",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recall is defined as:\n",
    "\n",
    "    $$\\text{recall} = \\frac{TP}{\\text{# actually positive}} = \\frac{TP}{TP + FN}$$\n",
    "    \n",
    "    Here, a false negative ($FN$) is when we predict that someone does not have diabetes, when they really do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b5f486",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How does the model's training **recall** change as the threshold changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7840ad",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_vs_threshold(X_train, y_train, 'Recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb478cb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that the denominator in $\\text{recall} = \\frac{TP}{\\text{# actually positive}}$ is constant. As the **threshold increases** ‚¨ÜÔ∏è:\n",
    "    - true positives get converted to false negatives, so\n",
    "    - the numerator of recall ($TP$) decreases, and so\n",
    "    - **recall decreases** ‚¨áÔ∏è."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080547ae",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision vs. recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fe81c0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can visualize how precision and recall vary **together**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d7335",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.pr_curve(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ef081",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The curve above is called a **PR curve**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9934a14",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: Given the information above, what threshold would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9b722",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Answer**: The threshold whose point is closest to the **top right corner** of the plot above. <br><small>Why? The top right corner is where precision = 1 and recall = 1, and we want both to be high.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8badf95",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb55cef6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A more popular variant of the PR curve is the **ROC curve**.<br><small>ROC stands for \"receiver operating characteristic.\"<br>See [**here**](https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves) for a good discussion on the differences between PR curves and ROC curves.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb22df",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A ROC curve plots true positive rate (TPR) vs. false positive rate (FPR) for all possible thresholds, where:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ecd5fc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\underbrace{\\text{true positive rate (TPR)} = \\frac{TP}{\\text{# actually positive}} = \\frac{TP}{TP + FN} = \\text{recall}}_\\text{we want this to be close to 1!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d099e717",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\underbrace{\\text{false positive rate (FPR)} = \\frac{FP}{\\text{# actually negative}} = \\frac{FP}{FP + TN}}_\\text{we want this to be close to 0!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91575296",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The ROC curve for our classifier looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5f804",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.draw_roc_curve(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bff43a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we care about TPR and FPR equally, the best threshold is the one whose point is closest to the **top left corner** in the plot above.<br><small>Why? The top left corner is where $TPR = 1$ and $FPR = 0$, and we want $TPR$ to be high and $FPR$ to be low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7662a1b4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A common metric for the quality of a binary classifier is the **area under curve (AUC)** for the ROC curve.<br><small>Larger values are better!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24320849",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have about thresholds and logistic regression?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e0b6c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear separability\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167280cd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb39e5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we're using $d$ features as inputs to our classifier. Consider a visualization of the features in $d$-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ed979",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: $d = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f6e42",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_one_feature_plot_in_1D(X_train, y_train, thres=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65a3b6c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: $d = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a4b536",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "util.create_base_scatter(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0df92b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that in both plots above, there are <span style=\"color:orange\">orange points</span> mixed in with the <span style=\"color:blue\">blue points</span>!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe2d22",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear separability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a3d609",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A dataset is **linearly separable** if a line, plane, or hyperplane can be drawn in $d$-dimensional space that **perfectly separates** the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0e3a1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Example: $d = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5428d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.lin_sep_1D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d509a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.non_lin_sep_1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f18c34",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Example: $d = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee9fb6",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.lin_sep_2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca116d6",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.non_lin_sep_2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a1c2c4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Why is the dataset below **not** linearly separable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3ccd8d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "util.bad_example_1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec15856f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear separability and decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2f7fe5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By definition, if a dataset is linearly separable, then there exists a **<span style=\"color:purple\">linear decision boundary</span>** that achieves 100\\% training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b51b4e2",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.lin_sep_1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f9d348",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Above, any value of $c$ in $(120, 150)$ would make the <b><span style=\"color:purple\">decision boundary</span></b> $$\\text{Glucose} = c$$\n",
    "achieve 100% training accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b230c3b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: How do we find this decision boundary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f52800",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic regression and linear separability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d40f4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Logistic regression, **without regularization**, **fails to converge** on linearly separable data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0689c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's re-draw the plot below, but with diabetes status drawn on the $y$-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb66248",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.lin_sep_1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9367168a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why would the optimal $w_1^*$ below tend to $\\infty$?<br><small>See the annotated slides for more details.</small>\n",
    "\n",
    "$$P(y_i = 1 | \\text{Glucose}_i) = \\sigma(w_0 + w_1 \\cdot \\text{Glucose}_i) = \\frac{1}{1 + e^{-(w_0 + w_1 \\cdot \\text{Glucose}_i)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf34b91",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.lin_sep_1D_elevated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d02c237",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To prevent this case, logistic regression should generally be regularized.<br><small>This is exactly why `sklearn` regularizes logistic regression by default.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ddb41",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic regression for multiclass classification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c50ea4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From binary to multiclass classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0bd56a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In binary classification, there are only two possible classes, typically either 0 or 1.\n",
    "\n",
    "$$y_i \\in \\{0, 1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e898bcc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In multiclass classification, there can be any finite number of classes, or **labels**. They need not be numbers, either.\n",
    "\n",
    "$$y_i \\in \\{ \\text{Adelie}, \\text{Chinstrap}, \\text{Gentoo} \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875fd8e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Important**: Let $C$ be the set of possible classes for our classification problem, and let $|C|$ be the number of classes total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d995e5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading the data üêß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bffd0d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "penguins = sns.load_dataset('penguins').dropna().reset_index(drop=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(penguins[['bill_length_mm', 'body_mass_g']], \n",
    "                                                    penguins['species'], \n",
    "                                                    random_state=26)\n",
    "display(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6680f863",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As we did two lectures ago, we'll aim to predict the `'species'` of a penguin given their `'bill_length_mm'` and `'bill_depth_mm'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a11c083",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.penguin_scatter_2d(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53d798c8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: $k$-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81fac2e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's fit a $k$-NN classifier with $k=5$ to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d4471",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "model_knn.fit(X_train, y_train)\n",
    "util.penguin_decision_boundary(model_knn, X_train, y_train, title=\"k-NN Decision Boundary when k = 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47759d41",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Notice the vastly different scales of the features! What happens if we standardize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d50ab",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "model_knn_standardized = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=5))\n",
    "model_knn_standardized.fit(X_train, y_train)\n",
    "util.penguin_decision_boundary(model_knn_standardized, X_train, y_train, title=\"k-NN Decision Boundary when k = 5 and with Standardization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5750bf06",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af84560",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's fit a decision tree classifier with a maximum depth of 3 to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f63ed",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_tree = DecisionTreeClassifier(max_depth=3)\n",
    "model_tree.fit(X_train, y_train)\n",
    "util.penguin_decision_boundary(model_tree, X_train, y_train, title=\"Decision Boundary for a Decision Tree of Depth 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8ff271",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What about logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47240d1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As we've seen, in **binary classification**, logistic regression models **the probability of belonging to class 1, given a feature vector $\\vec{x}_i$**:\n",
    "\n",
    "$$P(y_i = 1 | \\vec{x}_i) = \\sigma (w_0 + w_1 x_i^{(1)} + w_2 x_i^{(2)} + ... + w_d x_i^{(d)}) = \\sigma\\left(\\vec{w} \\cdot \\text{Aug}(\\vec{x}_i) \\right)$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d957a3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In logistic regression, $C = \\{0, 1\\}$. But, in our current penguin classification problem, $C = \\{ \\text{Adelie}, \\text{Chinstrap}, \\text{Gentoo} \\}$, so we can't use logistic regression directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b572543",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- One idea: **one-vs-rest**. Fit $|C| = 3$ separate logistic regression models ‚Äì one per class ‚Äì and predict the class that has the highest probability.\n",
    "    - Penguin is Adelie vs. penguin is not Adelie.\n",
    "    - Penguin is Chinstrap vs. penguin is not Chinstrap.\n",
    "    - Penguin is Gentoo vs. penguin is not Gentoo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc0a944",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Another idea: **one-vs-one**. Fit ${3 \\choose 2} = 3$ separate logistic regression models ‚Äì one per **pair** of classes ‚Äì and predict the class that \"wins\" the most predictions.\n",
    "    - Penguin is Adelie vs. penguin is Chinstrap.\n",
    "    - Penguin is Adelie vs. penguin is Gentoo.\n",
    "    - Penguin is Chinstrap vs. penguin is Gentoo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b7b23",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's try something slightly different than what's listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2820312",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multinomial logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0880e770",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Multinomial** logistic regression, also known as **softmax regression**, models the probability of belonging to **any class, given a feature vector $\\vec x_i$**.<br><small>Think of it as a generalization of logistic regression.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eb8ec6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$p_\\text{Adelie} = P(y_i = \\text{Adelie} | \\vec{x}_i) = \\frac{e^{\\vec{w}_\\text{Adelie} \\cdot \\text{Aug}(\\vec{x}_i)}}{e^{\\vec{w}_\\text{Adelie} \\cdot \\text{Aug}(\\vec{x}_i)} + e^{\\vec{w}_\\text{Chinstrap} \\cdot \\text{Aug}(\\vec{x}_i)} + e^{\\vec{w}_\\text{Gentoo} \\cdot \\text{Aug}(\\vec{x}_i)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2233d4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$p_\\text{Chinstrap} = P(y_i = \\text{Chinstrap} | \\vec{x}_i) = \\frac{e^{\\vec{w}_\\text{Chinstrap} \\cdot \\text{Aug}(\\vec{x}_i)}}{e^{\\vec{w}_\\text{Adelie} \\cdot \\text{Aug}(\\vec{x}_i)} + e^{\\vec{w}_\\text{Chinstrap} \\cdot \\text{Aug}(\\vec{x}_i)} + e^{\\vec{w}_\\text{Gentoo} \\cdot \\text{Aug}(\\vec{x}_i)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ecbee",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\underbrace{p_j = P(y_i = j | \\vec{x}_i) = \\frac{e^{\\vec{w}_j \\cdot \\text{Aug}(\\vec{x}_i)}}{\\sum_{k \\in C} e^{\\vec w_k \\cdot \\text{Aug}(\\vec x_i)}}}_\\text{in general}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff5dd93",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead of a single parameter vector $\\vec{w}$, there are $|C|$ parameter vectors, one per class!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b3aeb4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Multinomial logistic regression models the probability of each class directly, and then predicts the most likely class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36208a65",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: The softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad2247",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **softmax** function is a generalization of the logistic function to multiple dimensions.<br>\n",
    "Suppose $\\vec z \\in \\mathbb{R}^d$. Then, the softmax of $\\vec z$ is defined element-wise as follows:\n",
    "\n",
    "$$\\sigma(\\vec z)_i = \\frac{e^{z_i}}{\\sum_{j = 1}^d e^{z_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19321bb4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For example, suppose $\\vec{z} = \\begin{bmatrix} -5 \\\\ 2 \\\\ 4 \\end{bmatrix}$. Then:\n",
    "\n",
    "$$\\sigma(\\vec z) = \\begin{bmatrix} \\sigma(\\vec z)_1 \\\\ \\sigma(\\vec z)_2 \\\\ \\sigma(\\vec z)_3  \\end{bmatrix} = \\underbrace{\\begin{bmatrix} \\frac{e^{-5}}{e^{-5} + e^2 + e^4} \\\\ \\frac{e^{2}}{e^{-5} + e^2 + e^4} \\\\ \\frac{e^{4}}{e^{-5} + e^2 + e^4} \\end{bmatrix}}_\\text{note the constant denominator!} = \\begin{bmatrix} 0.0001 \\\\ 0.1192 \\\\ 0.8807 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1e7d8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Why is it defined this way? **It maps a vector of real numbers to a vector of probabilities!**<br><small>Note that the denominator, $\\sum_{j=1}^d e^{z_j}$, normalizes the $e^{z_i}$ terms so that the results sum to 1.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e52f9d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Multinomial logistic regression, i.e. softmax regression, trains $|C|$ linear models of the form $\\boxed{\\vec w_k \\cdot \\text{Aug}(\\vec x_i)}$ ‚Äì one per class, $k$ ‚Äì and feeds the output of each through the softmax function, so the results can be interpreted as probabilities.\n",
    "\n",
    "    $$p_j = P(y_i = j | \\vec{x}_i) = \\frac{e^{\\vec{w}_j \\cdot \\text{Aug}(\\vec{x}_i)}}{\\sum_{k \\in C} e^{\\vec w_k \\cdot \\text{Aug}(\\vec x_i)}}$$\n",
    "\n",
    "    The $|C|$ optimal parameter vectors ‚Äì $\\vec w_\\text{Adelie}^*$, $\\vec w_\\text{Chinstrap}^*$, and $\\vec w_\\text{Gentoo}^*$, in our case ‚Äì are chosen to minimize mean cross-entropy loss, just like before!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad628e84",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multinomial logistic regression in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d60330",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The `LogisticRegression` class supports multinomial logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e5dc8b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_log = LogisticRegression(multi_class='multinomial')\n",
    "model_log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b2b97f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In total, the fit model has $3 \\times 2 = 6$ coefficients and $3 \\times 1 = 3$ intercepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a1339",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_log.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab222c49",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_log.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a29ad5e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_log.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9bc627",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When calling `model_log.predict_proba`, we get back an array of three predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec408235",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_log.predict_proba(pd.DataFrame([{\n",
    "    'bill_length_mm': 45,\n",
    "    'body_mass_g': 4500\n",
    "}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae33e6d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What does this model _look_ like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41483283",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.penguin_decision_boundary(model_log, X_train, y_train, title=\"Softmax Regression Decision Boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820f4f9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural networks üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3debfa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Softmax regression is an example of a **neural network**.<br><small>Our brains are made up of **neurons** connected by \"links\", called synapses. The model diagram below loosely resembles this structure, which is why the model is called a **neural** network.</small>\n",
    "\n",
    "<center><img src=\"imgs/net.svg\" width=1200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c508f7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Each of the 9 diagonal lines connecting a value in the <b><span style=\"color:#b3e0ff\">input layer</span></b> with a value in the <b><span style=\"color:#ff7400\">out</span><span style=\"color:#c45bcc\">put</span> <span style=\"color:#077575\">layer</span></b> represents a parameter, $w^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417667bb",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_log.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133460df",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_log.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b368db9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can use the nine parameter values above to reproduce the network's calculations ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36ee6d8",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Same values as shown in model_log.predict, two slides ago!\n",
    "softmax = lambda z: np.e ** z / sum(np.e ** z)\n",
    "softmax(model_log.intercept_.reshape(-1, 1) + model_log.coef_ @ np.array([[45], [4500]]))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "None",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
