{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c74430a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lec_utils import *\n",
    "import lec19_util as util\n",
    "from IPython.display import Markdown\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, FunctionTransformer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e19c07",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Lecture 19\n",
    "\n",
    "# Regularization\n",
    "\n",
    "### EECS 398: Practical Data Science, Winter 2025\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> ‚Ä¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/wn25\">github.com/practicaldsc/wn25</a> ‚Ä¢ üì£ See latest announcements [**here on Ed**](https://edstem.org/us/courses/69737/discussion/5943734) </small>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d09a2d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda üìÜ\n",
    "\n",
    "- Recap: Model selection.\n",
    "- Ridge regression üèîÔ∏è.\n",
    "- LASSO üìø.\n",
    "\n",
    "Remember to look at the [Machine Learning section of the Resources tab](https://practicaldsc.org/resources#machine-learning) of the course website.\n",
    "\n",
    "In addition, this lecture has an associated [Guide](https://practicaldsc.org/guides/machine-learning/ridge-regression/), which we'll refer to in lecture and you'll need to read for Homework 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab7490",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Remember that you can always ask questions anonymously at the link above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496a3beb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Model selection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1319a49d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dfd346",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Last class, we used $k$-fold cross-validation to choose between the following five models that predict commute time in `'minutes'`.\n",
    "\n",
    "<center><img src=\"imgs/five-pipelines.png\" width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafc8ccd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which model has the highest **model bias**?<br><small>See the annotated slides for the answer.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e3b71",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which model has the highest **model variance**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be61adc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which model is most likely to perform best in practice? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a56d6e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge regression üèîÔ∏è\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ad4cc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b82314",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So far, to us, \"model complexity\" has essentially meant \"number of features.\"<br><small>The main hyperparameter we've tuned is polynomial degree. For instance, a polynomial of degree 5 has 5 features ‚Äì an $x$, $x^2$, $x^3$, $x^4$, and $x^5$ feature.<br>In the more recent example, we <b>manually</b> created several different pipelines, each of which used different combinations of features from the commute times dataset.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b757c83c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Once we've created several different candidate models, we've used cross-validation to choose the one that best generalizes to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8348e8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Another approach: **instead of manually choosing which features to include, put some constraint on the optimal parameters, $w_0^*, w_1^*, ..., w_d^*$**.<br><small>This would save us time from having to think of combinations of features that might be relevant.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca668b4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Intuition: **The bigger the optimal parameters $w_0^*, w_1^*, ..., w_d^*$ are, the more _overfit_ the model is to the training data.**<br><small>Why?</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b8c4c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Polynomial regression returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9d242",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Last class, we fit various polynomial regression models to Sample 1, shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e4f4a7",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_1 = util.sample_from_pop()\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_1[['x']], sample_1['y'], random_state=23)\n",
    "px.scatter(x=X_train['x'], y=y_train, title=\"Sample 1's Training Data\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291ae5cd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As we increase the degree of the polynomial, the resulting <b><span style=\"color:#ff7f0f\">fit polynomial</span></b> overfits the **training data** more and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3458695a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "interact(lambda d: util.fit_and_show_fit(X_train, y_train, d)[1], d=(1, 25));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1bea7f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inspecting the fit degree 25 polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3760e18",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's consider the degree 25 polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2200be",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model, fig = util.fit_and_show_fit(X_train, y_train, d=25)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a684c0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What does the <b><span style=\"color:#ff7f0f\">fit polynomial</span></b> actually look like, as an equation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d9e0b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# These coefficients are rounded to two decimal places.\n",
    "# The coefficient on x^25 is not 0.00, but is something very small.\n",
    "util.display_features(model.named_steps['linearregression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794ec83f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_coefficient_magnitudes(model.named_steps['linearregression'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df47467",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `sklearn` assigned **really large** coefficients to many features.<br><small>This means that if $x$ changes a little, the output is going to change **a lot**. It seems like some of the terms are trying to \"cancel\" each other out ‚Äì some have large negative coefficients, some have large positive coefficients.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36b824",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Intuition: In general, **the larger the optimal parameters $w_0^*, w_1^*, ..., w_d^*$ are, the more _overfit_ the model is to the training data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac1ca4e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb863b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Idea**: In addition to just minimizing mean squared error, what if we could **also** try and prevent large parameter values?<br><small>Maybe this would lead to less overfitting!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c9c1ff",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Regularization** is the act of adding a penalty on the norm of the parameter vector, $\\vec w$, to the objective function.\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\:\\: +  \\underbrace{\\lambda \\sum_{j = 1}^d w_j^2}_{{\\substack{\\text{regularization} \\\\ \\text{penalty}}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd865996",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Linear regression with $L_2$ regularization ‚Äì as shown above ‚Äì is called **ridge regression**.<br><small>You'll explore the reason why in Homework 9!</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b191d55",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Intuition: Instead of just minimizing mean squared error, we balance minimizing mean squared error and a penalty on the size of the fit coefficients, $w_1^*$, $w_2^*$, ..., $w_d^*$.<br><small>We don't regularize the intercept term!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0781386",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\lambda$ is a **hyperparameter**, which we choose through cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e595491",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The $\\vec{w}_\\text{ridge}^*$ that minimizes $R_\\text{ridge}(\\vec{w})$ is not necessarily the same as $\\vec{w}_\\text{OLS}^*$, which minimizes $R_\\text{sq}(\\vec{w})$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2952dec3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "### Activity\n",
    "    \n",
    "The objective function we minimize to find $\\vec{w}_\\text{ridge}^*$ in **ridge regression** is:\n",
    "    \n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$\n",
    "    \n",
    "$\\lambda$ is a **hyperparameter**, which we choose through cross-validation. Discuss the following points with those near you:\n",
    "    \n",
    "- What if we pick $\\lambda = 0$ ‚Äì what is $\\vec{w}_\\text{ridge}^*$ then?\n",
    "- What happens to $\\vec{w}_\\text{ridge}^*$ as $\\lambda \\rightarrow \\infty$?\n",
    "- Can $\\lambda$ be negative?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cc25ea",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another interpretation of ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fadb3f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As $\\lambda$ increases, the penalty on the size of $\\vec{w}_\\text{ridge}^*$ increases, meaning that each $w_j^*$ inches closer to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee42cb24",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An equivalent way of formulating the ridge regression objective function,\n",
    "\n",
    "    $$\\text{minimize} \\:\\:\\:\\: \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$\n",
    "\n",
    "    is as a **constrained** optimization problem:\n",
    "    \n",
    "    $$\\text{minimize} \\:\\:\\:\\:\\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\text{  such that   } \\sum_{j = 1}^d w_j^2 \\leq Q$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bfa924",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $Q$ and $\\lambda$ are **inversely related**: the larger $Q$ is, the less of a penalty we're putting on size of $\\vec{w}_\\text{ridge}^*$, so the smaller $\\lambda$ is.\n",
    "\n",
    "    $$\\lambda \\approx \\frac{1}{Q}$$\n",
    "\n",
    "    <br><small>The exact relationship between $Q$ and $\\lambda$ is outside of the scope of this course, as is the proof of this fact.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b530a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: Contour plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ffaaf2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, let's look at the loss surface for mean squared error **without** regularization, for some two feature regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d7405a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_ols_surface()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dab457",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can equivalently visualize this 3D surface as a **contour plot**, which can be thought of as a projection of the surface above into two dimensions, with the colors preserved.<br><small>Learn more about contour plots in [this video](https://youtu.be/WsZj5Rb6do8?si=SmJCSAAJOT2O5m7J).</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04dc8c6",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_ols_contour()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b38a4bb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing ridge regression as a constrained optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb475c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{minimize} \\:\\:\\:\\:\\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\text{  such that   } \\sum_{j = 1}^d w_j^2 \\leq Q; \\qquad \\lambda \\approx \\frac{1}{Q}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9c81e8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Intuitively:\n",
    "\n",
    "    - The **contour plot of the loss surface** for just the mean squared error component is in <span style=\"color:#440154\"><b>v</b></span><span style=\"color:#482778\"><b>i</b></span><span style=\"color:#3e4a89\"><b>r</b></span><span style=\"color:#31688e\"><b>i</b></span><span style=\"color:#26828e\"><b>d</b></span><span style=\"color:#1f9a8c\"><b>i</b></span><span style=\"color:#a3d8cf\"><b>s</b></span>.\n",
    "    - The constraint, $\\sum_{j = 1}^d w_j^2 \\leq Q$, is in <span style=\"color:red\"><b>red</b></span>. Ridge regression says, minimize mean squared error, <span style=\"color:red\"><b>while staying in the red circle</b></span>.<br><small>The larger $Q$ is, the larger the radius of the circle is.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148caf20",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_ridge_contour()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7d7c3b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The smaller $Q$ is ‚Äì so, the larger $\\lambda$ is ‚Äì the smaller the <span style=\"color:red\"><b>red circle</b></span> is!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f131421",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As $\\lambda$ increases, the constrained solution $\\vec w_\\text{ridge}^*$ shrinks closer to $\\vec 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f4e47",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding $\\vec{w}_\\text{ridge}^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4ebcb4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We know that the $\\vec{w}_\\text{OLS}^*$ that minimizes mean squared error,\n",
    "    $$R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2$$\n",
    "  is the one that satisfies the normal equations, $X^TX \\vec{w} = X^T \\vec{y}$.<br><small>Recall, linear regression that minimizes mean squared error, without any other constraints, is called **ordinary least squares (OLS)**.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca6f601",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Sometimes, $\\vec{w}^*_\\text{OLS}$ is unique, and sometimes there are infinitely many possible $\\vec{w}^*_\\text{OLS}$.<br><small>There are infinitely many possible $\\vec{w}^*_\\text{OLS}$ when the design matrix, $X$, is not full rank! All of these infinitely many solutions minimize mean squared error.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942e384b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which vector $\\vec{w}_\\text{ridge}^*$ minimizes the ridge regression objective function?\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79422314",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It turns out there is **always** a unique solution for $\\vec{w}_\\text{ridge}^*$, even if $X$ is not full rank. It is:\n",
    "    $$\\vec{w}_\\text{ridge}^* = (X^TX + n \\lambda I)^{-1} X^T \\vec{y}$$\n",
    "    <br><small>You'll prove this in Homework 9!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0474a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since there is **always** a unique solution, ridge regression is often used in the presence of multicollinearity!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094741e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing ridge regression as an unconstrained optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4dd45",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the new [**Ridge Regression guide**](https://practicaldsc.org/guides/machine-learning/ridge-regression/), we dive deeper into some of the theory of ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7891ec",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Scroll to the section titled [**Loss surfaces**](https://practicaldsc.org/guides/machine-learning/ridge-regression/#loss-surfaces) to see an interactive visualization on the effect of $\\lambda$ on the shape of the objective function's loss surface:\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$\n",
    "\n",
    "<center><img src=\"imgs/drag.png\" width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b93d34c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Taking a step back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ecbda",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\vec{w}_\\text{ridge}^*$ **doesn't** minimize mean squared error ‚Äì it minimizes a slightly different objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba09b8d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, why would we use ever use ridge regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da062e0e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge regression in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b8fb2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Fortunately, `sklearn` can perform ridge regression for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef6c963",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a436c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Just to experiment, let's set $\\lambda$ to something extremely large and look at the resulting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb522427",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The name of the lambda hyperparameter in sklearn is alpha.\n",
    "model_large_lambda = make_pipeline(PolynomialFeatures(25, include_bias=False), \n",
    "                                   Ridge(alpha=1000000000000000000000000000))\n",
    "model_large_lambda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f12eee",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the extremely regularized model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bf3c09",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do the <b><span style=\"color:purple\">resulting predictions</span></b> look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2352d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_given_model_dict(X_train, y_train, {'Extremely Regularized Polynomial of Degree 25': (model_large_lambda, 'purple')})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b650f3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e6eec",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_large_lambda.named_steps['ridge'].intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6370960c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# All 0!\n",
    "model_large_lambda.named_steps['ridge'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377df5b8",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ab058c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using `GridSearchCV` to choose $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8ee4d0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In general, we won't just arbitrarily choose a value of $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11855e03",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead, we'll perform $k$-fold cross-validation to choose the $\\lambda$ that leads to predictions that work best on unseen test data.<br><small>The value of $\\lambda$ depends on the specific dataset and model you've chosen; there's no universally \"best\" $\\lambda$.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d0d98",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'ridge__alpha': 10.0 ** np.arange(-2, 15) # Try 0.01, 0.1, 1, 10, 100, 1000, ... \n",
    "}\n",
    "model_regularized = GridSearchCV(\n",
    "    estimator=make_pipeline(PolynomialFeatures(25, include_bias=False), Ridge()),\n",
    "    param_grid=hyperparams,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "model_regularized.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52777107",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's check the optimal $\\lambda$ it found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b6621e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_regularized.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe9e59b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- While we used `GridSearchCV` here, note that `RidgeCV` also exists, which performs automatic cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf9e10",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the regularized degree 25 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915e8929",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do the <b><span style=\"color:green\">resulting predictions</span></b> look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c2a63c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_given_model_dict(X_train, y_train,\n",
    "                           {'Unregularized Polynomial of Degree 25': (model, '#ff7f0f'),\n",
    "                            'Regularized Polynomial of Degree 25': (model_regularized, 'green')})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54142ab4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems that the <b><span style=\"color:green\">regularized polynomial</span></b> is _less_ overfit to the specific noise in the training data than the <b><span style=\"color:#ff7f0f\">unregularized polynomial</span></b>!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac46ac5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The largest coefficients are all much smaller now, too.\n",
    "<br><small>The coefficient on $x^{20}$ is -0.000136.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcf7e64",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.display_features(model_regularized.best_estimator_.named_steps['ridge'], precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc9f22",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_coefficient_magnitudes(model_regularized.best_estimator_.named_steps['ridge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7740e6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that none of them are exactly 0, but many of them are close!<br><small>This will be important later.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f3d5fc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tuning multiple hyperparameters at once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6be217",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What if we don't want to fix a polynomial degree in advance, and instead want to choose **both** the degree and value of $\\lambda$ using cross-validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74f78dd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- No problem ‚Äì we can still grid search.<br><small>Note that the next cell takes much longer than the previous call to `fit` took, since it needs to try every combination of $\\lambda$ and polynomial degree.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042a3487",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'ridge__alpha': 10.0 ** np.arange(-2, 15),\n",
    "    'polynomialfeatures__degree': range(1, 26)\n",
    "}\n",
    "model_regularized_degree = GridSearchCV(\n",
    "    estimator=make_pipeline(PolynomialFeatures(include_bias=False), Ridge()),\n",
    "    param_grid=hyperparams,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "model_regularized_degree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4406bd6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, let's check the optimal $\\lambda$ **and** polynomial degree it found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da5222f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_regularized_degree.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8178a4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the regularized degree 3 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8331c5c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do the <b><span style=\"color:skyblue\">resulting predictions</span></b> look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321867aa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "polyfig = util.plot_given_model_dict(X_train, y_train,\n",
    "                                     {'Unregularized Polynomial of Degree 25': (model, '#ff7f0f'),\n",
    "                                      'Regularized Polynomial of Degree 25': (model_regularized, 'green'),\n",
    "                                      'Regularized Polynomial of Degree 3': (model_regularized_degree, 'skyblue')})\n",
    "polyfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d40fcb5",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.display_features(model_regularized_degree.best_estimator_.named_steps['ridge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb8482",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Run the cell below to set up the next slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c4ea4",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "unregularized_train = mean_squared_error(y_train, model.predict(X_train))\n",
    "unregularized_test = mean_squared_error(y_test, model.predict(X_test))\n",
    "regularized_lambda_train = mean_squared_error(y_train, model_regularized.predict(X_train))\n",
    "regularized_lambda_validation = (-model_regularized.cv_results_['mean_test_score']).min()\n",
    "regularized_lambda_test = mean_squared_error(y_test, model_regularized.predict(X_test))\n",
    "regularized_lambda_degree_train = mean_squared_error(y_train, model_regularized_degree.predict(X_train))\n",
    "regularized_lambda_degree_validation = (-model_regularized_degree.cv_results_['mean_test_score']).min()\n",
    "regularized_lambda_degree_test = mean_squared_error(y_test, model_regularized_degree.predict(X_test))\n",
    "results_df = pd.DataFrame(index=['training MSE', 'average validation MSE (across all folds)', 'test MSE']).assign(\n",
    "    unregularized=[unregularized_train, np.nan, unregularized_test],\n",
    "    regularized_lambda_only=[regularized_lambda_train, regularized_lambda_validation, regularized_lambda_test],\n",
    "    regularized_lambda_and_degree=[regularized_lambda_degree_train, regularized_lambda_degree_validation, regularized_lambda_degree_test]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad430b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reprs = {'unregularized': '<b><span style=\"color:#ff7f0f\">Unregularized (Degree 25)</span></b>',\n",
    "         'regularized_lambda_only': '<b><span style=\"color:green\">Regularized (Degree 25)<br><small>Used cross-validation to choose $\\lambda$</span></b>',\n",
    "         'regularized_lambda_and_degree': '<b><span style=\"color:skyblue\">Regularized (Degree 3)<br><small>Used cross-validation to choose $\\lambda$ and degree</small></span></b>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65093e9",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results_df_str = results_df.to_html()\n",
    "for rep in reprs:\n",
    "    results_df_str = results_df_str.replace(rep, reprs[rep])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ea92e5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing training, validation, and test errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e93fa9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's compare the training and testing error of the three polynomials below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea3f2a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "polyfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3703f4",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display(HTML(results_df_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53446a04",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems that the <b><span style=\"color:skyblue\">regularized polynomial, in which we used cross-validation to choose both the regularization penalty $\\lambda$ **and** degree</span></b>, generalizes best to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b5c448",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ba07a0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Could we have chosen a different method of penalizing each $w_j$ other than $w_j^2$?<br><small>We're about to see another option!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebebadd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Ridge regression's objective function happened to have a closed-form solution.<br>What if we want to minimize a function that **can't** be minimized by hand?<br><small>We'll talk about how in the next lecture!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b6c5d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have about ridge regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25849b0a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LASSO üìø\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9eff89",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Penalizing large parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef692d80",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The ridge regression objective function,\n",
    "    $$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$\n",
    "    minimizes mean squared error, **plus** a **squared** penalty on the size of the fit coefficients, $w_1^*, w_2^*, ..., w_d^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1dc114",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Could we have **regularized**, or penalized the coefficients, in some other way?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d739aa5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **LASSO** objective function penalizes the **absolute value** of each coefficient:\n",
    "\n",
    "    $$R_\\text{LASSO}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d |w_j| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c01be72",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- LASSO stands for \"least absolute shrinkage and selection operator.\"<br><small>We'll make sense of this name shortly.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913bfdd0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: Vector norms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4a0ebc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The $L_2$ norm, or Euclidean norm, of a vector $\\vec v \\in \\mathbb{R}^n$ is defined as:\n",
    "\n",
    "$$\\lVert \\vec v \\rVert = \\lVert \\vec v \\rVert_2 = \\sqrt{v_1^2 + v_2^2 + ... + v_n^2} = \\big(v_1^2 + v_2^2 + ... + v_n^2 \\big)^\\frac{1}{2} $$\n",
    "\n",
    "<center><small>The $L_2$ norm is the default norm, which is why the subscript 2 is often omitted.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f8fe8e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The $L_p$ norm of $\\vec v$, for $p \\geq 1$, is:\n",
    "\n",
    "$$\\lVert \\vec v \\rVert_p = \\big(|v_1|^p + |v_2|^p + ... + |v_n|^p \\big)^\\frac{1}{p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97bf4e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Ridge regression is said to use $L_2$ regularization because it penalizes the (squared) $L_2$ norm of $\\vec w$, ignoring the intercept term:\n",
    "\n",
    "    $$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ab646e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- LASSO is said to use $L_1$ regularization because it penalizes the $L_1$ norm of $\\vec w$, ignoring the intercept term:\n",
    "\n",
    "    $$R_\\text{LASSO}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d |w_j| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db79741",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LASSO in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35196ee2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Unlike with ridge regression or ordinary least squares, there is no general closed-form solution for $\\vec{w}_\\text{LASSO}^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f49482",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But, it can be estimated using numerical methods, which `sklearn` uses under-the-hood. Let's test it out.<br><small>More on numerical methods soon!</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ce77e4",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f3912",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's use LASSO to fit a degree 25 polynomial to Sample 1.<br><small>Here, we'll **fix** the degree, and cross-validate to find $\\lambda$.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81f4f52",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'lasso__alpha': 10.0 ** np.arange(-2, 15)\n",
    "}\n",
    "model_regularized_lasso = GridSearchCV(\n",
    "    estimator=make_pipeline(PolynomialFeatures(25, include_bias=False), Lasso()),\n",
    "    param_grid=hyperparams,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "model_regularized_lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e667ac40",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Our cross-validation routine ends up choosing $\\lambda = 0.1$, though on its own, this doesn't really tell us anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1109a40",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_regularized_lasso.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eb1d6f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the regularized degree 25 model, fit with LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dad503",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do the <b><span style=\"color:red\">resulting predictions</span></b> look like, relative to the fit polynomials from earlier in the lecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791e74e7",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_given_model_dict(X_train, y_train,\n",
    "                                     {'Unregularized Polynomial of Degree 25': (model, '#ff7f0f'),\n",
    "                                      'Regularized Polynomial of Degree 25': (model_regularized, 'green'),\n",
    "                                      'Regularized Polynomial of Degree 3': (model_regularized_degree, 'skyblue'),\n",
    "                                      'Regularized Polynomial of Degree 25, using LASSO': (model_regularized_lasso, 'red')})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9d1ed9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do you notice about the coefficients of the polynomial themselves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106c03c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.display_features(model_regularized_lasso.best_estimator_.named_steps['lasso'], precision=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0b81a2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Important**: Note that we fit a degree 25 polynomial, but many of the higher-order terms are missing, since their coefficients ended up being **exactly** 0!<br><small>There's are no $x^{18}, x^{19}, x^{20}, ..., x^{25}$ terms above, and also no $x$ term.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa2edf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The <b><span style=\"color:red\">resulting polynomial</span></b> ends up being of degree 17."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caa868f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When using LASSO, many coefficients are set to 0!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89f1e1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When using $L_1$ regularization ‚Äì that is, when performing LASSO ‚Äì many of the optimal coefficients $w_1^*, w_2^*, ..., w_d^*$ end up being **exactly 0**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab3e4a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This was not the case in ridge regression ‚Äì there, the optimal coefficients were all very small, but none were exactly 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba4900a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display(Markdown('#### Fit using Ridge:'))\n",
    "util.display_features(model_regularized.best_estimator_.named_steps['ridge'], precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c6c22",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_coefficient_magnitudes(model_regularized.best_estimator_.named_steps['ridge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d5534",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If a feature has a coefficient of 0, it means it's not being used at all in making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13560faf",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display(Markdown('#### Fit using LASSO (notice the larger coefficient on $x^3$):'))\n",
    "util.display_features(model_regularized_lasso.best_estimator_.named_steps['lasso'], precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec5617f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_coefficient_magnitudes(model_regularized_lasso.best_estimator_.named_steps['lasso'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63feb300",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- LASSO implicitly performs **feature selection** for us ‚Äì it automatically tells us which features we don't need to use.<br><small>Here, it told us \"don't use $x$, don't use $x^{18}$, don't use $x^{19}$, ..., don't use $x^{25}$, and instead weigh the $x^2$ and $x^3$ terms more.\"</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e782c3ec",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is where the name \"least absolute shrinkage and **selection** operator\" comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96691d8e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why does LASSO encourage sparsity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631450ed",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The fact that many of the optimal coefficients ‚Äì $w_1^*, w_2^*, ..., w_d^*$ ‚Äì are 0 when performing LASSO is often stated as:\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center><b>LASSO encourages <i>sparsity</i>.</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b244d6fe",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To make sense of this, let's look at the equivalent formulation of LASSO as a **constrained optimization problem**.\n",
    "\n",
    "    $$\\text{minimize} \\:\\:\\:\\: \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d | w_j |$$\n",
    "\n",
    "    is equivalent to:\n",
    "    \n",
    "    $$\\text{minimize} \\:\\:\\:\\:\\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\text{  such that   } \\sum_{j = 1}^d | w_j | \\leq Q$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740b4aea",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Again, $Q$ and $\\lambda$ are **inversely related**: the larger $Q$ is, the less of a penalty we're putting on size of $\\vec{w}_\\text{LASSO}^*$, so the smaller $\\lambda$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b89166",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing LASSO as a constrained optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc08c6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{minimize} \\:\\:\\:\\:\\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\text{  such that   } \\sum_{j = 1}^d |w_j| \\leq Q; \\qquad \\lambda \\approx \\frac{1}{Q}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b04d4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As before:\n",
    "\n",
    "    - The **contour plot of the loss surface** for just the mean squared error component is in <span style=\"color:#440154\"><b>v</b></span><span style=\"color:#482778\"><b>i</b></span><span style=\"color:#3e4a89\"><b>r</b></span><span style=\"color:#31688e\"><b>i</b></span><span style=\"color:#26828e\"><b>d</b></span><span style=\"color:#1f9a8c\"><b>i</b></span><span style=\"color:#a3d8cf\"><b>s</b></span>.\n",
    "    - The constraint, $\\sum_{j = 1}^d |w_j| \\leq Q$, is in <span style=\"color:red\"><b>red</b></span>. LASSO says, minimize mean squared error, <span style=\"color:red\"><b>while staying in the red diamond</b></span>.<br><small>The larger $Q$ is, the larger the side length of the diamond is.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a9c0b1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "util.show_lasso_contour()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafe197e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Notice that the <span style=\"color:red\"><b>constraint set</b></span> has clearly defined \"corners,\" which lie on the axes. The axes are where the parameter values, $w_1$ and $w_2$ here, are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c98fb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Due to the shape of the constraint set, it's likely that the minimum value of <b><span style=\"color:blue\">mean squared error</span></b>, among all options in the <b><span style=\"color:red\">red diamond</span></b>, will occur at a corner, where some of the parameter values are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24466dc1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have about LASSO, or regularization in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db325c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Commute times\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeddb6ec",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another example: Commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1cc739",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Last class, **before we learned about regularization**, we used $k$-fold cross-validation to choose between the following five models that predict commute time in `'minutes'`.\n",
    "\n",
    "<center><img src=\"imgs/five-pipelines.png\" width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197f2304",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The most complicated model, labeled `departure_hour with poly features + day OHE + month OHE + week`, didn't generalize well to unseen data, relative to more simple models.<br><small>At least, not when we used ordinary least squares to train it.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f661e8b0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's use ordinary least squares, ridge regression, **and** LASSO to train the most complicated model from above, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482382a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/commute-times.csv')\n",
    "df['day_of_month'] = pd.to_datetime(df['date']).dt.day\n",
    "df['month'] = pd.to_datetime(df['date']).dt.month_name()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46317900",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('minutes', axis=1), df['minutes'], random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d127a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ordinary least squares for commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c9a9ae",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The Pipeline below implements the feature transformations necessary to produce the \n",
    "\n",
    "    <center><code>departure_hour with poly features + day OHE + month OHE + week</code></center>\n",
    "\n",
    "    model from the last slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a0b390",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "week_converter = FunctionTransformer(lambda s: 'Week ' + ((s - 1) // 7 + 1).astype(str),\n",
    "                                     feature_names_out='one-to-one')\n",
    "day_of_month_transformer = make_pipeline(week_converter, OneHotEncoder(drop='first'))\n",
    "# Note the include_bias=False once again!\n",
    "commute_feature_pipe = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        (PolynomialFeatures(3, include_bias=False), ['departure_hour']),\n",
    "        (OneHotEncoder(drop='first', handle_unknown='ignore'), ['day', 'month']),\n",
    "        (day_of_month_transformer, ['day_of_month']),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0942989",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, we'll fit a \"vanilla\" linear regression model, i.e. one that just minimizes mean squared error, with no regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558aac89",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_model_ols = make_pipeline(commute_feature_pipe, LinearRegression())\n",
    "commute_model_ols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c8249",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There are no hyperparameters to grid search for here, so we'll just fit the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8032b9",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_model_ols.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809d2544",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We'll keep `commute_model_ols` aside for now, and compare its performance to the fit regularized models in a few moments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a6c55d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge regression for commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf2613",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Again, let's instantiate a Pipeline for the steps we want to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8684fe75",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_pipe_ridge = make_pipeline(commute_feature_pipe, Ridge())\n",
    "commute_pipe_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9fd66e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, since we need to choose the regularization penalty, $\\lambda$, we'll fit a `GridSearchCV` instance with a hyperparameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c1675",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lambdas = 10.0 ** np.arange(-10, 15)\n",
    "hyperparams = {\n",
    "    'ridge__alpha': lambdas \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64bb731",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "commute_model_ridge = GridSearchCV(\n",
    "    commute_pipe_ridge,\n",
    "    param_grid = hyperparams,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=10\n",
    ")\n",
    "commute_model_ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b26f25",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which $\\lambda$ did it choose?<br><small>On its own, this value of $\\lambda$ doesn't really tell us anything.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b4f792",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_model_ridge.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42969181",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: average validation error vs. $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1be5542",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How did the average validation MSE change with $\\lambda$?<br><small>Here, large values of $\\lambda$ mean **less complex models**, not more complex.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369fedeb",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pd.Series(-commute_model_ridge.cv_results_['mean_test_score'], \n",
    "              index=np.log10(lambdas))\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .plot(kind='line', x='index', y=0)\n",
    "    .update_layout(xaxis_title='$\\log(\\lambda)$', yaxis_title='Average Validation MSE')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5376f2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LASSO for commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fccffd2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's instantiate a third Pipeline for the steps we want to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5569076a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_pipe_lasso = make_pipeline(commute_feature_pipe, Lasso())\n",
    "commute_pipe_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac796e9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Again, we'll grid search to find the best $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22320ae1",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lambdas = 10.0 ** np.arange(-10, 15)\n",
    "hyperparams = {\n",
    "    'lasso__alpha': lambdas \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c1ad1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "commute_model_lasso = GridSearchCV(\n",
    "    commute_pipe_lasso,\n",
    "    param_grid = hyperparams,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=10\n",
    ")\n",
    "commute_model_lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b058e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which $\\lambda$ did it choose? Is it the same as when we used ridge regression?<br><small>On its own, this value of $\\lambda$ doesn't really tell us anything.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4edc9",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_model_lasso.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c31c4ec",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Run the cell below to set up the next slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5339e1",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_results = pd.concat([\n",
    "    util.display_commute_coefs(commute_model_ols),\n",
    "    util.display_commute_coefs(commute_model_ridge.best_estimator_),\n",
    "    util.display_commute_coefs(commute_model_lasso.best_estimator_)\n",
    "], axis=1)\n",
    "commute_results.columns = ['ols', 'ridge', 'lasso']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2285c4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing coefficients across models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d73ecd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do the resulting coefficients look like in all three models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a31519",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display_df(commute_results, rows=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc614e88",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The coefficients in the OLS model tend to be the largest in magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a4c2d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the ridge model, the coefficients are all generally small, but none are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aea636",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the LASSO model, many coefficients are 0 exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de1e547",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bc388c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which features did LASSO \"select\", i.e. assign a nonzero coefficient to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1c5a3",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display_df(\n",
    "    commute_results.loc[commute_results['lasso'] != 0, 'lasso'],\n",
    "    rows=22\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26b709b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How does this change if we **increase** the regularization penalty, $\\lambda$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02a4fa",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def control_alpha(lamb):\n",
    "    commute_pipe_lasso = make_pipeline(commute_feature_pipe, Lasso(alpha=lamb))\n",
    "    commute_pipe_lasso.fit(X_train, y_train)\n",
    "    coefs = commute_pipe_lasso[-1].coef_\n",
    "    names = commute_pipe_lasso[0].get_feature_names_out()\n",
    "    s = pd.Series(coefs, index=names)\n",
    "    fig = px.bar(x=s, y=s.index, title=f'Coefficients using LASSO with lambda={lamb}', height=800, width=800)\n",
    "    fig.update_layout(xaxis_title='Coefficient', yaxis_title='Feature')\n",
    "    return fig\n",
    "interact(control_alpha, lamb=(0, 3, 0.01));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7544b8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing training and test errors across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db2b57",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_dict = {'ols': commute_model_ols, 'ridge': commute_model_ridge, 'lasso': commute_model_lasso}\n",
    "df = pd.DataFrame().assign(**{\n",
    "    'Model': model_dict.keys(),\n",
    "    'Training MSE': [mean_squared_error(y_train, model_dict[model].predict(X_train)) for model in model_dict],\n",
    "    'Test MSE': [mean_squared_error(y_test, model_dict[model].predict(X_test)) for model in model_dict]\n",
    "}).set_index('Model')\n",
    "df.plot(kind='barh', barmode='group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be9dbf2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The best-fitting LASSO model seems to have a lower training and testing MSE than the best-fitting ridge model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540de0b5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But, in general, sometimes LASSO performs better on unseen data, and sometimes ridge does. Cross-validate!<br><small>Sometimes, machine learning practitioners say \"there's no free lunch\" ‚Äì there's no universal always-best technique to use to make predictions, it always depends on the specific data you have.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cba6eef",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Standardize when regularizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b425500",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As we discussed a few lectures ago, by **standardizing** our features, we bring them all to the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5be651",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Standardizing features in ordinary least squares doesn't change our model's **performance**; rather, it impacts the interpretability of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e834f2b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But, when regularizing, we're penalizing the sizes of the coefficients, which can be on wildly different scales if the features are on different scales.\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\mathbf{+} \\underbrace{\\lambda \\sum_{j = 1}^d w_j^2}_{\\substack{\\text{regularization} \\\\ \\text{penalty}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e769cf1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, **when regularizing a linear model, you should standardize the features first**, so the coefficients for all features are on the same scale, and are penalized equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e14da71",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# In other words, commute_feature_pipe should've been this!\n",
    "make_pipeline(commute_feature_pipe, StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf1638",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have about regularization in general?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "None",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
