{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28932bc4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lec_utils import *\n",
    "import lec19_util as util\n",
    "from IPython.display import Markdown\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, FunctionTransformer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419db7e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Lecture 19\n",
    "\n",
    "# Regularization\n",
    "\n",
    "### EECS 398: Practical Data Science, Winter 2025\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> ‚Ä¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/wn25\">github.com/practicaldsc/wn25</a> ‚Ä¢ üì£ See latest announcements [**here on Ed**](https://edstem.org/us/courses/69737/discussion/5943734) </small>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b340c36",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda üìÜ\n",
    "\n",
    "- Recap: Model selection.\n",
    "- Ridge regression üèîÔ∏è.\n",
    "- LASSO üìø.\n",
    "\n",
    "Remember to look at the [Machine Learning section of the Resources tab](https://practicaldsc.org/resources#machine-learning) of the course website.\n",
    "\n",
    "In addition, this lecture has an associated [Guide](https://practicaldsc.org/guides/machine-learning/ridge-regression/), which we'll refer to in lecture and you'll need to read for Homework 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c651a0a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Remember that you can always ask questions anonymously at the link above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffa9d82",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Model selection\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae528109",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c19d21",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Last class, we used $k$-fold cross-validation to choose between the following five models that predict commute time in `'minutes'`.\n",
    "\n",
    "<center><img src=\"imgs/five-pipelines.png\" width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be6ffed",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which model has the highest **model bias**?<br><small>See the annotated slides for the answer.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db7138",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which model has the highest **model variance**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faed141",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which model is most likely to perform best in practice? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246e80a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ridge regression üèîÔ∏è\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa871b68",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e645a235",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So far, to us, \"model complexity\" has essentially meant \"number of features.\"<br><small>The main hyperparameter we've tuned is polynomial degree. For instance, a polynomial of degree 5 has 5 features ‚Äì an $x$, $x^2$, $x^3$, $x^4$, and $x^5$ feature.<br>In the more recent example, we <b>manually</b> created several different pipelines, each of which used different combinations of features from the commute times dataset.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8669986",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Once we've created several different candidate models, we've used cross-validation to choose the one that best generalizes to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416a9bc6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Another approach: **instead of manually choosing which features to include, put some constraint on the optimal parameters, $w_0^*, w_1^*, ..., w_d^*$**.<br><small>This would save us time from having to think of combinations of features that might be relevant.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c844f0d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Intuition: **The bigger the optimal parameters $w_0^*, w_1^*, ..., w_d^*$ are, the more _overfit_ the model is to the training data.**<br><small>Why?</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a5c7ed",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Polynomial regression returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec14eb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Last class, we fit various polynomial regression models to Sample 1, shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e290a0",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_1 = util.sample_from_pop()\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_1[['x']], sample_1['y'], random_state=23)\n",
    "px.scatter(x=X_train['x'], y=y_train, title=\"Sample 1's Training Data\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb12c99",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As we increase the degree of the polynomial, the resulting <b><span style=\"color:#ff7f0f\">fit polynomial</span></b> overfits the **training data** more and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84cbcb3",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "interact(lambda d: util.fit_and_show_fit(X_train, y_train, d)[1], d=(1, 25));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26cb179",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inspecting the fit degree 25 polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd40e8d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's consider the degree 25 polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a7ee1",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model, fig = util.fit_and_show_fit(X_train, y_train, d=25)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4bcef1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What does the <b><span style=\"color:#ff7f0f\">fit polynomial</span></b> actually look like, as an equation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e9fbb",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# These coefficients are rounded to two decimal places.\n",
    "# The coefficient on x^25 is not 0.00, but is something very small.\n",
    "util.display_features(model.named_steps['linearregression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297353dc",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_coefficient_magnitudes(model.named_steps['linearregression'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56208e84",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `sklearn` assigned **really large** coefficients to many features.<br><small>This means that if $x$ changes a little, the output is going to change **a lot**. It seems like some of the terms are trying to \"cancel\" each other out ‚Äì some have large negative coefficients, some have large positive coefficients.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b1d03c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Intuition: In general, **the larger the optimal parameters $w_0^*, w_1^*, ..., w_d^*$ are, the more _overfit_ the model is to the training data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a42d793",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda576cf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Idea**: In addition to just minimizing mean squared error, what if we could **also** try and prevent large parameter values?<br><small>Maybe this would lead to less overfitting!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b45861",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Regularization** is the act of adding a penalty on the norm of the parameter vector, $\\vec w$, to the objective function.\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\:\\: +  \\underbrace{\\lambda \\sum_{j = 1}^d w_j^2}_{{\\substack{\\text{regularization} \\\\ \\text{penalty}}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18dca50",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Linear regression with $L_2$ regularization ‚Äì as shown above ‚Äì is called **ridge regression**.<br><small>You'll explore the reason why in Homework 9!</small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e0b19c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Intuition: Instead of just minimizing mean squared error, we balance minimizing mean squared error and a penalty on the size of the fit coefficients, $w_1^*$, $w_2^*$, ..., $w_d^*$.<br><small>We don't regularize the intercept term!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067e6ca",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\lambda$ is a **hyperparameter**, which we choose through cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567280cd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The $\\vec{w}_\\text{ridge}^*$ that minimizes $R_\\text{ridge}(\\vec{w})$ is not necessarily the same as $\\vec{w}_\\text{OLS}^*$, which minimizes $R_\\text{sq}(\\vec{w})$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fa201",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "### Activity\n",
    "    \n",
    "The objective function we minimize to find $\\vec{w}_\\text{ridge}^*$ in **ridge regression** is:\n",
    "    \n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$\n",
    "    \n",
    "$\\lambda$ is a **hyperparameter**, which we choose through cross-validation. Discuss the following points with those near you:\n",
    "    \n",
    "- What if we pick $\\lambda = 0$ ‚Äì what is $\\vec{w}_\\text{ridge}^*$ then?\n",
    "- What happens to $\\vec{w}_\\text{ridge}^*$ as $\\lambda \\rightarrow \\infty$?\n",
    "- Can $\\lambda$ be negative?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d968d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another interpretation of ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85e1a8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As $\\lambda$ increases, the penalty on the size of $\\vec{w}_\\text{ridge}^*$ increases, meaning that each $w_j^*$ inches closer to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8256a797",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An equivalent way of formulating the ridge regression objective function,\n",
    "\n",
    "    $$\\text{minimize} \\:\\:\\:\\: \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$\n",
    "\n",
    "    is as a **constrained** optimization problem:\n",
    "    \n",
    "    $$\\text{minimize} \\:\\:\\:\\:\\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\text{  such that   } \\sum_{j = 1}^d w_j^2 \\leq Q$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d8a7c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $Q$ and $\\lambda$ are **inversely related**: the larger $Q$ is, the less of a penalty we're putting on size of $\\vec{w}_\\text{ridge}^*$, so the smaller $\\lambda$ is.\n",
    "\n",
    "    $$\\lambda \\approx \\frac{1}{Q}$$\n",
    "\n",
    "    <br><small>The exact relationship between $Q$ and $\\lambda$ is outside of the scope of this course, as is the proof of this fact.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec7d7dc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: Contour plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba25b73",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, let's look at the loss surface for mean squared error **without** regularization, for some two feature regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c460d0e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_ols_surface()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf31a6f1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can equivalently visualize this 3D surface as a **contour plot**, which can be thought of as a projection of the surface above into two dimensions, with the colors preserved.<br><small>Learn more about contour plots in [this video](https://youtu.be/WsZj5Rb6do8?si=SmJCSAAJOT2O5m7J).</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686b026",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_ols_contour()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40727fd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing ridge regression as a constrained optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7691c1d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{minimize} \\:\\:\\:\\:\\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\text{  such that   } \\sum_{j = 1}^d w_j^2 \\leq Q; \\qquad \\lambda \\approx \\frac{1}{Q}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3cf2ac",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Intuitively:\n",
    "\n",
    "    - The **contour plot of the loss surface** for just the mean squared error component is in <span style=\"color:#440154\"><b>v</b></span><span style=\"color:#482778\"><b>i</b></span><span style=\"color:#3e4a89\"><b>r</b></span><span style=\"color:#31688e\"><b>i</b></span><span style=\"color:#26828e\"><b>d</b></span><span style=\"color:#1f9a8c\"><b>i</b></span><span style=\"color:#a3d8cf\"><b>s</b></span>.\n",
    "    - The constraint, $\\sum_{j = 1}^d w_j^2 \\leq Q$, is in <span style=\"color:red\"><b>red</b></span>. Ridge regression says, minimize mean squared error, <span style=\"color:red\"><b>while staying in the red circle</b></span>.<br><small>The larger $Q$ is, the larger the radius of the circle is.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c251c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.show_ridge_contour()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ba903",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The smaller $Q$ is ‚Äì so, the larger $\\lambda$ is ‚Äì the smaller the <span style=\"color:red\"><b>red circle</b></span> is!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07772ff5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As $\\lambda$ increases, the constrained solution $\\vec w_\\text{ridge}^*$ shrinks closer to $\\vec 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b7f70",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding $\\vec{w}_\\text{ridge}^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d21bb3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We know that the $\\vec{w}_\\text{OLS}^*$ that minimizes mean squared error,\n",
    "    $$R_\\text{sq}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2$$\n",
    "  is the one that satisfies the normal equations, $X^TX \\vec{w} = X^T \\vec{y}$.<br><small>Recall, linear regression that minimizes mean squared error, without any other constraints, is called **ordinary least squares (OLS)**.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a56e11",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Sometimes, $\\vec{w}^*_\\text{OLS}$ is unique, and sometimes there are infinitely many possible $\\vec{w}^*_\\text{OLS}$.<br><small>There are infinitely many possible $\\vec{w}^*_\\text{OLS}$ when the design matrix, $X$, is not full rank! All of these infinitely many solutions minimize mean squared error.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5d0795",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which vector $\\vec{w}_\\text{ridge}^*$ minimizes the ridge regression objective function?\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21e4b74",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It turns out there is **always** a unique solution for $\\vec{w}_\\text{ridge}^*$, even if $X$ is not full rank. It is:\n",
    "    $$\\vec{w}_\\text{ridge}^* = (X^TX + n \\lambda I)^{-1} X^T \\vec{y}$$\n",
    "    <br><small>You'll prove this in Homework 9!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487895ec",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since there is **always** a unique solution, ridge regression is often used in the presence of multicollinearity!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c00fabe",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing ridge regression as an unconstrained optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce3139",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the new [**Ridge Regression guide**](https://practicaldsc.org/guides/machine-learning/ridge-regression/), we dive deeper into some of the theory of ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32bf8de",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Scroll to the section titled [**Loss surfaces**](https://practicaldsc.org/guides/machine-learning/ridge-regression/#loss-surfaces) to see an interactive visualization on the effect of $\\lambda$ on the shape of the objective function's loss surface:\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$\n",
    "\n",
    "<center><img src=\"imgs/drag.png\" width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c561f3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Taking a step back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73feb0ce",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\vec{w}_\\text{ridge}^*$ **doesn't** minimize mean squared error ‚Äì it minimizes a slightly different objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4f63cb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, why would we use ever use ridge regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533bf5c2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge regression in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166cffd1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Fortunately, `sklearn` can perform ridge regression for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fee5fc",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b998862",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Just to experiment, let's set $\\lambda$ to something extremely large and look at the resulting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f3f81",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The name of the lambda hyperparameter in sklearn is alpha.\n",
    "model_large_lambda = make_pipeline(PolynomialFeatures(25, include_bias=False), \n",
    "                                   Ridge(alpha=1000000000000000000000000000))\n",
    "model_large_lambda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10751a54",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the extremely regularized model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64b6d3b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do the <b><span style=\"color:purple\">resulting predictions</span></b> look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89b1a0",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_given_model_dict(X_train, y_train, {'Extremely Regularized Polynomial of Degree 25': (model_large_lambda, 'purple')})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b3cbe3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4f9c86",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_large_lambda.named_steps['ridge'].intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2efc7",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# All 0!\n",
    "model_large_lambda.named_steps['ridge'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a20749",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1850dff8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using `GridSearchCV` to choose $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475f5052",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In general, we won't just arbitrarily choose a value of $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939631f0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Instead, we'll perform $k$-fold cross-validation to choose the $\\lambda$ that leads to predictions that work best on unseen test data.<br><small>The value of $\\lambda$ depends on the specific dataset and model you've chosen; there's no universally \"best\" $\\lambda$.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ac61d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'ridge__alpha': 10.0 ** np.arange(-2, 15) # Try 0.01, 0.1, 1, 10, 100, 1000, ... \n",
    "}\n",
    "model_regularized = GridSearchCV(\n",
    "    estimator=make_pipeline(PolynomialFeatures(25, include_bias=False), Ridge()),\n",
    "    param_grid=hyperparams,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "model_regularized.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad7534",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's check the optimal $\\lambda$ it found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c23ccc",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_regularized.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776919b9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- While we used `GridSearchCV` here, note that `RidgeCV` also exists, which performs automatic cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01934bb4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the regularized degree 25 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ea149",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do the <b><span style=\"color:green\">resulting predictions</span></b> look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1984ccc9",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_given_model_dict(X_train, y_train,\n",
    "                           {'Unregularized Polynomial of Degree 25': (model, '#ff7f0f'),\n",
    "                            'Regularized Polynomial of Degree 25': (model_regularized, 'green')})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891b0e5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems that the <b><span style=\"color:green\">regularized polynomial</span></b> is _less_ overfit to the specific noise in the training data than the <b><span style=\"color:#ff7f0f\">unregularized polynomial</span></b>!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e87c753",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The largest coefficients are all much smaller now, too.\n",
    "<br><small>The coefficient on $x^{20}$ is -0.000136.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa746b8",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.display_features(model_regularized.best_estimator_.named_steps['ridge'], precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a20684",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_coefficient_magnitudes(model_regularized.best_estimator_.named_steps['ridge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50fcc3d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that none of them are exactly 0, but many of them are close!<br><small>This will be important later.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f9c851",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tuning multiple hyperparameters at once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47ebf8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What if we don't want to fix a polynomial degree in advance, and instead want to choose **both** the degree and value of $\\lambda$ using cross-validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c8e1dc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- No problem ‚Äì we can still grid search.<br><small>Note that the next cell takes much longer than the previous call to `fit` took, since it needs to try every combination of $\\lambda$ and polynomial degree.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16428f5",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'ridge__alpha': 10.0 ** np.arange(-2, 15),\n",
    "    'polynomialfeatures__degree': range(1, 26)\n",
    "}\n",
    "model_regularized_degree = GridSearchCV(\n",
    "    estimator=make_pipeline(PolynomialFeatures(include_bias=False), Ridge()),\n",
    "    param_grid=hyperparams,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "model_regularized_degree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e6a6b8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, let's check the optimal $\\lambda$ **and** polynomial degree it found!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e40e37",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_regularized_degree.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e47e13b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the regularized degree 3 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb2bef3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do the <b><span style=\"color:skyblue\">resulting predictions</span></b> look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b234a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "polyfig = util.plot_given_model_dict(X_train, y_train,\n",
    "                                     {'Unregularized Polynomial of Degree 25': (model, '#ff7f0f'),\n",
    "                                      'Regularized Polynomial of Degree 25': (model_regularized, 'green'),\n",
    "                                      'Regularized Polynomial of Degree 3': (model_regularized_degree, 'skyblue')})\n",
    "polyfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee14827",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.display_features(model_regularized_degree.best_estimator_.named_steps['ridge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e30062",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Run the cell below to set up the next slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ab62e1",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "unregularized_train = mean_squared_error(y_train, model.predict(X_train))\n",
    "unregularized_test = mean_squared_error(y_test, model.predict(X_test))\n",
    "regularized_lambda_train = mean_squared_error(y_train, model_regularized.predict(X_train))\n",
    "regularized_lambda_validation = (-model_regularized.cv_results_['mean_test_score']).min()\n",
    "regularized_lambda_test = mean_squared_error(y_test, model_regularized.predict(X_test))\n",
    "regularized_lambda_degree_train = mean_squared_error(y_train, model_regularized_degree.predict(X_train))\n",
    "regularized_lambda_degree_validation = (-model_regularized_degree.cv_results_['mean_test_score']).min()\n",
    "regularized_lambda_degree_test = mean_squared_error(y_test, model_regularized_degree.predict(X_test))\n",
    "results_df = pd.DataFrame(index=['training MSE', 'average validation MSE (across all folds)', 'test MSE']).assign(\n",
    "    unregularized=[unregularized_train, np.nan, unregularized_test],\n",
    "    regularized_lambda_only=[regularized_lambda_train, regularized_lambda_validation, regularized_lambda_test],\n",
    "    regularized_lambda_and_degree=[regularized_lambda_degree_train, regularized_lambda_degree_validation, regularized_lambda_degree_test]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea604c",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reprs = {'unregularized': '<b><span style=\"color:#ff7f0f\">Unregularized (Degree 25)</span></b>',\n",
    "         'regularized_lambda_only': '<b><span style=\"color:green\">Regularized (Degree 25)<br><small>Used cross-validation to choose $\\lambda$</span></b>',\n",
    "         'regularized_lambda_and_degree': '<b><span style=\"color:skyblue\">Regularized (Degree 3)<br><small>Used cross-validation to choose $\\lambda$ and degree</small></span></b>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f96796",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results_df_str = results_df.to_html()\n",
    "for rep in reprs:\n",
    "    results_df_str = results_df_str.replace(rep, reprs[rep])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a263c393",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing training, validation, and test errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf4ff8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's compare the training and testing error of the three polynomials below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762117b2",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "polyfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabce522",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display(HTML(results_df_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3629284c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems that the <b><span style=\"color:skyblue\">regularized polynomial, in which we used cross-validation to choose both the regularization penalty $\\lambda$ **and** degree</span></b>, generalizes best to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc54df5b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7092b0b0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Could we have chosen a different method of penalizing each $w_j$ other than $w_j^2$?<br><small>We're about to see another option!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6154d900",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Ridge regression's objective function happened to have a closed-form solution.<br>What if we want to minimize a function that **can't** be minimized by hand?<br><small>We'll talk about how in the next lecture!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f154e406",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have about ridge regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f73133",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LASSO üìø\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2254a3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Penalizing large parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04306e9a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The ridge regression objective function,\n",
    "    $$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$\n",
    "    minimizes mean squared error, **plus** a **squared** penalty on the size of the fit coefficients, $w_1^*, w_2^*, ..., w_d^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a0e798",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Could we have **regularized**, or penalized the coefficients, in some other way?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180e6ff",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **LASSO** objective function penalizes the **absolute value** of each coefficient:\n",
    "\n",
    "    $$R_\\text{LASSO}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d |w_j| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4615586b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- LASSO stands for \"least absolute shrinkage and selection operator.\"<br><small>We'll make sense of this name shortly.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99854b28",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: Vector norms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c82d8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The $L_2$ norm, or Euclidean norm, of a vector $\\vec v \\in \\mathbb{R}^n$ is defined as:\n",
    "\n",
    "$$\\lVert \\vec v \\rVert = \\lVert \\vec v \\rVert_2 = \\sqrt{v_1^2 + v_2^2 + ... + v_n^2} = \\big(v_1^2 + v_2^2 + ... + v_n^2 \\big)^\\frac{1}{2} $$\n",
    "\n",
    "<center><small>The $L_2$ norm is the default norm, which is why the subscript 2 is often omitted.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5972f66",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The $L_p$ norm of $\\vec v$, for $p \\geq 1$, is:\n",
    "\n",
    "$$\\lVert \\vec v \\rVert_p = \\big(|v_1|^p + |v_2|^p + ... + |v_n|^p \\big)^\\frac{1}{p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0765998",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Ridge regression is said to use $L_2$ regularization because it penalizes the (squared) $L_2$ norm of $\\vec w$, ignoring the intercept term:\n",
    "\n",
    "    $$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d w_j^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7304e99d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- LASSO is said to use $L_1$ regularization because it penalizes the $L_1$ norm of $\\vec w$, ignoring the intercept term:\n",
    "\n",
    "    $$R_\\text{LASSO}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d |w_j| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc5926e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LASSO in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef06b2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Unlike with ridge regression or ordinary least squares, there is no general closed-form solution for $\\vec{w}_\\text{LASSO}^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2561ec9b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But, it can be estimated using numerical methods, which `sklearn` uses under-the-hood. Let's test it out.<br><small>More on numerical methods soon!</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8a76d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f217ee2a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's use LASSO to fit a degree 25 polynomial to Sample 1.<br><small>Here, we'll **fix** the degree, and cross-validate to find $\\lambda$.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73103f6d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'lasso__alpha': 10.0 ** np.arange(-2, 15)\n",
    "}\n",
    "model_regularized_lasso = GridSearchCV(\n",
    "    estimator=make_pipeline(PolynomialFeatures(25, include_bias=False), Lasso()),\n",
    "    param_grid=hyperparams,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "model_regularized_lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180de3bd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Our cross-validation routine ends up choosing $\\lambda = 0.1$, though on its own, this doesn't really tell us anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc9988a",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_regularized_lasso.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16570d4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing the regularized degree 25 model, fit with LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce0e3c6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do the <b><span style=\"color:red\">resulting predictions</span></b> look like, relative to the fit polynomials from earlier in the lecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9cc8b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_given_model_dict(X_train, y_train,\n",
    "                                     {'Unregularized Polynomial of Degree 25': (model, '#ff7f0f'),\n",
    "                                      'Regularized Polynomial of Degree 25': (model_regularized, 'green'),\n",
    "                                      'Regularized Polynomial of Degree 3': (model_regularized_degree, 'skyblue'),\n",
    "                                      'Regularized Polynomial of Degree 25, using LASSO': (model_regularized_lasso, 'red')})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222be3fb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do you notice about the coefficients of the polynomial themselves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f8f622",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.display_features(model_regularized_lasso.best_estimator_.named_steps['lasso'], precision=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9947b81f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Important**: Note that we fit a degree 25 polynomial, but many of the higher-order terms are missing, since their coefficients ended up being **exactly** 0!<br><small>There's are no $x^{18}, x^{19}, x^{20}, ..., x^{25}$ terms above, and also no $x$ term.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37046646",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The <b><span style=\"color:red\">resulting polynomial</span></b> ends up being of degree 17."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b31ca5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### When using LASSO, many coefficients are set to 0!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2551fd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When using $L_1$ regularization ‚Äì that is, when performing LASSO ‚Äì many of the optimal coefficients $w_1^*, w_2^*, ..., w_d^*$ end up being **exactly 0**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e802a3c2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This was not the case in ridge regression ‚Äì there, the optimal coefficients were all very small, but none were exactly 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121eea7e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display(Markdown('#### Fit using Ridge:'))\n",
    "util.display_features(model_regularized.best_estimator_.named_steps['ridge'], precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebedf31",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_coefficient_magnitudes(model_regularized.best_estimator_.named_steps['ridge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b75949",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If a feature has a coefficient of 0, it means it's not being used at all in making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f19131b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display(Markdown('#### Fit using LASSO (notice the larger coefficient on $x^3$):'))\n",
    "util.display_features(model_regularized_lasso.best_estimator_.named_steps['lasso'], precision=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e2b3b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "util.plot_coefficient_magnitudes(model_regularized_lasso.best_estimator_.named_steps['lasso'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b478d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- LASSO implicitly performs **feature selection** for us ‚Äì it automatically tells us which features we don't need to use.<br><small>Here, it told us \"don't use $x$, don't use $x^{18}$, don't use $x^{19}$, ..., don't use $x^{25}$, and instead weigh the $x^2$ and $x^3$ terms more.\"</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efbcd9c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is where the name \"least absolute shrinkage and **selection** operator\" comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea72fac",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why does LASSO encourage sparsity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20935aa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The fact that many of the optimal coefficients ‚Äì $w_1^*, w_2^*, ..., w_d^*$ ‚Äì are 0 when performing LASSO is often stated as:\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center><b>LASSO encourages <i>sparsity</i>.</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b63351a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To make sense of this, let's look at the equivalent formulation of LASSO as a **constrained optimization problem**.\n",
    "\n",
    "    $$\\text{minimize} \\:\\:\\:\\: \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 + \\lambda \\sum_{j = 1}^d | w_j |$$\n",
    "\n",
    "    is equivalent to:\n",
    "    \n",
    "    $$\\text{minimize} \\:\\:\\:\\:\\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\text{  such that   } \\sum_{j = 1}^d | w_j | \\leq Q$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993e8478",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Again, $Q$ and $\\lambda$ are **inversely related**: the larger $Q$ is, the less of a penalty we're putting on size of $\\vec{w}_\\text{LASSO}^*$, so the smaller $\\lambda$ is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9130f058",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing LASSO as a constrained optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0793e7fe",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{minimize} \\:\\:\\:\\:\\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\text{  such that   } \\sum_{j = 1}^d |w_j| \\leq Q; \\qquad \\lambda \\approx \\frac{1}{Q}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73d4a36",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As before:\n",
    "\n",
    "    - The **contour plot of the loss surface** for just the mean squared error component is in <span style=\"color:#440154\"><b>v</b></span><span style=\"color:#482778\"><b>i</b></span><span style=\"color:#3e4a89\"><b>r</b></span><span style=\"color:#31688e\"><b>i</b></span><span style=\"color:#26828e\"><b>d</b></span><span style=\"color:#1f9a8c\"><b>i</b></span><span style=\"color:#a3d8cf\"><b>s</b></span>.\n",
    "    - The constraint, $\\sum_{j = 1}^d |w_j| \\leq Q$, is in <span style=\"color:red\"><b>red</b></span>. LASSO says, minimize mean squared error, <span style=\"color:red\"><b>while staying in the red diamond</b></span>.<br><small>The larger $Q$ is, the larger the side length of the diamond is.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889fbdc7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "util.show_lasso_contour()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5accf00",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Notice that the <span style=\"color:red\"><b>constraint set</b></span> has clearly defined \"corners,\" which lie on the axes. The axes are where the parameter values, $w_1$ and $w_2$ here, are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d75aa1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Due to the shape of the constraint set, it's likely that the minimum value of <b><span style=\"color:blue\">mean squared error</span></b>, among all options in the <b><span style=\"color:red\">red diamond</span></b>, will occur at a corner, where some of the parameter values are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdbdbe8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have about LASSO, or regularization in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0373e67",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Commute times\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8935dd5d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another example: Commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f282fd08",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Last class, **before we learned about regularization**, we used $k$-fold cross-validation to choose between the following five models that predict commute time in `'minutes'`.\n",
    "\n",
    "<center><img src=\"imgs/five-pipelines.png\" width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb022f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The most complicated model, labeled `departure_hour with poly features + day OHE + month OHE + week`, didn't generalize well to unseen data, relative to more simple models.<br><small>At least, not when we used ordinary least squares to train it.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22462a81",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's use ordinary least squares, ridge regression, **and** LASSO to train the most complicated model from above, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f06d8b2",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/commute-times.csv')\n",
    "df['day_of_month'] = pd.to_datetime(df['date']).dt.day\n",
    "df['month'] = pd.to_datetime(df['date']).dt.month_name()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24fc9e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('minutes', axis=1), df['minutes'], random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b027c5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ordinary least squares for commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec1ec43",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The Pipeline below implements the feature transformations necessary to produce the \n",
    "\n",
    "    <center><code>departure_hour with poly features + day OHE + month OHE + week</code></center>\n",
    "\n",
    "    model from the last slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259523b7",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "week_converter = FunctionTransformer(lambda s: 'Week ' + ((s - 1) // 7 + 1).astype(str),\n",
    "                                     feature_names_out='one-to-one')\n",
    "day_of_month_transformer = make_pipeline(week_converter, OneHotEncoder(drop='first'))\n",
    "# Note the include_bias=False once again!\n",
    "commute_feature_pipe = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        (PolynomialFeatures(3, include_bias=False), ['departure_hour']),\n",
    "        (OneHotEncoder(drop='first', handle_unknown='ignore'), ['day', 'month']),\n",
    "        (day_of_month_transformer, ['day_of_month']),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a15e48a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, we'll fit a \"vanilla\" linear regression model, i.e. one that just minimizes mean squared error, with no regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df5531",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_model_ols = make_pipeline(commute_feature_pipe, LinearRegression())\n",
    "commute_model_ols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2469084d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There are no hyperparameters to grid search for here, so we'll just fit the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6e39f",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_model_ols.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a319c75",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We'll keep `commute_model_ols` aside for now, and compare its performance to the fit regularized models in a few moments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2e9dcf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ridge regression for commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3fd53f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Again, let's instantiate a Pipeline for the steps we want to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca88da",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_pipe_ridge = make_pipeline(commute_feature_pipe, Ridge())\n",
    "commute_pipe_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32da8763",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, since we need to choose the regularization penalty, $\\lambda$, we'll fit a `GridSearchCV` instance with a hyperparameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e272f6aa",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lambdas = 10.0 ** np.arange(-10, 15)\n",
    "hyperparams = {\n",
    "    'ridge__alpha': lambdas \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a86cc0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "commute_model_ridge = GridSearchCV(\n",
    "    commute_pipe_ridge,\n",
    "    param_grid = hyperparams,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=10\n",
    ")\n",
    "commute_model_ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30908d98",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which $\\lambda$ did it choose?<br><small>On its own, this value of $\\lambda$ doesn't really tell us anything.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae91bb",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_model_ridge.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d926ac5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: average validation error vs. $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf7717",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How did the average validation MSE change with $\\lambda$?<br><small>Here, large values of $\\lambda$ mean **less complex models**, not more complex.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731db1bf",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pd.Series(-commute_model_ridge.cv_results_['mean_test_score'], \n",
    "              index=np.log10(lambdas))\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .plot(kind='line', x='index', y=0)\n",
    "    .update_layout(xaxis_title='$\\log(\\lambda)$', yaxis_title='Average Validation MSE')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54948d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LASSO for commute times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65d7e4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's instantiate a third Pipeline for the steps we want to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaeacbc",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_pipe_lasso = make_pipeline(commute_feature_pipe, Lasso())\n",
    "commute_pipe_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0aa5b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Again, we'll grid search to find the best $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c8fba",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lambdas = 10.0 ** np.arange(-10, 15)\n",
    "hyperparams = {\n",
    "    'lasso__alpha': lambdas \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f55280",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "commute_model_lasso = GridSearchCV(\n",
    "    commute_pipe_lasso,\n",
    "    param_grid = hyperparams,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=10\n",
    ")\n",
    "commute_model_lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211fbaa6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which $\\lambda$ did it choose? Is it the same as when we used ridge regression?<br><small>On its own, this value of $\\lambda$ doesn't really tell us anything.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2238521",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_model_lasso.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0693075",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Run the cell below to set up the next slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a63dfb1",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "commute_results = pd.concat([\n",
    "    util.display_commute_coefs(commute_model_ols),\n",
    "    util.display_commute_coefs(commute_model_ridge.best_estimator_),\n",
    "    util.display_commute_coefs(commute_model_lasso.best_estimator_)\n",
    "], axis=1)\n",
    "commute_results.columns = ['ols', 'ridge', 'lasso']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeade70",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing coefficients across models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c038200b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What do the resulting coefficients look like in all three models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c164be34",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display_df(commute_results, rows=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6ff2ca",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The coefficients in the OLS model tend to be the largest in magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de45d9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the ridge model, the coefficients are all generally small, but none are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb044f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the LASSO model, many coefficients are 0 exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d36f96",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1a0240",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which features did LASSO \"select\", i.e. assign a nonzero coefficient to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74a379",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "display_df(\n",
    "    commute_results.loc[commute_results['lasso'] != 0, 'lasso'],\n",
    "    rows=22\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112a25d2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How does this change if we **increase** the regularization penalty, $\\lambda$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87278a13",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def control_alpha(lamb):\n",
    "    commute_pipe_lasso = make_pipeline(commute_feature_pipe, Lasso(alpha=lamb))\n",
    "    commute_pipe_lasso.fit(X_train, y_train)\n",
    "    coefs = commute_pipe_lasso[-1].coef_\n",
    "    names = commute_pipe_lasso[0].get_feature_names_out()\n",
    "    s = pd.Series(coefs, index=names)\n",
    "    fig = px.bar(x=s, y=s.index, title=f'Coefficients using LASSO with lambda={lamb}', height=800, width=800)\n",
    "    fig.update_layout(xaxis_title='Coefficient', yaxis_title='Feature')\n",
    "    return fig\n",
    "interact(control_alpha, lamb=(0, 3, 0.01));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a2e57e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing training and test errors across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac31df",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_dict = {'ols': commute_model_ols, 'ridge': commute_model_ridge, 'lasso': commute_model_lasso}\n",
    "df = pd.DataFrame().assign(**{\n",
    "    'Model': model_dict.keys(),\n",
    "    'Training MSE': [mean_squared_error(y_train, model_dict[model].predict(X_train)) for model in model_dict],\n",
    "    'Test MSE': [mean_squared_error(y_test, model_dict[model].predict(X_test)) for model in model_dict]\n",
    "}).set_index('Model')\n",
    "df.plot(kind='barh', barmode='group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd6cc5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The best-fitting LASSO model seems to have a lower training and testing MSE than the best-fitting ridge model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9bc6fc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But, in general, sometimes LASSO performs better on unseen data, and sometimes ridge does. Cross-validate!<br><small>Sometimes, machine learning practitioners say \"there's no free lunch\" ‚Äì there's no universal always-best technique to use to make predictions, it always depends on the specific data you have.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0f638d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Standardize when regularizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45331ae4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As we discussed a few lectures ago, by **standardizing** our features, we bring them all to the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e8742",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Standardizing features in ordinary least squares doesn't change our model's **performance**; rather, it impacts the interpretability of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f4ae3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But, when regularizing, we're penalizing the sizes of the coefficients, which can be on wildly different scales if the features are on different scales.\n",
    "\n",
    "$$R_\\text{ridge}(\\vec{w}) = \\frac{1}{n} \\lVert \\vec{y} - X \\vec{w} \\rVert^2 \\mathbf{+} \\underbrace{\\lambda \\sum_{j = 1}^d w_j^2}_{\\substack{\\text{regularization} \\\\ \\text{penalty}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce5a84",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So, **when regularizing a linear model, you should standardize the features first**, so the coefficients for all features are on the same scale, and are penalized equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771014b0",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# In other words, commute_feature_pipe should've been this!\n",
    "make_pipeline(commute_feature_pipe, StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4077bc2f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ü§î (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "What questions do you have about regularization in general?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "None",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
