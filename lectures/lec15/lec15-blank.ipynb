{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16ebc2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from lec_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2eb5c0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\" markdown=\"1\">\n",
    "\n",
    "#### Lecture 15\n",
    "\n",
    "# Multiple Linear Regression\n",
    "\n",
    "### EECS 398: Practical Data Science, Winter 2025\n",
    "\n",
    "<small><a style=\"text-decoration: none\" href=\"https://practicaldsc.org\">practicaldsc.org</a> â€¢ <a style=\"text-decoration: none\" href=\"https://github.com/practicaldsc/wn25\">github.com/practicaldsc/wn25</a> â€¢ ðŸ“£ See latest announcements [**here on Ed**](https://edstem.org/us/courses/69737/discussion/5943734) </small>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a5c74",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- Regression and linear algebra.\n",
    "- Multiple linear regression.\n",
    "- Regression in `sklearn`.\n",
    "\n",
    "Today's lecture is back to being in a notebook, but is still quite math heavy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d981ea",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ðŸ¤” (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Remember that you can always ask questions anonymously at the link above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508a7b4c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression and linear algebra\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be85b6",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The big picture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7f6a6e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we want to fit a simple linear regression model, $H(x_i) = w_0 + w_1 x_i$.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"imgs/model.png\" width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a75810",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Goal**: Find the values of $w_0^*$ and $w_1^*$ that minimize mean squared error:\n",
    "\n",
    "$$R_\\text{sq}(w_0, w_1) = \\frac{1}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1 x_i))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab0b8c1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We've already done this using calculus. By formulating $R_\\text{sq}$ in terms of matrices and vectors, we'll be able to extend our reasoning to models that use **multiple** input variables (the model above just uses one)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92359cee",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Terminology recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c7db1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Define the **design matrix** $X \\in \\mathbb{R}^{n \\times 2}$, **observation vector** $\\vec y \\in \\mathbb{R}^n$, and **parameter vector** $\\vec w \\in \\mathbb{R}^2$ as follows:\n",
    "\n",
    "$$X = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} \\qquad \\vec y = \\underbrace{\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}}_\\text{vector of actual $y$ values} \\qquad \\vec{w} = \\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ab56fb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Define the **hypothesis vector**, $\\vec{h}$, as follows:\n",
    "\n",
    "$$\\vec h = \\underbrace{\\begin{bmatrix} H(x_1) \\\\ H(x_2) \\\\ \\vdots \\\\ H(x_n) \\end{bmatrix}}_\\text{vector of predicted $y$ values} = \\begin{bmatrix} w_0 + w_1 x_1 \\\\ w_0 + w_1 x_2 \\\\ \\vdots \\\\ w_0 + w_1 x_n \\end{bmatrix} = \\underbrace{X \\vec w}_\\text{matrix-vector multiplication!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17389b87",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Rewriting mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e110047",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The mean squared error of the predictions in $\\vec h$ is:\n",
    "\n",
    "$$\\begin{align*} R_\\text{sq}(w_0, w_1) &= \\frac{1}{n} \\sum_{i = 1}^n (y_i - (w_0 + w_1 x_i))^2 \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9acd03",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Equivalently, we have:\n",
    "\n",
    "$$R_\\text{sq}(\\vec w) = \\frac{1}{n} \\lVert \\vec y - \\underbrace{X \\vec w}_{\\vec h} \\rVert^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf88c82",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Which vector, $\\vec w^*$, minimizes $R_\\text{sq}(\\vec w)$?<br>\n",
    "Equivalently, which vector $\\vec w^*$ makes $\\vec h = X \\vec w^*$ as **close to** $\\vec y$ as possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7c63b4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Geometric intuition\n",
    "\n",
    "<center><img src=\"imgs/recap.png\" width=1000></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c1166e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let $\\vec e = \\vec y - X \\vec w$ be the <span style=\"color:red\"><b>error vector</b></span> corresponding to the hypothesis vector $\\vec h = X \\vec w$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129b5ae1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To make $X \\vec w$ as close to $\\vec y$ as possible, choose $\\vec w^*$ such that:\n",
    "\n",
    "$$\\boxed{\\text{the error vector}, \\: \\vec e = \\vec y - X \\vec w^*, \\: \\text{is orthogonal to the columns of} \\: X!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ef29e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In other words, we choose $\\vec w^*$ such that $X \\vec w^*$ is the **orthogonal projection** of $\\vec y$ onto the span of the columns of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03624395",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimizing mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260aff53",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To minimize mean squared error, $R_\\text{sq}(\\vec w) = \\frac{1}{n} \\lVert \\vec y - X \\vec w \\rVert^2$, we must choose the $\\vec w^*$ such that the error vector, $\\vec e = \\vec y - X \\vec w^*$, is orthogonal to the columns of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79ffd7b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Equivalently, we have:\n",
    "\n",
    "    $$X^T \\vec e = 0$$\n",
    "\n",
    "    <small>What are the dimensions of $X^T \\vec e$?</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9906da2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Expanding, we have:\n",
    "\n",
    "$$\\begin{align*} X^T (\\vec y - X \\vec w^*) &= 0 \\\\ X^T \\vec y - X^TX \\vec w^* &= 0 \\\\ X^T X \\vec w^* &= X^T \\vec y \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3d65fe",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- **Key idea**: The $\\vec{w}^*$ that satisfies the last line above is the $\\vec w^*$ that **minimizes** $R_\\text{sq}(\\vec{w})$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b752bb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The normal equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d8d4dc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- From the last slide, we have that the $\\vec w^*$ that minimizes $R_\\text{sq}(\\vec w)$ is the $\\vec w^*$ that satisfies the **normal equations**:\n",
    "\n",
    "$$\\boxed{X^TX \\vec w^* = X^T \\vec y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db0866",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The normal equation**s** are a system of two equations in two unknowns ($w_0$ and $w_1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7467aaaa",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **If** $X^TX$ is invertible, there is a unique solution to the normal equations:\n",
    "\n",
    "    $$\\vec{w}^* = ({X^TX})^{-1} X^T \\vec y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837037e8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If $X^TX$ is not invertible, then there are infinitely many solutions to the normal equations. We will explore this idea as the semester progresses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e0cc0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The optimal parameter vector, $\\vec{w}^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85407ade",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To find the optimal model parameters for simple linear regression, $w_0^*$ and $w_1^*$, we previously minimized \n",
    "\n",
    "$$R_\\text{sq}(w_0, w_1) = \\frac{1}{n} \\sum_{i = 1}^n ({ y_i} - (w_0 + w_1 { x_i}))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1062bd",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We found, using calculus, that:\n",
    "    - $\\boxed{w_1^* = \\frac{\\sum_{i = 1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i = 1}^n (x_i - \\bar{x})^2} = r \\frac{\\sigma_y}{\\sigma_x}}$.\n",
    "    - $\\boxed{w_0^* = \\bar{y} - w_1^* \\bar{x}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f6a60",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Another way of finding optimal model parameters for simple linear regression is to find the $\\vec{w}^*$ that minimizes $R_\\text{sq}(\\vec{w}) = \\frac{1}{n}  \\lVert \\vec y - X \\vec w \\rVert^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4db24b4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The minimizer, if $X^TX$ is invertible, is the vector $\\boxed{\\vec w^* = (X^TX)^{-1}X^T \\vec y}$.<br><small>If not, solve the **normal equations** from the previous slide.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3156e07",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- These $\\boxed{\\text{formulas}}$ are equivalent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f65474",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple linear regression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9580b674",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"imgs/multiple-head.png\" width=40%>\n",
    "\n",
    "So far, we've fit **simple** linear regression models, which use only **one** feature (`'departure_hour'`) for making predictions.\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04786ef0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Incorporating multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b9104c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the context of the commute times dataset, the simple linear regression model we fit was of the form:\n",
    "\n",
    "$$\\begin{align*}\\text{pred. commute} &= H(\\text{departure hour}_i) \\\\ &= w_0 + w_1 \\cdot \\text{departure hour}_i \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add2300",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now, we'll try and fit a multiple linear regression model of the form:\n",
    "\n",
    "$$\\begin{align*}\\text{pred. commute} &= H(\\text{departure hour}_i, \\text{day of month}_i) \\\\ &= w_0 + w_1 \\cdot \\text{departure hour}_i + w_2 \\cdot \\text{day of month}_i \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b7acf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Linear regression with **multiple** features is called **multiple linear regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e61c1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How do we find $w_0^*$, $w_1^*$, and $w_2^*$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ad128",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Geometric interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a818ea55",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The hypothesis function:\n",
    "\n",
    "    $$H(\\text{departure hour}_i) = w_0 + w_1 \\cdot \\text{departure hour}_i$$\n",
    "\n",
    "    looks like a **line** in 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a721ce17",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Questions**: \n",
    "    - How many dimensions do we need to graph the hypothesis function:\n",
    "\n",
    "    $$H(\\text{departure hour}_i, \\text{day of month}_i) = w_0 + w_1 \\cdot \\text{departure hour}_i + w_2 \\cdot \\text{day of month}_i$$\n",
    "\n",
    "    - What is the shape of the hypothesis function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56439bb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "\n",
    "<img src=\"imgs/plane.png\" width=40%>\n",
    "\n",
    "Our new hypothesis function is a **plane** in 3D!<br>Our goal is to find the **plane** of best fit that pierces through the cloud of points.\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7424f26b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The hypothesis vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c817f43a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When our hypothesis function is of the form:\n",
    "\n",
    "    $$H(\\text{departure hour}) = w_0 + w_1 \\cdot \\text{departure hour} + w_2 \\cdot \\text{day of month}$$\n",
    "\n",
    "    the hypothesis vector $\\vec h \\in \\mathbb{R}^n$ can be written as:\n",
    "\n",
    "$$\\vec h = \\begin{bmatrix}\n",
    "    H(\\text{departure hour}_1, \\text{day}_1) \\\\\n",
    "    H(\\text{departure hour}_2, \\text{day}_2) \\\\\n",
    "        ... \\\\\n",
    "     H(\\text{departure hour}_n, \\text{day}_n) \\\\\n",
    "    \\end{bmatrix}\n",
    "    = \\underbrace{\\begin{bmatrix}\n",
    "        1      & \\text{departure hour}_1 & \\text{day}_1   \\\\\n",
    "        1      & \\text{departure hour}_2 & \\text{day}_2    \\\\\n",
    "        ... & ... & ... \\\\\n",
    "        1      & \\text{departure hour}_n & \\text{day}_n\n",
    "    \\end{bmatrix}}_{\\text{3 columns!}} \\begin{bmatrix}\n",
    "        w_0 \\\\\n",
    "        w_1 \\\\\n",
    "        w_2\n",
    "    \\end{bmatrix}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603496bf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding the optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c736f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To find the optimal parameter vector, $\\vec{w}^*$, we can use the **design matrix** $ X \\in \\mathbb{R}^{n \\times 3}$ and **observation vector** $ \\vec y \\in \\mathbb{R}^n$:\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "        1      & \\text{departure hour}_1 & \\text{day}_1   \\\\\n",
    "        1      & \\text{departure hour}_2 & \\text{day}_2    \\\\\n",
    "        ... & ... & ... \\\\\n",
    "        1      & \\text{departure hour}_n & \\text{day}_n\n",
    "    \\end{bmatrix} \\qquad \\vec{y} = \\begin{bmatrix} \\text{commute time}_1 \\\\ \\text{commute time}_2 \\\\ \\vdots \\\\ \\text{commute time}_n \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81539a73",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Then, all we need to do is solve the normal equations once again:\n",
    "\n",
    "    $$X^TX \\vec{w}^* = X^T \\vec y$$\n",
    "\n",
    "    If $X^TX$ is invertible, we know the solution is:\n",
    "\n",
    "$$\\vec w^* = (X^TX)^{-1} X^T \\vec y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6547794e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's generalize this notion beyond just two features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33649daf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Notation for multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db94a46",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We will need to keep track of multiple features for every individual in our dataset.<br><small>In practice, we could have hundreds, millions, or billions of features!</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ec757c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As before, subscripts distinguish between individuals in our dataset. We have $n$ individuals, also called **training examples**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13fd0eb",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Superscripts distinguish between **features**. We have $d$ features. \n",
    "\n",
    "    $$\\text{departure hour}_i: \\:\\: x_i^{(1)}$$\n",
    "\n",
    "    $$\\text{day of month}_i: \\:\\: x_i^{(2)}$$\n",
    "\n",
    "    Think of $x^{(1)}$, $x^{(2)}$, ... as new variable names, like new letters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3ec49",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd360d3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we have the following dataset.\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"imgs/example-values.png\" width=35%>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2916e",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can represent each day with a **feature vector**, $\\vec x_i = \\begin{bmatrix} x_i^{(1)} \\\\ x_i^{(2)} \\end{bmatrix}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b285ad27",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\vec x_1 = \\begin{bmatrix} 8.45 \\\\ 22 \\end{bmatrix} \\qquad \\vec x_2 = \\begin{bmatrix} 8.90 \\\\ 28 \\end{bmatrix} \\qquad \\vec x_2 = \\begin{bmatrix} 8.72 \\\\ 18 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6fc51d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Augmented feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7f9cba",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **augmented feature vector** $\\text{Aug}({\\vec x_i})$ is the vector obtained by adding a 1 to the front of feature vector $\\vec x_i$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2799d6f8",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "$$\n",
    "{ \\vec x = \\begin{bmatrix}\n",
    "    {x^{(1)}} \\\\ {x^{(2)}} \\\\ \\vdots \\\\ {x^{(d)}}\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "\\text{Aug}({ \\vec x}) = \\begin{bmatrix}\n",
    "    1 \\\\ {x^{(1)}} \\\\ {x^{(2)}} \\\\ \\vdots \\\\ { x^{(d)}}\n",
    "\\end{bmatrix}}\n",
    "\\qquad\n",
    "\\vec w = \\begin{bmatrix}\n",
    "    w_0 \\\\ w_1 \\\\ w_2\\\\ \\vdots \\\\ w_d \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4babed14",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For example, if $\\vec x_1 = \\begin{bmatrix} 8.45 \\\\ 22 \\end{bmatrix}$, then $\\text{Aug}({\\vec x_1}) = {\\begin{bmatrix} 1 \\\\ 8.45 \\\\ 22 \\end{bmatrix}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28055a5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Then, our hypothesis function for individual $i$ is:\n",
    "\n",
    "$$H({\\vec x_i}) = w_0 + w_1 { x_i^{(1)}} + w_2 { x_i^{(2)}} + \\ldots + w_d { x_i^{(d)}} = \\vec w \\cdot \\text{Aug}({ \\vec x_i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860e97bc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The general problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d705e4",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  We have $n$ data points, $\\left({ \\vec x_1}, {y_1}\\right), \\left({ \\vec x_2}, {y_2}\\right),  \\ldots, \\left({ \\vec x_n}, {y_n}\\right)$,\n",
    "where each $ \\vec x_i$ is a feature vector of $d$ features:\n",
    "$${\\vec{x_i}} = \\begin{bmatrix} \n",
    "{x^{(1)}_i} \\\\ {x^{(2)}_i} \\\\ \\vdots \\\\ {x^{(d)}_i}\n",
    "\\end{bmatrix}$$\t   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e2fa1a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  We want to find a good linear hypothesis function:\n",
    "\n",
    "$$H({\\vec x_i}) = w_0 + w_1 { x_i^{(1)}} + w_2 { x_i^{(2)}} + \\ldots + w_d { x_i^{(d)}} = \\vec w \\cdot \\text{Aug}({ \\vec x_i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7e4830",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Specifically, we want to find the optimal parameters, $w_0^*$, $w_1^*$, ..., $w_d^*$ that minimize mean squared error:\n",
    "\n",
    "$$\\begin{align*} R_\\text{sq}(\\vec w) &= \\frac{1}{n} \\sum_{i = 1}^n (y_i - H(\\vec x_i))^2 \\\\ &=\n",
    "\\frac{1}{n} \\sum_{i = 1}^n \\left( y_i - (w_0 + w_1 { x_i^{(1)}} + w_2 { x_i^{(2)}} + \\ldots + w_d { x_i^{(d)}})\\right)^2 \n",
    "\\\\ &= \\frac{1}{n} \\sum_{i = 1}^n \\left(y_i - \\text{Aug}(\\vec x_i) \\cdot \\vec{w} \\right)^2 \\\\ &= \\frac{1}{n} \\lVert \\vec y - X \\vec w \\rVert^2 \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092d9ed9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The general solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1981f1ab",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Define the **design matrix** $ X \\in \\mathbb{R}^{n \\times (d + 1)}$ and **observation vector** $\\vec y \\in \\mathbb{R}^n$:\n",
    "\n",
    "$${ X=  \\begin{bmatrix}  \n",
    "{1} & { x^{(1)}_1} & { x^{(2)}_1} & \\dots & { x^{(d)}_1} \\\\\\\\\n",
    "{ 1} & { x^{(1)}_2} & { x^{(2)}_2} & \\dots & { x^{(d)}_2} \\\\\\\\\n",
    "\\vdots & \\vdots & \\vdots  &  & \\vdots \\\\\\\\\n",
    "{ 1} & { x^{(1)}_n} & { x^{(2)}_n} & \\dots & { x^{(d)}_n}\n",
    "\\end{bmatrix} = \\begin{bmatrix} \n",
    "       \\text{Aug}({\\vec{x_1}})^T \\\\\\\\\n",
    "       \\text{Aug}({\\vec{x_2}})^T \\\\\\\\\n",
    "       \\vdots \\\\\\\\\n",
    "       \\text{Aug}({\\vec{x_n}})^T\n",
    "   \\end{bmatrix}} \\qquad { \\vec y = \\begin{bmatrix} { y_1} \\\\ { y_2} \\\\ \\vdots \\\\ { y_n} \\end{bmatrix}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92547291",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Then, solve the **normal equations** to find the optimal parameter vector, $\\vec{w}^*$:\n",
    "\n",
    "$$X^TX \\vec w^* = X^T \\vec y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d213a629",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The $\\vec w^*$ that satisfies the equations above minimizes mean squared error, $R_\\text{sq}(\\vec w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780fa09f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Note on parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc92ca17",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  With $d$ features, $\\vec w$ has $d+1$ entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba124540",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Again, $\\vec w^*$ represents our **optimal** parameter vector, that minimizes mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808191a8",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  $w_0^*$ is the **bias**, also known as the **intercept**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06cb86c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  $w_1^*, w_2^*, ... , w_d^*$ each give the **weight**, or **coefficient**, or **slope**, of a feature.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$H({\\vec x_i}) = w_0 + w_1 { x_i^{(1)}} + w_2 { x_i^{(2)}} + \\ldots + w_d { x_i^{(d)}} = \\vec w \\cdot \\text{Aug}({ \\vec x_i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438d0913",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Question ðŸ¤” (Answer at <a style=\"text-decoration: none; color: #0066cc\" href=\"https://docs.google.com/forms/d/e/1FAIpQLSd4oliiZYeNh76jWy-arfEtoAkCrVSsobZxPwxifWggo3EO0Q/viewform\">practicaldsc.org/q</a>)</h3>\n",
    "    \n",
    "Remember that you can always ask questions anonymously at the link above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9fb068",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression in `sklearn`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ca5af",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3119ac1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Run the cell below to load in our commute times dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f452ad6",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/commute-times.csv')\n",
    "df['day_of_month'] = pd.to_datetime(df['date']).dt.day\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607ed75",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For now, the only relevant columns for us are `'departure_hour'`, `'day_of_month'`, and `'minutes'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e00b277",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df[['departure_hour', 'day_of_month', 'minutes']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adf7a60",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aef719",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `sklearn` (scikit-learn) implements many common steps in the feature and model creation pipeline.<br><small>It is **widely** used throughout [industry](https://scikit-learn.org/stable/testimonials/testimonials.html#:~:text=It%20is%20very%20widely%20used,very%20approachable%20and%20very%20powerful.) and academia.</small>\n",
    "\n",
    "<center><img src='imgs/sklearn.png' width=20%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96adef50",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It interfaces with `numpy` arrays, and to an extent, `pandas` DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4775d03",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Huge benefit: the [documentation online](https://scikit-learn.org/stable/modules/classes.html) is excellent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfd3465",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The `LinearRegression` class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd239b9",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `sklearn` comes with several subpackages, including `linear_model` and `tree`, each of which contains several classes of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120e21e3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We'll start with the `LinearRegression` class from `linear_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2b8ab",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e4bc2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Important**: From [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression), we have:\n",
    "> LinearRegression fits a linear model with coefficients w = (w1, â€¦, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
    "\n",
    "    In other words, **`LinearRegression` minimizes mean squared error by default**! (Per the documentation, it also includes an intercept term by default.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf4d1ab",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LinearRegression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c823b92f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fitting a multiple linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a885031",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's aim to use `sklearn` to find the optimal parameters for the following model:\n",
    "\n",
    "$$\\begin{align*}\\text{pred. commute} &= H(\\text{departure hour}_i, \\text{day of month}_i) \\\\ &= w_0 + w_1 \\cdot \\text{departure hour}_i + w_2 \\cdot \\text{day of month}_i \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb3c442",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, we must instantiate a `LinearRegression` object and fit it. By calling `fit`, we are saying \"minimize mean squared error on this dataset and find $w^*$.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde055e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_multiple = LinearRegression()\n",
    "# Note that there are two arguments to fit â€“ X and y!\n",
    "# (It is not necessary to write X= and y=)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa2ac7",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- After fitting, we can access $w^*$ â€“ that is, the best intercept and coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8595a754",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04340f45",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- These coefficients tell us that the \"best way\" (according to squared loss) to make commute time predictions using a linear model is using:\n",
    "\n",
    "$$\\text{pred. commute}_i = 141.86 - 8.22 \\cdot \\text{departure hour}_i + 0.06 \\cdot \\text{day of month}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86221f33",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is the **plane of best fit** given historical data; it is not a causal statement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d387951d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's visualize this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca5bc49",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "XX, YY = np.mgrid[5:14:1, 0:31:1]\n",
    "Z = model_multiple.intercept_ + model_multiple.coef_[0] * XX + model_multiple.coef_[1] * YY\n",
    "plane = go.Surface(x=XX, y=YY, z=Z, colorscale='Reds')\n",
    "fig = go.Figure(data=[plane])\n",
    "fig.add_trace(go.Scatter3d(x=df['departure_hour'], \n",
    "                           y=df['day_of_month'], \n",
    "                           z=df['minutes'], mode='markers', marker = {'color': '#656DF1'}))\n",
    "fig.update_layout(scene=dict(xaxis_title='Departure Hour',\n",
    "                             yaxis_title='Day of Month',\n",
    "                             zaxis_title='Minutes'),\n",
    "                  title='Commute Time vs. Departure Hour and Day of Month',\n",
    "                  width=1000, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5927a",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59370e1d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Fit `LinearRegression` objects also have a `predict` method, which can be used to predict commute times given a `'departure_hour'` and `'day_of_month'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f01bce",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# What if I leave at 9:15AM on the 26th of the month?\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2199336",
   "metadata": {
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_multiple.predict(pd.DataFrame({'departure_hour': [9.25], 'day_of_month': [26]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42765d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330064ba",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since we're going to start to fit lots of different models to the commute times dataset, it'll be useful to compare their mean squared errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc546ed",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `sklearn` has a built-in `mean_squared_error` function.<br><small>Remember, the units of MSE are the units of $y$, squared. So the value below is 96.78 $\\text{minutes}^2$.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e470d0b",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94061830",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b646f49b",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's construct a dictionary to keep track of the MSEs we've seen so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c2a07",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mse_dict = {}\n",
    "mse_dict['departure_hour + day_of_month'] = mean_squared_error(df['minutes'], model_multiple.predict(df[['departure_hour', 'day_of_month']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba504da",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To compare, let's also fit and measure a simple linear model and constant model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c0b50",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Simple linear model.\n",
    "model_simple = LinearRegression()\n",
    "model_simple.fit(X=df[['departure_hour']], y=df['minutes'])\n",
    "mse_dict['departure_hour'] = mean_squared_error(df['minutes'], model_simple.predict(df[['departure_hour']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3551c7d",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Constant model.\n",
    "model_constant = df['minutes'].mean()\n",
    "mse_dict['constant'] = mean_squared_error(df['minutes'], np.ones(df.shape[0]) * model_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211e0bf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As we can see, adding `'day_of_month'` as a feature **barely** reduced our model's MSE.<br><small>Next week, when we learn about generalization, we'll see why sometimes adding more features is a **bad** thing!</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03ac05",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mse_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e556e95",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `LinearRegression` class summary\n",
    "\n",
    "|Property|Example|Description|\n",
    "|---|---|---|\n",
    "|Initialize model parameters| `lr = LinearRegression()` | Create (empty) linear regression model|\n",
    "|Fit the model to the data | `lr.fit(X, y)` | Determines regression coefficients|\n",
    "|Use model for prediction |`lr.predict(X_new)`| Uses regression line to make predictions|\n",
    "|Access model attributes| `lr.coef_`, `lr.intercept_` | Accesses the regression coefficients and intercept|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de71d3e2",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981d068c",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So far, in our journey to predict `'minutes'`, we've only used two numerical features in our dataset, `'departure_hour'` and `'day_of_month'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f1971e",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df[['departure_hour', 'day_of_month', 'minutes']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f65a8e3",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There's a lot of information in `df` that we didn't use â€“`'day'`, for example. We can't use these `'day'` in it's current form as it's non-numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5105b8f",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **How do we use categorical features in a regression model?**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "None",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
