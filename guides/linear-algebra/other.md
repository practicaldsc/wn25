---
layout: page
title: 8. Other resources
description: >-
  Other resources for learning linear algebra.
parent: üßÆ Linear Algebra
grand_parent: üßë‚Äçü§ù‚Äçüßë Guides
nav_order: 8
---

# {{ page.title }}
{:.no_toc}

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

---

The links below are more detailed than our guides, covering lots of important ideas we didn't touch on above (and won't get a chance to cover in _Practical Data Science_).

## Khan Academy

For more depth and added explanations, take a look at Khan Academy's [linear algebra](https://www.khanacademy.org/math/linear-algebra) course. Specifically, look at:
1. All of [Unit 1: Vectors and spaces](https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces).
1. The first four lessons in [Unit 2: Matrix transformations](https://www.khanacademy.org/math/linear-algebra/matrix-transformations), through Inverse functions and transformations.
1. The first two lessons in [Unit 3: Alternate coordinate systems (bases)](https://www.khanacademy.org/math/linear-algebra/alternate-bases), through Orthogonal projections. **Understanding the content of the orthogonal projections lesson is the primary learning objective of these guides.**

In addition, look at Khan Academy's [Multivariable calculus](https://www.khanacademy.org/math/multivariable-calculus) course, specifically:
1. Most of [Unit 1: Thinking about multivariable functions](https://www.khanacademy.org/math/multivariable-calculus/thinking-about-multivariable-function), though the lesson on Visualizing vector-valued functions is not necessary.
1. The first three lessons in [Unit 2: Derivatives of multivariable functions](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives), through Partial derivative and gradient. (These are also linked above.)

---


## 3Blue1Brown

Grant Sanderson (3Blue1Brown) produces excellent visual explanations of a lot of ideas that are central to data science, including neural networks, but also linear algebra. His Essence of Linear Algebra series overlaps a fair bit with the videos we've provided above, but with some content missing (namely, projecting onto the span of multiple vectors) and other content more heavily emphasized (e.g. the idea of a linear transformation). The series exists in two forms:
- As [videos on YouTube](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab).
- As [written articles](https://www.3blue1brown.com/topics/linear-algebra).

Either way, (at least) **the first 9 chapters** are relevant to us.

---

## Mathematics for Machine Learning

[Mathematics for Machine Learning](https://gwthomas.github.io/docs/math4ml.pdf), by Garrett Thomas, is a document that discusses the mathematics relevant for machine learning in **much** greater detail than we'll need in this course. It's quite advanced, so don't be alarmed if the material there feels inaccessible. (More of it will make sense after _Practical Data Science_!)

---

## Linear Algebra for Data Science

If you'd prefer to read a more formal textbook on these ideas, take a look at [Linear Algebra for Data Science](https://kyunghyuncho.me/linear-algebra-for-data-science/) by Wanmo Kang and Kyunghyun Cho (a PDF is available at the linked site). Specifically, read Chapters 1-4.